#!/usr/bin/env python3
"""
å¤šç»´åº¦Pairwise Comparisonè¯„æµ‹è„šæœ¬
åŸºäºQwen3-235B Judge APIè¿›è¡Œå¤šç»´åº¦è¯„åˆ†
è¾“å‡ºæ ¼å¼ç¬¦åˆç”¨æˆ·æŒ‡å®šçš„JSONæ¨¡æ¿
"""

import json
import requests
from typing import List, Dict, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import pandas as pd
from datetime import datetime

# ==================== APIé…ç½® ====================

JUDGE_API_URL = "https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions"
JUDGE_API_KEY = "msk-c3e6e836acff79160513e563d6d288d3e24b9605349a03bb9564e9a7b3bafefe"

# æ•°æ®è·¯å¾„
BASE_MODEL_FILE = "/data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl"
TUNED_MODEL_FILE = "/data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-newtemplateqwen3.jsonl"

# è¯„æµ‹å‚æ•°
NUM_SAMPLES = 100   # æµ‹è¯•æ•°é‡ï¼ŒNoneè¡¨ç¤ºå…¨éƒ¨
MAX_WORKERS = 5     # APIå¹¶å‘æ•°
OUTPUT_DIR = "/data/private/outputs"
PAIRWISE_DATA_FILE = f"{OUTPUT_DIR}/pairwise_comparison_data.jsonl"
EVALUATION_RESULTS_FILE = f"{OUTPUT_DIR}/evaluation_results.jsonl"
SUMMARY_FILE = f"{OUTPUT_DIR}/evaluation_summary.json"

# ==================== è¯„æµ‹ç»´åº¦å®šä¹‰ ====================

EVALUATION_DIMENSIONS = {
    "correctness": {
        "name": "æ­£ç¡®æ€§",
        "weight": 0.30,
        "description": "å›ç­”çš„äº‹å®å‡†ç¡®æ€§ã€é€»è¾‘æ¨ç†çš„æ­£ç¡®æ€§ï¼Œæ˜¯å¦æœ‰é”™è¯¯ä¿¡æ¯"
    },
    "relevance": {
        "name": "ç›¸å…³æ€§",
        "weight": 0.15,
        "description": "å›ç­”æ˜¯å¦åˆ‡é¢˜ï¼Œæ˜¯å¦å‡†ç¡®ç†è§£å¹¶å›åº”äº†ç”¨æˆ·çš„é—®é¢˜"
    },
    "completeness": {
        "name": "å®Œæ•´æ€§",
        "weight": 0.15,
        "description": "å›ç­”æ˜¯å¦å…¨é¢ï¼Œæ˜¯å¦æ¶µç›–äº†å…³é”®ä¿¡æ¯ï¼Œæœ‰æ— é—æ¼é‡è¦å†…å®¹"
    },
    "reasoning": {
        "name": "æ¨ç†èƒ½åŠ›",
        "weight": 0.20,
        "description": "æ€è€ƒè¿‡ç¨‹çš„é€»è¾‘æ€§ã€æ¨ç†é“¾æ¡çš„æ¸…æ™°åº¦ã€è§£é‡Šçš„æ·±åº¦"
    },
    "clarity": {
        "name": "è¡¨è¾¾æ¸…æ™°åº¦",
        "weight": 0.10,
        "description": "è¯­è¨€æ˜¯å¦ç®€æ´æ˜äº†ã€è¡¨è¿°æ˜¯å¦è‡ªç„¶æµç•…ã€ç»“æ„æ˜¯å¦æ¸…æ™°"
    },
    "helpfulness": {
        "name": "æœ‰ç”¨æ€§",
        "weight": 0.10,
        "description": "å›ç­”å¯¹ç”¨æˆ·çš„å®é™…å¸®åŠ©ç¨‹åº¦ã€æ˜¯å¦æ˜“äºç†è§£å’Œåº”ç”¨"
    }
}

# ==================== å·¥å…·å‡½æ•° ====================

def extract_think_and_answer(text: str) -> Tuple[str, str]:
    """æå–thinkingå’Œanswer"""
    if '<think>' in text and '</think>' in text:
        parts = text.split('</think>')
        thinking = parts[0].split('<think>')[-1].strip()
        answer = parts[-1].strip()
        return thinking, answer
    return "", text.strip()

def call_judge_api_pairwise(
    prompt: str,
    response_a: str,
    response_b: str,
    label: str,
    dimension_key: str
) -> Dict:
    """
    è°ƒç”¨Judge APIè¿›è¡ŒPairwiseå¯¹æ¯”è¯„åˆ†
    
    è¿”å›: {"score": 1-5, "reason": "..."}
    - 1åˆ†: Aæ˜æ˜¾æ›´ä¼˜
    - 2åˆ†: Aç•¥ä¼˜
    - 3åˆ†: ç›¸å½“
    - 4åˆ†: Bç•¥ä¼˜
    - 5åˆ†: Bæ˜æ˜¾æ›´ä¼˜
    """
    dim_config = EVALUATION_DIMENSIONS[dimension_key]
    
    # æå–thinkingå’Œanswer
    think_a, ans_a = extract_think_and_answer(response_a)
    think_b, ans_b = extract_think_and_answer(response_b)
    
    judge_prompt = f"""# ä»»åŠ¡è¯´æ˜
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ¨¡å‹å›ç­”è¯„æµ‹å‘˜ã€‚è¯·å¯¹ä¸¤ä¸ªAIæ¨¡å‹çš„å›ç­”è¿›è¡ŒPairwiseæ¯”è¾ƒï¼Œå¹¶åœ¨ã€{dim_config['name']}ã€‘è¿™ä¸ªç»´åº¦ä¸Šè¿›è¡Œè¯„åˆ†ã€‚

## è¯„åˆ†ç»´åº¦
**{dim_config['name']}**: {dim_config['description']}

## é—®é¢˜
{prompt}

## å‚è€ƒç­”æ¡ˆ
{label}

## æ¨¡å‹Açš„å›ç­”
{response_a}

## æ¨¡å‹Bçš„å›ç­”
{response_b}

## è¯„åˆ†æ ‡å‡†
è¯·åœ¨ã€{dim_config['name']}ã€‘ç»´åº¦ä¸Šï¼Œå¯¹æ¨¡å‹Bç›¸å¯¹äºæ¨¡å‹Açš„è¡¨ç°æ‰“åˆ†ï¼ˆ1-5åˆ†ï¼‰ï¼š
- **5åˆ†**: Bæ˜æ˜¾ä¼˜äºAï¼ˆBåœ¨è¯¥ç»´åº¦ä¸Šæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼‰
- **4åˆ†**: Bç•¥ä¼˜äºAï¼ˆBåœ¨è¯¥ç»´åº¦ä¸Šç¨å¥½ï¼‰
- **3åˆ†**: Aå’ŒBæŒå¹³ï¼ˆä¸¤è€…åœ¨è¯¥ç»´åº¦ä¸Šè¡¨ç°ç›¸å½“ï¼‰
- **2åˆ†**: Aç•¥ä¼˜äºBï¼ˆAåœ¨è¯¥ç»´åº¦ä¸Šç¨å¥½ï¼‰
- **1åˆ†**: Aæ˜æ˜¾ä¼˜äºBï¼ˆAåœ¨è¯¥ç»´åº¦ä¸Šæœ‰æ˜¾è‘—ä¼˜åŠ¿ï¼‰

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦åŒ…å«ä»»ä½•é¢å¤–æ–‡å­—ï¼‰ï¼š
{{
  "score": <1-5çš„æ•´æ•°>,
  "reason": "<ç®€çŸ­çš„è¯„åˆ†ç†ç”±ï¼Œ50å­—ä»¥å†…>"
}}"""

    try:
        response = requests.post(
            JUDGE_API_URL,
            headers={
                "Content-Type": "application/json",
                "Authorization": f"Bearer {JUDGE_API_KEY}"
            },
            json={
                "model": "/model/qwen3-235b-a22b",
                "messages": [{"role": "user", "content": judge_prompt}],
                "temperature": 0.1,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content'].strip()
            
            # å°è¯•è§£æJSON
            try:
                # æå–JSONå†…å®¹
                if '```json' in content:
                    content = content.split('```json')[1].split('```')[0].strip()
                elif '```' in content:
                    content = content.split('```')[1].split('```')[0].strip()
                
                parsed = json.loads(content)
                score = parsed.get('score', 3)
                reason = parsed.get('reason', content[:100])
                
                # éªŒè¯åˆ†æ•°èŒƒå›´
                if not isinstance(score, int) or score < 1 or score > 5:
                    score = 3
                
                return {"score": score, "reason": reason}
            except:
                # å¦‚æœè§£æå¤±è´¥ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–
                return {"score": 3, "reason": content[:100]}
        else:
            return {"score": 3, "reason": "APIè°ƒç”¨å¤±è´¥"}
    except Exception as e:
        print(f"è¯„åˆ†å¤±è´¥: {e}")
        return {"score": 3, "reason": f"å¼‚å¸¸: {str(e)[:50]}"}

# ==================== ä¸»æµç¨‹ ====================

print("="*70)
print("ğŸ¯ å¤šç»´åº¦Pairwise Comparisonè¯„æµ‹")
print("="*70)

# 0. åˆ›å»ºè¾“å‡ºç›®å½•
import os
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"ğŸ“ è¾“å‡ºç›®å½•: {OUTPUT_DIR}")

# 1. åŠ è½½æ•°æ®
print("\nğŸ“– åŠ è½½æ•°æ®...")
with open(BASE_MODEL_FILE, 'r', encoding='utf-8') as f:
    base_data = [json.loads(line) for line in f]

with open(TUNED_MODEL_FILE, 'r', encoding='utf-8') as f:
    tuned_data = [json.loads(line) for line in f]

if NUM_SAMPLES:
    base_data = base_data[:NUM_SAMPLES]
    tuned_data = tuned_data[:NUM_SAMPLES]

print(f"âœ… åŠ è½½äº† {len(base_data)} å¯¹æ ·æœ¬")
assert len(base_data) == len(tuned_data), "æ•°æ®é›†é•¿åº¦ä¸ä¸€è‡´ï¼"

# 2. æ„å»ºPairwise Comparisonæ•°æ®
print("\nğŸ“Š æ„å»ºPairwiseå¯¹æ¯”æ•°æ®...")
pairwise_data = []

for i, (base_item, tuned_item) in enumerate(zip(base_data, tuned_data)):
    pairwise_record = {
        "question_id": f"qwen3_eval_{i:04d}",
        "question": base_item['prompt'],
        "answer_a": base_item['predict'],  # åŸå§‹æ¨¡å‹
        "answer_b": tuned_item['predict'], # å¾®è°ƒæ¨¡å‹
        "reference_answer": base_item['label']
    }
    pairwise_data.append(pairwise_record)

print(f"âœ… æ„å»ºäº† {len(pairwise_data)} ä¸ªå¯¹æ¯”æ ·æœ¬")

# ä¿å­˜pairwiseæ•°æ®
print(f"\nğŸ’¾ ä¿å­˜Pairwiseæ•°æ®åˆ°: {PAIRWISE_DATA_FILE}")
with open(PAIRWISE_DATA_FILE, 'w', encoding='utf-8') as f:
    for item in pairwise_data:
        f.write(json.dumps(item, ensure_ascii=False) + '\n')
print("âœ… Pairwiseæ•°æ®å·²ä¿å­˜")

# 3. å¤šç»´åº¦è¯„åˆ†
print(f"\nğŸ¯ å¼€å§‹å¤šç»´åº¦è¯„åˆ† (å¹¶å‘æ•°={MAX_WORKERS})...")
print(f"ğŸ“‹ è¯„æµ‹ç»´åº¦: {', '.join([v['name'] for v in EVALUATION_DIMENSIONS.values()])}")

# å‡†å¤‡æ‰€æœ‰è¯„åˆ†ä»»åŠ¡
tasks = []
for i, item in enumerate(pairwise_data):
    for dim_key in EVALUATION_DIMENSIONS.keys():
        tasks.append({
            'index': i,
            'dimension': dim_key,
            'prompt': item['question'],
            'response_a': item['answer_a'],
            'response_b': item['answer_b'],
            'label': item['reference_answer']
        })

total_tasks = len(tasks)
print(f"ğŸ“ æ€»è®¡ {total_tasks} ä¸ªè¯„åˆ†ä»»åŠ¡ ({len(pairwise_data)}æ ·æœ¬ Ã— {len(EVALUATION_DIMENSIONS)}ç»´åº¦)")

# å¹¶å‘è°ƒç”¨Judge API
results_map = {}  # (index, dimension) -> {"score": ..., "reason": ...}

with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
    futures = {
        executor.submit(
            call_judge_api_pairwise,
            task['prompt'],
            task['response_a'],
            task['response_b'],
            task['label'],
            task['dimension']
        ): task
        for task in tasks
    }
    
    for future in tqdm(as_completed(futures), total=total_tasks, desc="è¯„åˆ†è¿›åº¦"):
        task = futures[future]
        try:
            result = future.result()
            key = (task['index'], task['dimension'])
            results_map[key] = result
        except Exception as e:
            print(f"\nä»»åŠ¡å¤±è´¥: {e}")
            key = (task['index'], task['dimension'])
            results_map[key] = {"score": 3, "reason": "å¼‚å¸¸"}

# 4. æ±‡æ€»ç»“æœå¹¶ç”Ÿæˆç¬¦åˆæ¨¡æ¿çš„è¾“å‡º
print("\nğŸ“Š æ±‡æ€»å¤šç»´åº¦è¯„åˆ†...")

evaluation_results = []
for i, item in enumerate(pairwise_data):
    # æ”¶é›†å„ç»´åº¦è¯„åˆ†
    criteria = {}
    weighted_score = 0
    
    for dim_key, dim_config in EVALUATION_DIMENSIONS.items():
        key = (i, dim_key)
        score_data = results_map.get(key, {"score": 3, "reason": ""})
        score = score_data['score']
        reason = score_data['reason']
        
        criteria[dim_key] = {
            "score": score,
            "weight": dim_config['weight'],
            "comment": reason
        }
        weighted_score += score * dim_config['weight']
    
    # åˆ¤æ–­ä¼˜å…ˆé€‰æ‹©
    if weighted_score > 3.2:
        preferred = "B"
        final_reason = "æ¨¡å‹Bï¼ˆå¾®è°ƒæ¨¡å‹ï¼‰åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°æ›´ä¼˜"
    elif weighted_score < 2.8:
        preferred = "A"
        final_reason = "æ¨¡å‹Aï¼ˆåŸå§‹æ¨¡å‹ï¼‰åœ¨å¤šä¸ªç»´åº¦ä¸Šè¡¨ç°æ›´ä¼˜"
    else:
        preferred = "Tie"
        final_reason = "ä¸¤ä¸ªæ¨¡å‹è¡¨ç°ç›¸å½“ï¼Œå„æœ‰ä¼˜åŠ¿"
    
    # æŒ‰ç…§ç”¨æˆ·æ¨¡æ¿æ ¼å¼æ„å»ºç»“æœ
    result = {
        "question_id": pairwise_data[i]['question_id'],
        "question": pairwise_data[i]['question'],
        "answer_a": pairwise_data[i]['answer_a'],
        "answer_b": pairwise_data[i]['answer_b'],
        "evaluation": {
            "criteria": criteria,
            "overall_score": round(weighted_score, 2),
            "preferred": preferred,
            "final_reason": final_reason
        },
        "judge_model": "qwen3-235b-a22b",
        "evaluation_time": datetime.now().isoformat() + "Z"
    }
    
    evaluation_results.append(result)

# 5. ä¿å­˜è¯¦ç»†è¯„æµ‹ç»“æœï¼ˆæŒ‰ç”¨æˆ·æ¨¡æ¿æ ¼å¼ï¼‰
print(f"\nğŸ’¾ ä¿å­˜è¯¦ç»†è¯„æµ‹ç»“æœåˆ°: {EVALUATION_RESULTS_FILE}")
with open(EVALUATION_RESULTS_FILE, 'w', encoding='utf-8') as f:
    for result in evaluation_results:
        f.write(json.dumps(result, ensure_ascii=False) + '\n')
print("âœ… è¯¦ç»†è¯„æµ‹ç»“æœå·²ä¿å­˜")

# 6. è®¡ç®—æ±‡æ€»ç»Ÿè®¡
print("\nğŸ“Š è®¡ç®—æ±‡æ€»ç»Ÿè®¡...")

# ç»Ÿè®¡èƒœç‡
a_wins = sum(1 for r in evaluation_results if r['evaluation']['preferred'] == 'A')
b_wins = sum(1 for r in evaluation_results if r['evaluation']['preferred'] == 'B')
ties = sum(1 for r in evaluation_results if r['evaluation']['preferred'] == 'Tie')
total_cases = len(evaluation_results)

# è®¡ç®—å„ç»´åº¦å¹³å‡åˆ†
# å°†pairwiseåˆ†æ•°è½¬æ¢ä¸ºå„æ¨¡å‹çš„ç»å¯¹åˆ†æ•°
model_a_scores = []
model_b_scores = []
model_a_dim_scores = {dim_key: [] for dim_key in EVALUATION_DIMENSIONS.keys()}
model_b_dim_scores = {dim_key: [] for dim_key in EVALUATION_DIMENSIONS.keys()}

for result in evaluation_results:
    overall_score = result['evaluation']['overall_score']
    
    for dim_key in EVALUATION_DIMENSIONS.keys():
        score = result['evaluation']['criteria'][dim_key]['score']
        # å°†pairwiseåˆ†æ•°(1-5)è½¬æ¢ä¸ºç»å¯¹åˆ†æ•°
        # score=1(Aå¼º), 2(Aç•¥ä¼˜), 3(å¹³), 4(Bç•¥ä¼˜), 5(Bå¼º)
        if score == 1:
            model_a_dim_scores[dim_key].append(5.0)
            model_b_dim_scores[dim_key].append(2.0)
        elif score == 2:
            model_a_dim_scores[dim_key].append(4.5)
            model_b_dim_scores[dim_key].append(3.0)
        elif score == 3:
            model_a_dim_scores[dim_key].append(4.0)
            model_b_dim_scores[dim_key].append(4.0)
        elif score == 4:
            model_a_dim_scores[dim_key].append(3.0)
            model_b_dim_scores[dim_key].append(4.5)
        else:  # score == 5
            model_a_dim_scores[dim_key].append(2.0)
            model_b_dim_scores[dim_key].append(5.0)
    
    # æ•´ä½“åˆ†æ•°
    if overall_score < 2.5:
        model_a_scores.append(5.0)
        model_b_scores.append(2.5)
    elif overall_score < 2.9:
        model_a_scores.append(4.5)
        model_b_scores.append(3.5)
    elif overall_score <= 3.1:
        model_a_scores.append(4.0)
        model_b_scores.append(4.0)
    elif overall_score <= 3.5:
        model_a_scores.append(3.5)
        model_b_scores.append(4.5)
    else:
        model_a_scores.append(2.5)
        model_b_scores.append(5.0)

# è®¡ç®—å¹³å‡å€¼
model_a_per_dim_avg = {
    dim_key: round(sum(scores) / len(scores), 2)
    for dim_key, scores in model_a_dim_scores.items()
}
model_b_per_dim_avg = {
    dim_key: round(sum(scores) / len(scores), 2)
    for dim_key, scores in model_b_dim_scores.items()
}

model_a_avg_overall = round(sum(model_a_scores) / len(model_a_scores), 2)
model_b_avg_overall = round(sum(model_b_scores) / len(model_b_scores), 2)

# æŒ‰ç”¨æˆ·æ¨¡æ¿æ ¼å¼æ„å»ºæ±‡æ€»æŠ¥å‘Š
evaluation_summary = {
    "evaluation_summary": {
        "total_cases": total_cases,
        "model_A": {
            "model_name": "Qwen3-4B (åŸå§‹æ¨¡å‹)",
            "model_path": BASE_MODEL_FILE,
            "win_rate": round(a_wins / total_cases, 3),
            "avg_overall_score": model_a_avg_overall,
            "per_dimension_avg": model_a_per_dim_avg
        },
        "model_B": {
            "model_name": "Qwen3-4B-LoRA (å¾®è°ƒæ¨¡å‹)",
            "model_path": TUNED_MODEL_FILE,
            "win_rate": round(b_wins / total_cases, 3),
            "avg_overall_score": model_b_avg_overall,
            "per_dimension_avg": model_b_per_dim_avg
        },
        "tie_rate": round(ties / total_cases, 3),
        "judge_model": "qwen3-235b-a22b"
    }
}

# ä¿å­˜æ±‡æ€»æŠ¥å‘Š
print(f"\nğŸ’¾ ä¿å­˜æ±‡æ€»æŠ¥å‘Šåˆ°: {SUMMARY_FILE}")
with open(SUMMARY_FILE, 'w', encoding='utf-8') as f:
    json.dump(evaluation_summary, f, ensure_ascii=False, indent=2)
print("âœ… æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜")

# 7. æ‰“å°ç»Ÿè®¡ç»“æœ
print("\n" + "="*70)
print("ğŸ“ˆ è¯„æµ‹ç»“æœæ±‡æ€»")
print("="*70)

print(f"\nğŸ“Š æ€»è¯„æµ‹æ ·æœ¬æ•°: {total_cases}")

print("\nã€èƒœç‡ç»Ÿè®¡ã€‘")
print(f"  æ¨¡å‹Aï¼ˆåŸå§‹ï¼‰èƒœ: {a_wins}/{total_cases} ({a_wins/total_cases*100:.1f}%)")
print(f"  æ¨¡å‹Bï¼ˆå¾®è°ƒï¼‰èƒœ: {b_wins}/{total_cases} ({b_wins/total_cases*100:.1f}%)")
print(f"  å¹³å±€:           {ties}/{total_cases} ({ties/total_cases*100:.1f}%)")

print("\nã€å„ç»´åº¦å¹³å‡åˆ†å¯¹æ¯”ã€‘(1-5åˆ†åˆ¶)")
print(f"{'ç»´åº¦':<15} {'æ¨¡å‹A':<10} {'æ¨¡å‹B':<10} {'å·®å€¼':<10}")
print("-" * 50)
for dim_key, dim_config in EVALUATION_DIMENSIONS.items():
    a_score = model_a_per_dim_avg[dim_key]
    b_score = model_b_per_dim_avg[dim_key]
    diff = b_score - a_score
    print(f"{dim_config['name']:<15} {a_score:<10.2f} {b_score:<10.2f} {diff:+.2f}")

print("\nã€æ•´ä½“å¹³å‡åˆ†ã€‘")
print(f"  æ¨¡å‹A: {model_a_avg_overall:.2f}")
print(f"  æ¨¡å‹B: {model_b_avg_overall:.2f}")

if b_wins > a_wins * 1.5:
    conclusion = "âœ… å¾®è°ƒæ¨¡å‹(B)è¡¨ç°æ˜æ˜¾ä¼˜äºåŸå§‹æ¨¡å‹(A)"
elif b_wins > a_wins:
    conclusion = "â†— å¾®è°ƒæ¨¡å‹(B)è¡¨ç°ç•¥ä¼˜äºåŸå§‹æ¨¡å‹(A)"
elif a_wins > b_wins * 1.5:
    conclusion = "âŒ åŸå§‹æ¨¡å‹(A)è¡¨ç°æ˜æ˜¾ä¼˜äºå¾®è°ƒæ¨¡å‹(B)"
elif a_wins > b_wins:
    conclusion = "â†˜ åŸå§‹æ¨¡å‹(A)è¡¨ç°ç•¥ä¼˜äºå¾®è°ƒæ¨¡å‹(B)"
else:
    conclusion = "â¡ ä¸¤ä¸ªæ¨¡å‹æ•´ä½“è¡¨ç°ç›¸å½“"

print(f"\nã€ç»“è®ºã€‘: {conclusion}")

print("\n" + "="*70)
print("ğŸ‰ è¯„æµ‹å®Œæˆï¼")
print("="*70)
print(f"\nğŸ“ è¾“å‡ºæ–‡ä»¶:")
print(f"  1. Pairwiseæ•°æ®:    {PAIRWISE_DATA_FILE}")
print(f"  2. è¯¦ç»†è¯„æµ‹ç»“æœ:    {EVALUATION_RESULTS_FILE}")
print(f"  3. æ±‡æ€»æŠ¥å‘Š:        {SUMMARY_FILE}")
