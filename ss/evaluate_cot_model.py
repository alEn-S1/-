#!/usr/bin/env python3
"""
Chain-of-Thoughtæ¨¡å‹è¯„æµ‹è„šæœ¬
è¯„ä¼°LoRAå¾®è°ƒåæ¨¡å‹åœ¨ç”Ÿæˆ<think>æ ‡ç­¾ã€thinkingè´¨é‡å’Œç­”æ¡ˆè´¨é‡æ–¹é¢çš„è¡¨ç°
"""

import json
import re
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import argparse
from pathlib import Path
import numpy as np
from vllm import LLM, SamplingParams
from tqdm import tqdm
import pandas as pd
import requests
import time
from concurrent.futures import ThreadPoolExecutor, as_completed


@dataclass
class EvaluationResult:
    """å•æ¡æ•°æ®çš„è¯„æµ‹ç»“æœï¼ˆä»…enable_thinking=Falseï¼‰"""
    index: int
    instruction: str
    
    # åŸå§‹æ¨¡å‹ç»“æœ (enable_thinking=False)
    base_has_think: bool
    base_think_length: int
    base_answer_length: int
    base_response: str
    
    # å¾®è°ƒæ¨¡å‹ç»“æœ (enable_thinking=False)
    tuned_has_think: bool
    tuned_think_length: int
    tuned_answer_length: int
    tuned_response: str
    
    # è´¨é‡è¯„åˆ†(åç»­å¡«å……ï¼Œå¯é€‰)
    base_think_score: float = 0.0
    base_answer_score: float = 0.0
    tuned_think_score: float = 0.0
    tuned_answer_score: float = 0.0


class CoTEvaluator:
    """Chain-of-Thoughtè¯„æµ‹å™¨"""
    
    def __init__(
        self,
        base_model_path: str,
        tuned_model_path: str,
        test_data_path: str,
        judge_api_url: str = None,
        judge_api_key: str = None,
        max_model_len: int = 2048,
        tensor_parallel_size: int = 1,
        num_samples: int = None,
        max_workers: int = 15,
    ):
        self.base_model_path = base_model_path
        self.tuned_model_path = tuned_model_path
        self.test_data_path = test_data_path
        self.judge_api_url = judge_api_url
        self.judge_api_key = judge_api_key
        self.max_model_len = max_model_len
        self.tensor_parallel_size = tensor_parallel_size
        self.max_workers = max_workers
        
        # åŠ è½½æµ‹è¯•æ•°æ®
        print(f"ğŸ“– åŠ è½½æµ‹è¯•æ•°æ®: {test_data_path}")
        with open(test_data_path, 'r', encoding='utf-8') as f:
            self.test_data = json.load(f)
        
        # é™åˆ¶æ ·æœ¬æ•°é‡
        if num_samples:
            self.test_data = self.test_data[:num_samples]
            print(f"âœ… ä½¿ç”¨å‰ {len(self.test_data)} æ¡æµ‹è¯•æ•°æ®")
        else:
            print(f"âœ… åŠ è½½äº† {len(self.test_data)} æ¡æµ‹è¯•æ•°æ®")
        
    def extract_think_and_answer(self, text: str) -> Tuple[str, str]:
        """
        ä»ç”Ÿæˆæ–‡æœ¬ä¸­æå–thinkingéƒ¨åˆ†å’Œansweréƒ¨åˆ†
        
        Returns:
            (thinking_content, answer_content)
        """
        # åŒ¹é…<think>æ ‡ç­¾
        think_pattern = r'<think>(.*?)</think>'
        match = re.search(think_pattern, text, re.DOTALL)
        
        if match:
            thinking = match.group(1).strip()
            # answeræ˜¯</think>ä¹‹åçš„å†…å®¹
            answer = text[match.end():].strip()
            return thinking, answer
        else:
            # æ²¡æœ‰<think>æ ‡ç­¾,å…¨éƒ¨ç®—ä½œanswer
            return "", text.strip()
    
    def generate_responses(
        self, 
        model_path: str, 
        messages_list: List[List[Dict]],
        enable_thinking: bool = False
    ) -> List[str]:
        """
        ä½¿ç”¨vllmæ‰¹é‡ç”Ÿæˆå“åº”ï¼Œé€šè¿‡tokenizeræ­£ç¡®ä¼ é€’enable_thinkingå‚æ•°
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            messages_list: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ä¸º [[{"role": "user", "content": "..."}], ...]
            enable_thinking: æ˜¯å¦å¯ç”¨thinkingæ¨¡å¼
            
        Returns:
            ç”Ÿæˆçš„å“åº”åˆ—è¡¨
        """
        from transformers import AutoTokenizer
        
        thinking_status = "å¯ç”¨" if enable_thinking else "ç¦ç”¨"
        print(f"\nğŸš€ åŠ è½½æ¨¡å‹: {model_path} (enable_thinking={enable_thinking})")
        
        # åŠ è½½tokenizer
        print("ğŸ“ åŠ è½½tokenizerå¹¶åº”ç”¨chat_template...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True
        )
        
        # ä½¿ç”¨tokenizer.apply_chat_templateå¹¶ä¼ é€’enable_thinkingå‚æ•°
        prompts = []
        for messages in messages_list:
            try:
                prompt = tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=True,
                    chat_template_kwargs={"enable_thinking": enable_thinking}  # âœ… å…³é”®å‚æ•°
                )
                prompts.append(prompt)
            except Exception as e:
                print(f"âš ï¸  apply_chat_templateå¤±è´¥: {e}")
                # é™çº§åˆ°æ‰‹åŠ¨æ„å»ºprompt
                user_content = messages[0]["content"]
                prompt = f"<|im_start|>system\nä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„AIåŠ©æ‰‹ã€‚<|im_end|>\n<|im_start|>user\n{user_content}<|im_end|>\n<|im_start|>assistant\n"
                prompts.append(prompt)
        
        print(f"âœ… ç”Ÿæˆäº† {len(prompts)} ä¸ªprompts (enable_thinking={enable_thinking})")
        
        # åŠ è½½vllmæ¨¡å‹
        llm = LLM(
            model=model_path,
            trust_remote_code=True,
            max_model_len=self.max_model_len,
            tensor_parallel_size=self.tensor_parallel_size,
            gpu_memory_utilization=0.85,
        )
        
        # è®¾ç½®é‡‡æ ·å‚æ•°
        sampling_params = SamplingParams(
            temperature=0.7,
            top_p=0.9,
            max_tokens=2048,
            stop=["</s>", "<|endoftext|>", "<|im_end|>"],
        )
        
        print(f"ğŸ”„ å¼€å§‹æ‰¹é‡æ¨ç† ({len(prompts)} æ¡æ•°æ®, enable_thinking={thinking_status})...")
        outputs = llm.generate(prompts, sampling_params)
        
        # æå–ç”Ÿæˆçš„æ–‡æœ¬
        responses = [output.outputs[0].text for output in outputs]
        
        # é‡Šæ”¾æ¨¡å‹å’Œtokenizer
        del llm
        del tokenizer
        import torch
        torch.cuda.empty_cache()
        
        return responses
    
    def prepare_messages(self) -> List[List[Dict]]:
        """
        å‡†å¤‡æ¨ç†ç”¨çš„messagesï¼ˆOpenAIæ ¼å¼ï¼‰
        è¿”å›æ ¼å¼: [[{"role": "user", "content": "..."}], ...]
        """
        messages_list = []
        for item in self.test_data:
            instruction = item['instruction']
            input_text = item.get('input', '')
            
            # æ„å»ºuser content
            if input_text:
                user_content = f"{instruction}\n\n{input_text}"
            else:
                user_content = instruction
            
            # OpenAI messagesæ ¼å¼
            messages = [
                {"role": "user", "content": user_content}
            ]
            
            messages_list.append(messages)
        
        return messages_list
    
    def call_judge_api(
        self, 
        instruction: str, 
        thinking: str, 
        answer: str,
        ground_truth: str,
        eval_type: str
    ) -> float:
        """
        è°ƒç”¨Judge APIè¯„ä¼°è´¨é‡
        
        Args:
            instruction: é—®é¢˜
            thinking: æ€ç»´è¿‡ç¨‹
            answer: ç­”æ¡ˆ
            ground_truth: æ ‡å‡†ç­”æ¡ˆ
            eval_type: è¯„ä¼°ç±»å‹ ("thinking" æˆ– "answer")
            
        Returns:
            è¯„åˆ† (0-10)
        """
        if eval_type == "thinking":
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ€ç»´è¿‡ç¨‹è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ€ç»´è¿‡ç¨‹(thinking)çš„è´¨é‡ã€‚

è¯„åˆ†æ ‡å‡†(0-10åˆ†):
- é€»è¾‘è¿è´¯æ€§: æ€ç»´è¿‡ç¨‹æ˜¯å¦æœ‰æ¸…æ™°çš„æ¨ç†é“¾æ¡
- å®Œæ•´æ€§: æ˜¯å¦æ¶µç›–äº†è§£å†³é—®é¢˜çš„å…³é”®æ­¥éª¤
- æ·±åº¦: æ€è€ƒæ˜¯å¦æ·±å…¥,æœ‰æ— è€ƒè™‘å¤šä¸ªè§’åº¦
- ç»“æ„æ€§: æ€ç»´ç»„ç»‡æ˜¯å¦æ¸…æ™°,æ˜“äºç†è§£

é—®é¢˜: {instruction}

æ€ç»´è¿‡ç¨‹:
{thinking if thinking else "[æ— æ€ç»´è¿‡ç¨‹]"}

æ ‡å‡†ç­”æ¡ˆä¾›å‚è€ƒ:
{ground_truth}

è¯·åªè¾“å‡ºä¸€ä¸ª0-10ä¹‹é—´çš„åˆ†æ•°,ä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        else:  # answer
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„è´¨é‡ã€‚

è¯„åˆ†æ ‡å‡†(0-10åˆ†):
- å‡†ç¡®æ€§: ç­”æ¡ˆæ˜¯å¦æ­£ç¡®å›ç­”äº†é—®é¢˜
- å®Œæ•´æ€§: ç­”æ¡ˆæ˜¯å¦å…¨é¢,æœ‰æ— é—æ¼å…³é”®ä¿¡æ¯
- æ¸…æ™°åº¦: è¡¨è¾¾æ˜¯å¦æ¸…æ™°æ˜“æ‡‚
- ä¸“ä¸šæ€§: æ˜¯å¦ä½¿ç”¨æ°å½“çš„ä¸“ä¸šæœ¯è¯­å’Œè¡¨è¾¾æ–¹å¼

é—®é¢˜: {instruction}

ç”Ÿæˆçš„ç­”æ¡ˆ:
{answer if answer else "[æ— ç­”æ¡ˆ]"}

æ ‡å‡†ç­”æ¡ˆä¾›å‚è€ƒ:
{ground_truth}

è¯·åªè¾“å‡ºä¸€ä¸ª0-10ä¹‹é—´çš„åˆ†æ•°,ä¸è¦æœ‰å…¶ä»–å†…å®¹ã€‚"""
        
        # è°ƒç”¨API
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.judge_api_key}"
        }
        
        payload = {
            "model": "/model/qwen3-235b-a22b",
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": True,
            "chat_template_kwargs": {"enable_thinking": False}
        }
        
        try:
            response = requests.post(
                self.judge_api_url,
                headers=headers,
                json=payload,
                timeout=60
            )
            response.raise_for_status()
            
            # è§£ææµå¼å“åº”
            full_response = ""
            for line in response.iter_lines():
                if line:
                    line = line.decode('utf-8')
                    if line.startswith('data: '):
                        data_str = line[6:]
                        if data_str.strip() == '[DONE]':
                            break
                        try:
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                full_response += content
                        except json.JSONDecodeError:
                            continue
            
            # æå–åˆ†æ•°
            score_match = re.search(r'(\d+\.?\d*)', full_response.strip())
            if score_match:
                score = float(score_match.group(1))
                score = min(max(score, 0), 10)  # é™åˆ¶åœ¨0-10ä¹‹é—´
                return score
            else:
                print(f"âš ï¸  æ— æ³•ä»å“åº”ä¸­æå–åˆ†æ•°: {full_response[:100]}")
                return 5.0
                
        except Exception as e:
            print(f"âš ï¸  APIè°ƒç”¨å¤±è´¥: {e}")
            return 5.0
    
    def basic_evaluation(self) -> List[EvaluationResult]:
        """
        åŸºç¡€è¯„æµ‹: ç”Ÿæˆå“åº”å¹¶æå–thinking/answer
        åªæµ‹è¯•enable_thinking=Falseçš„æƒ…å†µï¼ˆçœ‹æ˜¯å¦è‡ªå‘ç”Ÿæˆ<think>ï¼‰
        """
        # å‡†å¤‡messagesåˆ—è¡¨
        messages_list = self.prepare_messages()
        
        # 1. åŸå§‹æ¨¡å‹ - enable_thinking=False
        print("\n" + "="*70)
        print("ğŸ“Š è¯„æµ‹åŸå§‹æ¨¡å‹ (enable_thinking=False)")
        print("="*70)
        base_responses = self.generate_responses(
            self.base_model_path, messages_list, enable_thinking=False
        )
        
        # 2. å¾®è°ƒæ¨¡å‹ - enable_thinking=False
        print("\n" + "="*70)
        print("ğŸ“Š è¯„æµ‹å¾®è°ƒæ¨¡å‹ (enable_thinking=False)")
        print("="*70)
        tuned_responses = self.generate_responses(
            self.tuned_model_path, messages_list, enable_thinking=False
        )
        
        # åˆ†æç»“æœ
        print("\nğŸ” åˆ†æç”Ÿæˆç»“æœ...")
        results = []
        for idx, item in enumerate(self.test_data):
            # æå–thinkingå’Œanswer
            base_think, base_ans = self.extract_think_and_answer(base_responses[idx])
            tuned_think, tuned_ans = self.extract_think_and_answer(tuned_responses[idx])
            
            result = EvaluationResult(
                index=idx,
                instruction=item['instruction'],
                # Base
                base_has_think=bool(base_think),
                base_think_length=len(base_think),
                base_answer_length=len(base_ans),
                base_response=base_responses[idx],
                # Tuned
                tuned_has_think=bool(tuned_think),
                tuned_think_length=len(tuned_think),
                tuned_answer_length=len(tuned_ans),
                tuned_response=tuned_responses[idx],
            )
            results.append(result)
        
        return results
    
    def judge_quality(self, results: List[EvaluationResult]) -> List[EvaluationResult]:
        """
        ä½¿ç”¨judge APIè¯„ä¼°thinkingå’Œanswerçš„è´¨é‡ (æ”¯æŒå¹¶å‘è°ƒç”¨)
        """
        if not self.judge_api_url or not self.judge_api_key:
            print("âš ï¸  æœªé…ç½®Judge API,è·³è¿‡è´¨é‡è¯„åˆ†")
            return results
        
        print("\n" + "="*60)
        print("ğŸ¯ ä½¿ç”¨Judge APIè¯„ä¼°è´¨é‡ (å¹¶å‘æ•°: {})".format(self.max_workers))
        print("="*60)
        print(f"Judge API: {self.judge_api_url}")
        
        total_calls = len(results) * 4  # æ¯æ¡æ•°æ®4ä¸ªè¯„åˆ†(2ä¸ªæ¨¡å‹Ã—2ä¸ªç»´åº¦)
        estimated_time = total_calls / self.max_workers * 2 / 60  # ä¼°ç®—æ—¶é—´
        print(f"ğŸ“ éœ€è¦è°ƒç”¨API {total_calls} æ¬¡ï¼Œé¢„è®¡è€—æ—¶ {estimated_time:.1f} åˆ†é’Ÿ...")
        
        # å‡†å¤‡æ‰€æœ‰è¯„åˆ†ä»»åŠ¡
        tasks = []
        for idx, result in enumerate(results):
            instruction = result.instruction
            ground_truth = self.test_data[idx]['output']
            
            # æå–thinkingå’Œanswer
            base_think, base_ans = self.extract_think_and_answer(result.base_response)
            tuned_think, tuned_ans = self.extract_think_and_answer(result.tuned_response)
            
            # åˆ›å»º4ä¸ªè¯„åˆ†ä»»åŠ¡
            tasks.extend([
                # Base - thinking
                {
                    'idx': idx,
                    'type': 'base_think',
                    'instruction': instruction,
                    'thinking': base_think,
                    'answer': base_ans,
                    'ground_truth': ground_truth,
                    'eval_type': 'thinking'
                },
                # Base - answer
                {
                    'idx': idx,
                    'type': 'base_answer',
                    'instruction': instruction,
                    'thinking': base_think,
                    'answer': base_ans,
                    'ground_truth': ground_truth,
                    'eval_type': 'answer'
                },
                # Tuned - thinking
                {
                    'idx': idx,
                    'type': 'tuned_think',
                    'instruction': instruction,
                    'thinking': tuned_think,
                    'answer': tuned_ans,
                    'ground_truth': ground_truth,
                    'eval_type': 'thinking'
                },
                # Tuned - answer
                {
                    'idx': idx,
                    'type': 'tuned_answer',
                    'instruction': instruction,
                    'thinking': tuned_think,
                    'answer': tuned_ans,
                    'ground_truth': ground_truth,
                    'eval_type': 'answer'
                },
            ])
        
        # å¹¶å‘è°ƒç”¨API
        print(f"ğŸš€ å¼€å§‹å¹¶å‘è°ƒç”¨API (æœ€å¤§å¹¶å‘æ•°: {self.max_workers})...")
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_task = {
                executor.submit(
                    self.call_judge_api,
                    task['instruction'],
                    task['thinking'],
                    task['answer'],
                    task['ground_truth'],
                    task['eval_type']
                ): task
                for task in tasks
            }
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
            with tqdm(total=len(tasks), desc="è¯„åˆ†è¿›åº¦") as pbar:
                for future in as_completed(future_to_task):
                    task = future_to_task[future]
                    try:
                        score = future.result()
                        idx = task['idx']
                        score_type = task['type']
                        
                        # æ›´æ–°ç»“æœ
                        if score_type == 'base_think':
                            results[idx].base_think_score = score
                        elif score_type == 'base_answer':
                            results[idx].base_answer_score = score
                        elif score_type == 'tuned_think':
                            results[idx].tuned_think_score = score
                        elif score_type == 'tuned_answer':
                            results[idx].tuned_answer_score = score
                        
                    except Exception as e:
                        print(f"\nâš ï¸  è¯„åˆ†ä»»åŠ¡å¤±è´¥: {e}")
                    
                    pbar.update(1)
        
        print("âœ… æ‰€æœ‰è¯„åˆ†ä»»åŠ¡å®Œæˆ!")
        return results
    
    def generate_report(self, results: List[EvaluationResult], output_dir: str):
        """
        ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print("\n" + "="*60)
        print("ğŸ“ ç”Ÿæˆè¯„æµ‹æŠ¥å‘Š")
        print("="*60)
        
        # 1. ç»Ÿè®¡æ•°æ®
        n = len(results)
        
        # <think>æ ‡ç­¾ç”Ÿæˆç‡ç»Ÿè®¡
        base_think_rate = sum(r.base_has_think for r in results) / n * 100
        tuned_think_rate = sum(r.tuned_has_think for r in results) / n * 100
        
        # å¹³å‡thinkingé•¿åº¦
        def safe_mean(values):
            filtered = [v for v in values if v > 0]
            return np.mean(filtered) if filtered else 0
        
        base_avg_len = safe_mean([r.base_think_length for r in results])
        tuned_avg_len = safe_mean([r.tuned_think_length for r in results])
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¯„åˆ†
        has_scores = results[0].base_think_score > 0
        
        # 2. ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        report = {
            "æ€»ä½“ç»Ÿè®¡": {
                "æµ‹è¯•æ ·æœ¬æ•°": n,
                "è¯„æµ‹æ¨¡å¼": "enable_thinking=Falseï¼ˆæµ‹è¯•è‡ªå‘ç”Ÿæˆèƒ½åŠ›ï¼‰",
                "åŸå§‹æ¨¡å‹(Qwen3-4B)": {
                    "<think>æ ‡ç­¾ç”Ÿæˆç‡": f"{base_think_rate:.1f}%",
                    "å¹³å‡thinkingé•¿åº¦": f"{base_avg_len:.0f} å­—ç¬¦",
                },
                "å¾®è°ƒæ¨¡å‹(Qwen3-4B-LoRA)": {
                    "<think>æ ‡ç­¾ç”Ÿæˆç‡": f"{tuned_think_rate:.1f}%",
                    "å¹³å‡thinkingé•¿åº¦": f"{tuned_avg_len:.0f} å­—ç¬¦",
                },
                "å…³é”®å‘ç°": {
                    "ç”Ÿæˆç‡æå‡": f"{tuned_think_rate - base_think_rate:+.1f}%",
                    "ç»“è®º": "âœ… å¾®è°ƒæˆåŠŸï¼å­¦ä¼šäº†è‡ªå‘ç”ŸæˆCoT" if tuned_think_rate > 80 else "âš ï¸ å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œç”Ÿæˆç‡åä½"
                }
            }
        }
        
        if has_scores:
            # è®¡ç®—å¹³å‡è¯„åˆ†
            report["è´¨é‡è¯„åˆ†(0-10åˆ†)"] = {
                "åŸå§‹æ¨¡å‹": {
                    "Thinkingè´¨é‡": f"{np.mean([r.base_think_score for r in results]):.2f}",
                    "Answerè´¨é‡": f"{np.mean([r.base_answer_score for r in results]):.2f}",
                },
                "å¾®è°ƒæ¨¡å‹": {
                    "Thinkingè´¨é‡": f"{np.mean([r.tuned_think_score for r in results]):.2f}",
                    "Answerè´¨é‡": f"{np.mean([r.tuned_answer_score for r in results]):.2f}",
                },
                "è´¨é‡æå‡": {
                    "Thinking": f"{np.mean([r.tuned_think_score for r in results]) - np.mean([r.base_think_score for r in results]):+.2f}",
                    "Answer": f"{np.mean([r.tuned_answer_score for r in results]) - np.mean([r.base_answer_score for r in results]):+.2f}",
                }
            }
        
        # 3. ä¿å­˜JSONæŠ¥å‘Š
        report_json_path = output_path / "evaluation_report.json"
        with open(report_json_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONæŠ¥å‘Šå·²ä¿å­˜: {report_json_path}")
        
        # 4. ä¿å­˜è¯¦ç»†ç»“æœåˆ°CSV
        df_data = []
        for r in results:
            row = {
                "åºå·": r.index,
                "é—®é¢˜": r.instruction[:50] + "..." if len(r.instruction) > 50 else r.instruction,
                "Base_æœ‰<think>": "âœ“" if r.base_has_think else "âœ—",
                "Base_Thinké•¿åº¦": r.base_think_length,
                "Tuned_æœ‰<think>": "âœ“" if r.tuned_has_think else "âœ—",
                "Tuned_Thinké•¿åº¦": r.tuned_think_length,
            }
            
            if has_scores:
                row.update({
                    "Base_Thinkè¯„åˆ†": f"{r.base_think_score:.1f}",
                    "Base_Answerè¯„åˆ†": f"{r.base_answer_score:.1f}",
                    "Tuned_Thinkè¯„åˆ†": f"{r.tuned_think_score:.1f}",
                    "Tuned_Answerè¯„åˆ†": f"{r.tuned_answer_score:.1f}",
                })
            
            df_data.append(row)
        
        df = pd.DataFrame(df_data)
        csv_path = output_path / "detailed_results.csv"
        df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… è¯¦ç»†ç»“æœå·²ä¿å­˜: {csv_path}")
        
        # 5. ä¿å­˜å®Œæ•´çš„å“åº”
        full_results_path = output_path / "full_responses.jsonl"
        with open(full_results_path, 'w', encoding='utf-8') as f:
            for r in results:
                f.write(json.dumps(asdict(r), ensure_ascii=False) + '\n')
        print(f"âœ… å®Œæ•´å“åº”å·²ä¿å­˜: {full_results_path}")
        
        # 6. æ‰“å°æ§åˆ¶å°æŠ¥å‘Š
        print("\n" + "="*60)
        print("ğŸ“Š è¯„æµ‹ç»“æœæ±‡æ€»")
        print("="*60)
        print(json.dumps(report, ensure_ascii=False, indent=2))
        print("\nğŸ‰ è¯„æµ‹å®Œæˆ!")
        
    def run(self, output_dir: str = "./evaluation_results"):
        """
        è¿è¡Œå®Œæ•´è¯„æµ‹æµç¨‹
        """
        # 1. åŸºç¡€è¯„æµ‹
        results = self.basic_evaluation()
        
        # 2. è´¨é‡è¯„åˆ†
        if self.judge_api_url and self.judge_api_key:
            results = self.judge_quality(results)
        
        # 3. ç”ŸæˆæŠ¥å‘Š
        self.generate_report(results, output_dir)


def main():
    parser = argparse.ArgumentParser(description="Chain-of-Thoughtæ¨¡å‹è¯„æµ‹è„šæœ¬")
    parser.add_argument(
        "--base_model",
        type=str,
        default="/data/public/models/base/Qwen/Qwen3-4B",
        help="åŸå§‹æ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--tuned_model",
        type=str,
        default="/data/private/models/qwen3_4b_merged1",
        help="å¾®è°ƒåæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--test_data",
        type=str,
        default="qwen3_sft_alpaca_format1101-1200.json",
        help="æµ‹è¯•æ•°æ®è·¯å¾„"
    )
    parser.add_argument(
        "--judge_api_url",
        type=str,
        default=None,
        help="Judge API URL"
    )
    parser.add_argument(
        "--judge_api_key",
        type=str,
        default=None,
        help="Judge API Key"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./evaluation_results",
        help="è¯„æµ‹ç»“æœè¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--max_model_len",
        type=int,
        default=2048,
        help="æ¨¡å‹æœ€å¤§é•¿åº¦"
    )
    parser.add_argument(
        "--tensor_parallel_size",
        type=int,
        default=1,
        help="å¼ é‡å¹¶è¡Œå¤§å°(å¤šå¡æ¨ç†)"
    )
    parser.add_argument(
        "--num_samples",
        type=int,
        default=None,
        help="ä½¿ç”¨å‰Næ¡æ•°æ®æµ‹è¯•(None=å…¨éƒ¨)"
    )
    parser.add_argument(
        "--max_workers",
        type=int,
        default=15,
        help="Judge APIå¹¶å‘è°ƒç”¨æ•°(é»˜è®¤15)"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¯„æµ‹å™¨
    evaluator = CoTEvaluator(
        base_model_path=args.base_model,
        tuned_model_path=args.tuned_model,
        test_data_path=args.test_data,
        judge_api_url=args.judge_api_url,
        judge_api_key=args.judge_api_key,
        max_model_len=args.max_model_len,
        tensor_parallel_size=args.tensor_parallel_size,
        num_samples=args.num_samples,
        max_workers=args.max_workers,
    )
    
    # è¿è¡Œè¯„æµ‹
    evaluator.run(output_dir=args.output_dir)


if __name__ == "__main__":
    main()

