{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7019965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (4.0.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2025.3.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86473e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: modelscope in /usr/local/lib/python3.10/dist-packages (1.31.0)\n",
      "Requirement already satisfied: requests>=2.25 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.32.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from modelscope) (3.20.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from modelscope) (59.6.0)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from modelscope) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26 in /usr/local/lib/python3.10/dist-packages (from modelscope) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (2024.2.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.25->modelscope) (3.3.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fb5f2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _   .-')                _ .-') _     ('-.             .-')                              _ (`-.    ('-.\n",
      "( '.( OO )_             ( (  OO) )  _(  OO)           ( OO ).                           ( (OO  ) _(  OO)\n",
      " ,--.   ,--.).-'),-----. \\     .'_ (,------.,--.     (_)---\\_)   .-----.  .-'),-----.  _.`     \\(,------.\n",
      " |   `.'   |( OO'  .-.  ',`'--..._) |  .---'|  |.-') /    _ |   '  .--./ ( OO'  .-.  '(__...--'' |  .---'\n",
      " |         |/   |  | |  ||  |  \\  ' |  |    |  | OO )\\  :` `.   |  |('-. /   |  | |  | |  /  | | |  |\n",
      " |  |'.'|  |\\_) |  |\\|  ||  |   ' |(|  '--. |  |`-' | '..`''.) /_) |OO  )\\_) |  |\\|  | |  |_.' |(|  '--.\n",
      " |  |   |  |  \\ |  | |  ||  |   / : |  .--'(|  '---.'.-._)   \\ ||  |`-'|   \\ |  | |  | |  .___.' |  .--'\n",
      " |  |   |  |   `'  '-'  '|  '--'  / |  `---.|      | \\       /(_'  '--'\\    `'  '-'  ' |  |      |  `---.\n",
      " `--'   `--'     `-----' `-------'  `------'`------'  `-----'    `-----'      `-----'  `--'      `------'\n",
      "\n",
      "Downloading Dataset to directory: /root/.cache/modelscope/hub/datasets/swift/Chinese-Qwen3-235B-Thinking-2507-Distill-data-110k-SFT\n",
      "Processing 4 items:   0%|                           | 0.00/4.00 [00:00<?, ?it/s]\n",
      "Downloading [.gitattributes]:   0%|                 | 0.00/3.73k [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "Downloading [dataset_infos.json]:   0%|               | 0.00/173 [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 0.00/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]:   0%|                      | 0.00/15.0k [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "Downloading [.gitattributes]: 100%|████████| 3.73k/3.73k [00:00<00:00, 4.59kB/s]\u001b[A\n",
      "Processing 4 items:  25%|█████               | 1.00/4.00 [00:00<00:02, 1.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "Downloading [README.md]: 100%|█████████████| 15.0k/15.0k [00:00<00:00, 17.3kB/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [dataset_infos.json]: 100%|██████████| 173/173 [00:00<00:00, 179B/s]\u001b[A\u001b[A\n",
      "Processing 4 items:  75%|███████████████     | 3.00/4.00 [00:00<00:00, 3.76it/s]\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 1.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 2.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 3.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 4.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 5.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 6.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 7.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   0%| | 8.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 9.00M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 10.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 11.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 12.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 13.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 14.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 15.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 16.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 17.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 18.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 19.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 20.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 21.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 22.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 23.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   1%| | 24.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 25.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 26.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 27.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 28.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 29.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 30.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 31.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 32.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 33.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 34.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 35.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 36.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 37.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 38.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 39.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 40.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   2%| | 41.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 42.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 43.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 44.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 45.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 46.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 47.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 48.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 49.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 50.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 51.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 52.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 53.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 54.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 55.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 56.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   3%| | 57.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 58.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 59.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 60.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 61.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 62.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 63.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 64.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 65.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 66.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 67.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 68.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 69.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 70.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 71.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 72.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 73.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   4%| | 74.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 75.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 76.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 77.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 78.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 79.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 80.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 81.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 82.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 83.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 84.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 85.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 86.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 87.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 88.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 89.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   5%| | 90.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 91.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 92.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 93.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 94.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 95.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 96.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 97.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 98.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 99.0M/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 100M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 101M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 102M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 103M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 104M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 105M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 106M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   6%| | 107M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 108M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 109M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 110M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 111M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 112M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 113M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 114M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 115M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 116M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 117M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 118M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 119M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 120M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 121M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 122M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   7%| | 123M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 124M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 125M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 126M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 127M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 128M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 129M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 130M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 131M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 132M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 133M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 134M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 135M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 136M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 137M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 138M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 139M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   8%| | 140M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 141M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 142M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 143M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 144M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 145M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 146M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 147M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 148M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 149M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 150M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 151M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 152M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 153M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 154M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 155M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:   9%| | 156M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 157M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 158M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 159M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 160M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 161M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 162M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 163M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 164M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 165M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 166M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 167M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 168M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 169M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 170M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 171M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  10%| | 172M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 173M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 174M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 175M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 176M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 177M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 178M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 179M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 180M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 181M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 182M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 183M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 184M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 185M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 186M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 187M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 188M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  11%| | 189M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 190M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 191M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 192M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 193M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 194M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 195M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 196M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 197M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 198M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 199M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 200M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 201M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 202M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 203M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 204M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  12%| | 205M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 206M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 207M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 208M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 209M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 210M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 211M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 212M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 213M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 214M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 215M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 216M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 217M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 218M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 219M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 220M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 221M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  13%|▏| 222M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 223M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 224M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 225M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 226M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 227M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 228M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 229M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 230M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 231M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 232M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 233M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 234M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 235M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 236M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 237M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  14%|▏| 238M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 239M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 240M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 241M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 242M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 243M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 244M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 245M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 246M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 247M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 248M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 249M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 250M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 251M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 252M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 253M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 254M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  15%|▏| 255M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 256M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 257M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 258M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 259M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 260M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 261M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 262M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 263M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 264M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 265M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 266M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 267M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 268M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 269M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 270M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  16%|▏| 271M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 272M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 273M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 274M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 275M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 276M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 277M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 278M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 279M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 280M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 281M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 282M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 283M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 284M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 285M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 286M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 287M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  17%|▏| 288M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 289M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 290M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 291M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 292M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 293M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 294M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 295M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 296M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 297M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 298M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 299M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 300M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 301M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 302M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 303M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  18%|▏| 304M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 305M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 306M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 307M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 308M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 309M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 310M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 311M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 312M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 313M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 314M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 315M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 316M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 317M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 318M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 319M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 320M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  19%|▏| 321M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 322M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 323M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 324M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 325M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 326M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 327M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 328M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 329M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 330M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 331M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 332M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 333M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 334M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 335M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 336M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  20%|▏| 337M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 338M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 339M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 340M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 341M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 342M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 343M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 344M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 345M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 346M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 347M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 348M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 349M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 350M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 351M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 352M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 353M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  21%|▏| 354M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 355M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 356M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 357M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 358M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 359M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 360M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 361M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 362M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 363M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 364M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 365M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 366M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 367M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 368M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 369M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  22%|▏| 370M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 371M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 372M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 373M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 374M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 375M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 376M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 377M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 378M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 379M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 380M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 381M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 382M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 383M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 384M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 385M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 386M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  23%|▏| 387M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 388M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 389M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 390M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 391M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 392M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 393M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 394M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 395M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 396M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 397M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 398M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 399M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 400M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 401M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 402M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  24%|▏| 403M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 404M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 405M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 406M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 407M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 408M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 409M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 410M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▏| 411M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 412M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 413M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 414M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 415M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 416M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 417M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 418M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 419M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  25%|▎| 420M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 421M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 422M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 423M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 424M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 425M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 426M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 427M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 428M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 429M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 430M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 431M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 432M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 433M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 434M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 435M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  26%|▎| 436M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 437M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 438M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 439M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 440M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 441M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 442M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 443M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 444M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 445M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 446M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 447M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 448M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 449M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 450M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 451M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 452M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  27%|▎| 453M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 454M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 455M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 456M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 457M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 458M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 459M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 460M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 461M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 462M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 463M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 464M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 465M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 466M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 467M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 468M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  28%|▎| 469M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 470M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 471M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 472M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 473M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 474M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 475M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 476M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 477M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 478M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 479M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 480M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 481M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 482M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 483M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 484M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  29%|▎| 485M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 486M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 487M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 488M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 489M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 490M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 491M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 492M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 493M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 494M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 495M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 496M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 497M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 498M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 499M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 500M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 501M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  30%|▎| 502M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 503M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 504M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 505M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 506M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 507M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 508M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 509M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 510M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 511M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 512M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 513M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 514M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 515M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 516M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 517M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  31%|▎| 518M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 519M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 520M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 521M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 522M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 523M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 524M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 525M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 526M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 527M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 528M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 529M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 530M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 531M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 532M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 533M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 534M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  32%|▎| 535M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 536M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 537M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 538M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 539M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 540M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 541M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 542M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 543M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 544M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 545M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 546M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 547M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 548M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 549M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 550M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  33%|▎| 551M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 552M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 553M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 554M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 555M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 556M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 557M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 558M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 559M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 560M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 561M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 562M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 563M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 564M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 565M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 566M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 567M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  34%|▎| 568M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 569M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 570M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 571M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 572M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 573M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 574M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 575M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 576M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 577M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 578M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 579M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 580M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 581M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 582M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 583M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  35%|▎| 584M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 585M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 586M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 587M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 588M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 589M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 590M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 591M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 592M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 593M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 594M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 595M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 596M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 597M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 598M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 599M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 600M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  36%|▎| 601M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 602M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 603M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 604M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 605M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 606M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 607M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 608M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 609M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 610M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 611M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 612M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 613M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 614M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 615M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 616M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  37%|▎| 617M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 618M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 619M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 620M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 621M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 622M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 623M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 624M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 625M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 626M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 627M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 628M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 629M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 630M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 631M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 632M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 633M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  38%|▍| 634M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 635M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 636M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 637M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 638M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 639M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 640M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 641M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 642M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 643M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 644M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 645M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 646M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 647M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 648M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 649M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  39%|▍| 650M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 651M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 652M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 653M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 654M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 655M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 656M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 657M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 658M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 659M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 660M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 661M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 662M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 663M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 664M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 665M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 666M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  40%|▍| 667M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 668M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 669M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 670M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 671M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 672M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 673M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 674M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 675M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 676M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 677M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 678M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 679M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 680M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 681M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 682M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  41%|▍| 683M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 684M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 685M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 686M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 687M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 688M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 689M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 690M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 691M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 692M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 693M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 694M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 695M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 696M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 697M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 698M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 699M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  42%|▍| 700M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 701M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 702M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 703M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 704M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 705M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 706M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 707M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 708M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 709M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 710M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 711M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 712M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 713M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 714M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 715M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  43%|▍| 716M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 717M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 718M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 719M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 720M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 721M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 722M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 723M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 724M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 725M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 726M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 727M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 728M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 729M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 730M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 731M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 732M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  44%|▍| 733M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 734M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 735M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 736M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 737M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 738M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 739M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 740M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 741M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 742M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 743M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 744M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 745M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 746M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 747M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 748M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  45%|▍| 749M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 750M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 751M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 752M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 753M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 754M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 755M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 756M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 757M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 758M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 759M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 760M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 761M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 762M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 763M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 764M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 765M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  46%|▍| 766M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 767M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 768M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 769M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 770M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 771M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 772M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 773M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 774M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 775M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 776M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 777M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 778M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 779M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 780M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 781M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  47%|▍| 782M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 783M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 784M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 785M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 786M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 787M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 788M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 789M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  48%|▍| 790M/1.61G [0\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.54G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.54G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  96%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.55G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.56G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  97%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.57G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  98%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.58G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.59G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]:  99%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.60G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.61G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.61G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.61G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 100%|▉| 1.61G/1.61G [\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:55, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:56, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:56, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:56, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:57, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.61GB [04:57, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:57, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:57, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:58, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:58, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:58, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:58, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:59, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:59, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:59, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [04:59, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.62GB [05:00, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:00, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:01, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:01, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:01, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:01, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:02, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:02, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:02, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.63GB [05:02, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:03, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:03, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:03, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:04, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:04, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:04, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:04, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:05, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:05, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.64GB [05:05, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:05, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:06, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:06, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:06, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:07, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:07, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:07, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:07, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:08, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.65GB [05:08, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:08, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:08, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:09, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:09, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:09, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:10, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:10, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:10, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:10, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.66GB [05:11, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:11, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:11, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:11, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:12, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:12, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:12, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:13, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:13, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:13, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:13, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.67GB [05:14, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:14, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:14, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:14, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:15, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:15, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:16, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:16, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.68GB [05:16, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:17, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:17, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:17, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:17, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:18, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:18, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:18, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:18, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:19, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.69GB [05:19, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:19, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:20, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:20, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:21, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:21, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:21, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:21, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.70GB [05:22, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:22, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:22, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:23, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:23, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:24, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:24, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:24, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:24, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.71GB [05:25, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:25, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:25, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:26, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:26, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:26, 3.82MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:26, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:27, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:27, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:27, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.72GB [05:27, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:28, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:28, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:28, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:29, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:29, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:29, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:29, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:30, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:30, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.73GB [05:30, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:30, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:31, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:31, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:31, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:31, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:32, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:32, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:32, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:33, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.74GB [05:33, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:33, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:33, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:34, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:34, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:34, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:34, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:35, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:35, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:35, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:36, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.75GB [05:36, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:36, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:36, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:37, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:37, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:37, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:37, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:38, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:38, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:38, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.76GB [05:39, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:39, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:39, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:39, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:40, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:40, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:40, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:40, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:41, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:41, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.77GB [05:41, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:42, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:42, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:42, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:42, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:43, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:43, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:43, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:43, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:44, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.78GB [05:44, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:44, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:45, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:45, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:45, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:45, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:46, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:46, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:46, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:46, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:47, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.79GB [05:47, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:47, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:47, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:48, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:48, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:48, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:49, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:49, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:49, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:49, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.80GB [05:50, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:50, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:50, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:50, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:51, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:51, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:51, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:52, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:52, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:52, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.81GB [05:52, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:53, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:53, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:53, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:53, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:54, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:54, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:54, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:55, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:55, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.82GB [05:55, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:55, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:56, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:56, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:56, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:56, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:57, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:57, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:57, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:58, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:58, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.83GB [05:58, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [05:58, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [05:59, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [05:59, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [05:59, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [05:59, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [06:00, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [06:00, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [06:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [06:01, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.84GB [06:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:01, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:01, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:02, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:02, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:02, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:02, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:03, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:03, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:03, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.85GB [06:03, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:04, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:04, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:04, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:05, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:05, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:05, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:05, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:06, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:06, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.86GB [06:06, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:06, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:07, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:07, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:07, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:08, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:08, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:08, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:08, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:09, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.87GB [06:09, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:09, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:09, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:10, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:10, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:10, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:11, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:11, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:11, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:11, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:12, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.88GB [06:12, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:12, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:12, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:13, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:13, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:13, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:14, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:14, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:14, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:14, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.89GB [06:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:15, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:15, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:16, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:16, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:16, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:16, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:17, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:17, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.90GB [06:17, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:18, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:18, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:18, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:18, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:19, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:19, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:19, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:19, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:20, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.91GB [06:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:20, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:21, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:21, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:21, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:21, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:22, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:22, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:22, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:22, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:23, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.92GB [06:23, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:24, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:24, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:24, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:24, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:25, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:25, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:25, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:25, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.93GB [06:26, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:26, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:26, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:27, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:27, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:27, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:27, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:28, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:28, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:28, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.94GB [06:28, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:29, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:29, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:29, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:30, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:30, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:30, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:30, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:31, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:31, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.95GB [06:31, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:31, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:32, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:32, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:32, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:32, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:33, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:33, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:33, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:34, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:34, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.96GB [06:34, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:34, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:35, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:35, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:35, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:35, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:36, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:36, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:36, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:37, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.97GB [06:37, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:37, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:37, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:38, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:38, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:38, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:38, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:39, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:39, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:39, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.98GB [06:40, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:40, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:40, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:40, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:41, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:41, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:41, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:41, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:42, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:42, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 1.99GB [06:42, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:43, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:43, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:43, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:43, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:44, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:44, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:44, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:44, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:45, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:45, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.00GB [06:45, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:46, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:46, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:46, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:46, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:47, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:47, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:47, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:47, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:48, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.01GB [06:48, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:48, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:49, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:49, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:49, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:49, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:50, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:50, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:50, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:50, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.02GB [06:51, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:51, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:51, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:51, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:52, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:52, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:52, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:53, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:53, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:53, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.03GB [06:53, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:54, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:54, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:54, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:54, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:55, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:55, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:55, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:56, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:56, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:56, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.04GB [06:56, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:57, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:57, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:57, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:57, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:58, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:58, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:58, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:59, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:59, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.05GB [06:59, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [06:59, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:00, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:00, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:01, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:02, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.06GB [07:02, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:02, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:02, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:03, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:03, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:03, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:03, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:04, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:04, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:04, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.07GB [07:05, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:05, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:05, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:05, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:06, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:06, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:06, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:06, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:07, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:07, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:07, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.08GB [07:07, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:08, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:08, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:08, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:09, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:09, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:09, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:09, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:10, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:10, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.09GB [07:10, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:10, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:11, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:11, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:11, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:12, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:12, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:12, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:12, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:13, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.10GB [07:13, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:13, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:13, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:14, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:14, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:14, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:15, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:15, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:15, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.11GB [07:16, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:16, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:16, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:16, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:17, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:17, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:17, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:18, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:18, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:18, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:18, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.12GB [07:19, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:19, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:19, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:19, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:20, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:20, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:21, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:21, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:21, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.13GB [07:21, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:22, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:22, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:22, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:22, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:23, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:23, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:24, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.14GB [07:24, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:24, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:25, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:25, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:25, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:25, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:26, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:26, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:26, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:26, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.15GB [07:27, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:27, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:27, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:28, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:28, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:28, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:28, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:29, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:29, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:29, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.16GB [07:29, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:30, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:30, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:30, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:31, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:31, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:31, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:31, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:32, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:32, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:32, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.17GB [07:32, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:33, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:33, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:33, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:34, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:34, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:34, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:34, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:35, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:35, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.18GB [07:35, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:35, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:36, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:36, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:36, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:37, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:37, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:37, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:37, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:38, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.19GB [07:38, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:38, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:38, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:39, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:39, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:39, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:39, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:40, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:40, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:40, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.20GB [07:41, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:41, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:41, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:41, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:42, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:42, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:42, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:42, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:43, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:43, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:43, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.21GB [07:44, 3.82MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:44, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:44, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:44, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:45, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:45, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:45, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:45, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:46, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:46, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.22GB [07:46, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:47, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:47, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:47, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:47, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:48, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:48, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:48, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:48, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:49, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.23GB [07:49, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:49, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:50, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:50, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:50, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:50, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:51, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:51, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:51, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:51, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.24GB [07:52, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:52, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:52, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:53, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:53, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:53, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:53, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:54, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:54, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:54, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:54, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.25GB [07:55, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:55, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:55, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:55, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:56, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:56, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:56, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:57, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:57, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:57, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.26GB [07:57, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:58, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:58, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:58, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:58, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:59, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:59, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [07:59, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [08:00, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [08:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.27GB [08:00, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:00, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:01, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:01, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:02, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:02, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:02, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:03, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.28GB [08:03, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:03, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:03, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:04, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:04, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:04, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:04, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:05, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:05, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:05, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:06, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.29GB [08:06, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:06, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:06, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:07, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:07, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:07, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:07, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:08, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:08, 3.82MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:08, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.30GB [08:09, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:09, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:09, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:09, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:10, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:10, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:10, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:10, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:11, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:11, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.31GB [08:11, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:12, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:12, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:12, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:12, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:13, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:13, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:13, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:13, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:14, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.32GB [08:14, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:14, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:14, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:15, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:15, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:15, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:16, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:16, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:16, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:16, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:17, 3.79MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.33GB [08:17, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:17, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:17, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:18, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:18, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:18, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:19, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:19, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:19, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:19, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.34GB [08:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:20, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:20, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:20, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:21, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:21, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:21, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:22, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:22, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:22, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.35GB [08:22, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:23, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:23, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:23, 3.85MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:23, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:24, 3.83MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:24, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:24, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:25, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:25, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.36GB [08:25, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:25, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:26, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:26, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:26, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:26, 3.86MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:27, 3.88MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:27, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:27, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:28, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.37GB [08:28, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:28, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:28, 3.89MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:29, 3.84MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:29, 3.87MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:29, 3.90MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "Downloading [qwen3_235b_thinking_2507_distill_110k.jsonl]: 2.38GB [08:30, 5.01MB/s]\u001b[A\u001b[A\u001b[A\n",
      "Processing 4 items: 100%|█████████████████████| 4.00/4.00 [08:31<00:00, 128s/it]\n",
      "\n",
      "Successfully Downloaded from dataset swift/Chinese-Qwen3-235B-Thinking-2507-Distill-data-110k-SFT.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!modelscope download --dataset swift/Chinese-Qwen3-235B-Thinking-2507-Distill-data-110k-SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "033aeaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: The HTML index page being used (http://mirrors.aliyun.com/pypi/simple/jsonlines/) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be well-formed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install requests tqdm jsonlines -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e46f4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 限流器和统计器类已加载\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# ==================== 限流器 ====================\n",
    "\n",
    "class RateLimiter:\n",
    "    \"\"\"令牌桶限流器 - 控制QPS\"\"\"\n",
    "    def __init__(self, max_calls, period=1.0):\n",
    "        self.max_calls = max_calls\n",
    "        self.period = period\n",
    "        self.calls = []\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def wait(self):\n",
    "        \"\"\"等待获取令牌\"\"\"\n",
    "        with self.lock:\n",
    "            now = time.time()\n",
    "            # 清理过期的调用记录\n",
    "            self.calls = [call for call in self.calls if call > now - self.period]\n",
    "            \n",
    "            # 如果达到限制，等待\n",
    "            if len(self.calls) >= self.max_calls:\n",
    "                sleep_time = self.period - (now - self.calls[0])\n",
    "                if sleep_time > 0:\n",
    "                    time.sleep(sleep_time)\n",
    "                    now = time.time()\n",
    "                    self.calls = [call for call in self.calls if call > now - self.period]\n",
    "            \n",
    "            self.calls.append(time.time())\n",
    "\n",
    "\n",
    "# ==================== 统计收集器 ====================\n",
    "\n",
    "class StatisticsCollector:\n",
    "    \"\"\"线程安全的统计收集器\"\"\"\n",
    "    def __init__(self):\n",
    "        self.lock = threading.Lock()\n",
    "        self.success_count = 0\n",
    "        self.failed_count = 0\n",
    "        self.error_types = defaultdict(int)\n",
    "        self.start_time = time.time()\n",
    "        self.api_times = []  # 记录API响应时间\n",
    "    \n",
    "    def add_success(self, api_time=0):\n",
    "        with self.lock:\n",
    "            self.success_count += 1\n",
    "            if api_time > 0:\n",
    "                self.api_times.append(api_time)\n",
    "    \n",
    "    def add_failure(self, error_msg):\n",
    "        with self.lock:\n",
    "            self.failed_count += 1\n",
    "            # 提取错误类型\n",
    "            if '429' in str(error_msg):\n",
    "                self.error_types['429限流'] += 1\n",
    "            elif '超时' in str(error_msg):\n",
    "                self.error_types['请求超时'] += 1\n",
    "            elif '网络' in str(error_msg) or 'Connection' in str(error_msg):\n",
    "                self.error_types['网络错误'] += 1\n",
    "            else:\n",
    "                self.error_types['其他错误'] += 1\n",
    "    \n",
    "    def get_stats(self):\n",
    "        with self.lock:\n",
    "            total = self.success_count + self.failed_count\n",
    "            success_rate = (self.success_count / total * 100) if total > 0 else 0\n",
    "            elapsed = time.time() - self.start_time\n",
    "            speed = total / elapsed if elapsed > 0 else 0\n",
    "            avg_api_time = sum(self.api_times) / len(self.api_times) if self.api_times else 0\n",
    "            \n",
    "            return {\n",
    "                'success': self.success_count,\n",
    "                'failed': self.failed_count,\n",
    "                'total': total,\n",
    "                'success_rate': success_rate,\n",
    "                'elapsed': elapsed,\n",
    "                'speed': speed,\n",
    "                'avg_api_time': avg_api_time,\n",
    "                'error_types': dict(self.error_types)\n",
    "            }\n",
    "    \n",
    "    def reset(self):\n",
    "        with self.lock:\n",
    "            self.success_count = 0\n",
    "            self.failed_count = 0\n",
    "            self.error_types.clear()\n",
    "            self.start_time = time.time()\n",
    "            self.api_times.clear()\n",
    "\n",
    "\n",
    "# ==================== 初始化全局实例 ====================\n",
    "\n",
    "# 根据配置创建限流器（后面会用到配置）\n",
    "# rate_limiter 将在 Cell 5 之后创建\n",
    "\n",
    "print(\"✅ 限流器和统计器类已加载\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e88e88cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📋 推理配置\n",
      "============================================================\n",
      "数据集路径: /data/private/datasets/qwen3_235b_thinking_2507_distill_110k.jsonl\n",
      "输出路径: /data/private/outputs/qwen3_inference_1000_samples.jsonl\n",
      "\n",
      "并发配置:\n",
      "  - 并发线程数: 20\n",
      "  - QPS限制: 15 请求/秒\n",
      "  - 重试次数: 5\n",
      "  - 请求超时: 30 秒\n",
      "\n",
      "推理配置:\n",
      "  - 样本数量: 1000\n",
      "  - 保留思考: 是\n",
      "  - 最大Tokens: 1024\n",
      "\n",
      "预计完成时间: ~2-3分钟（不含API响应时间）\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ========== 路径配置 ==========\n",
    "\n",
    "# ==================== 路径配置 ====================\n",
    "\n",
    "# 工作目录\n",
    "WORK_DIR = \"/data/private\"\n",
    "\n",
    "# 数据集文件夹和路径\n",
    "DATASET_DIR = f\"{WORK_DIR}/datasets\"\n",
    "DATASET_PATH = f\"{DATASET_DIR}/qwen3_235b_thinking_2507_distill_110k.jsonl\"\n",
    "\n",
    "# 输出文件夹和路径\n",
    "OUTPUT_DIR = f\"{WORK_DIR}/outputs\"\n",
    "OUTPUT_PATH = f\"{OUTPUT_DIR}/qwen3_inference_1000_samples.jsonl\"\n",
    "\n",
    "# ==================== API配置 ====================\n",
    "\n",
    "API_KEY = \"msk-c3e6e836acff79160513e563d6d288d3e24b9605349a03bb9564e9a7b3bafefe\"\n",
    "API_URL = \"https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\"\n",
    "\n",
    "# ==================== 推理配置（平衡方案）====================\n",
    "\n",
    "SAMPLE_SIZE = 1000          # 选取的样本数量\n",
    "MAX_WORKERS = 20            # 并发线程数（从60降到20）\n",
    "MAX_QPS = 15                # 每秒最大请求数（新增）\n",
    "MAX_TOKENS = 1024           # 最大生成token数\n",
    "MAX_RETRIES = 5             # 失败重试次数（从3增到5）\n",
    "TEMPERATURE = 0.7           # 温度参数\n",
    "TOP_P = 0.9                 # Top-p 采样\n",
    "REQUEST_TIMEOUT = 30        # 请求超时时间（从120降到30）\n",
    "\n",
    "# ==================== 报告配置 ====================\n",
    "\n",
    "REPORT_INTERVAL = 50        # 每多少条打印阶段报告\n",
    "SHOW_SUCCESS_SAMPLE = True  # 是否显示成功样例\n",
    "ENABLE_THINKING = True      # 是否保留思考内容\n",
    "\n",
    "# ==================== 创建限流器 ====================\n",
    "\n",
    "rate_limiter = RateLimiter(max_calls=MAX_QPS, period=1.0)\n",
    "stats_collector = StatisticsCollector()\n",
    "\n",
    "# ==================== 打印配置信息 ====================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📋 推理配置\")\n",
    "print(\"=\"*60)\n",
    "print(f\"数据集路径: {DATASET_PATH}\")\n",
    "print(f\"输出路径: {OUTPUT_PATH}\")\n",
    "print(f\"\\n并发配置:\")\n",
    "print(f\"  - 并发线程数: {MAX_WORKERS}\")\n",
    "print(f\"  - QPS限制: {MAX_QPS} 请求/秒\")\n",
    "print(f\"  - 重试次数: {MAX_RETRIES}\")\n",
    "print(f\"  - 请求超时: {REQUEST_TIMEOUT} 秒\")\n",
    "print(f\"\\n推理配置:\")\n",
    "print(f\"  - 样本数量: {SAMPLE_SIZE}\")\n",
    "print(f\"  - 保留思考: {'是' if ENABLE_THINKING else '否'}\")\n",
    "print(f\"  - 最大Tokens: {MAX_TOKENS}\")\n",
    "print(f\"\\n预计完成时间: ~2-3分钟（不含API响应时间）\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92c3de04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 已创建数据集文件夹: /data/private/datasets\n",
      "✓ 已创建输出文件夹: /data/private/outputs\n",
      "\n",
      "✓ 数据集已存在: /data/private/datasets/qwen3_235b_thinking_2507_distill_110k.jsonl\n",
      "  文件大小: 1647.36 MB\n"
     ]
    }
   ],
   "source": [
    "## 准备数据集和输出文件夹\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# 创建文件夹\n",
    "Path(DATASET_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"✓ 已创建数据集文件夹: {DATASET_DIR}\")\n",
    "print(f\"✓ 已创建输出文件夹: {OUTPUT_DIR}\")\n",
    "\n",
    "# 检查原始数据集是否存在\n",
    "original_dataset_path = \"/root/.cache/modelscope/hub/datasets/swift/Chinese-Qwen3-235B-Thinking-2507-Distill-data-110k-SFT/qwen3_235b_thinking_2507_distill_110k.jsonl\"\n",
    "\n",
    "if os.path.exists(original_dataset_path):\n",
    "    # 如果目标位置还没有数据集，则复制过去\n",
    "    if not os.path.exists(DATASET_PATH):\n",
    "        print(f\"\\n正在复制数据集到 {DATASET_PATH} ...\")\n",
    "        shutil.copy2(original_dataset_path, DATASET_PATH)\n",
    "        print(f\"✓ 数据集复制完成\")\n",
    "    else:\n",
    "        print(f\"\\n✓ 数据集已存在: {DATASET_PATH}\")\n",
    "    \n",
    "    # 显示文件大小\n",
    "    file_size = os.path.getsize(DATASET_PATH) / (1024 * 1024)  # MB\n",
    "    print(f\"  文件大小: {file_size:.2f} MB\")\n",
    "else:\n",
    "    print(f\"\\n⚠ 警告: 原始数据集不存在于 {original_dataset_path}\")\n",
    "    print(\"  请确认数据集已下载，或手动指定正确的路径\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5b7673",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始读取数据集: /data/private/datasets/qwen3_235b_thinking_2507_distill_110k.jsonl\n",
      "第一条数据类型: <class 'dict'>\n",
      "数据结构示例: {\"messages\": [{\"content\": \"能给我讲一个寓意深刻的故事吗？\", \"role\": \"user\"}, {\"content\": \"<think>\\n嗯，用户想要一个寓意深刻的故事。这个请求看似简单，但背后可能有几种需求：ta可能正在寻找人生启发，或是需要素材用于教学分享，也可能只是睡前想读点有深度的内容。从“寓意深刻”这个关键词看，ta应该厌倦了快餐式鸡汤，想要能引发真实思考的...\n",
      "\n",
      "\n",
      "成功读取 1000 条问题\n",
      "示例问题: 能给我讲一个寓意深刻的故事吗？...\n"
     ]
    }
   ],
   "source": [
    "#读取数据集\n",
    "def load_dataset(file_path, sample_size=1000):\n",
    "    \"\"\"读取JSONL数据集并提取前N条的user问题\"\"\"\n",
    "    questions = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if idx >= sample_size:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # 调试：打印第一条数据的类型和结构\n",
    "                if idx == 0:\n",
    "                    print(f\"第一条数据类型: {type(data)}\")\n",
    "                    print(f\"数据结构示例: {json.dumps(data, ensure_ascii=False)[:200]}...\\n\")\n",
    "                \n",
    "                # 提取 messages 列表\n",
    "                messages = None\n",
    "                if isinstance(data, dict) and 'messages' in data:\n",
    "                    messages = data['messages']\n",
    "                elif isinstance(data, list):\n",
    "                    messages = data\n",
    "                else:\n",
    "                    if idx < 3:  # 只打印前3条的警告\n",
    "                        print(f\"警告：第 {idx} 行格式不支持，跳过\")\n",
    "                    continue\n",
    "                \n",
    "                # 遍历 messages 提取 user 的问题\n",
    "                for message in messages:\n",
    "                    if not isinstance(message, dict):\n",
    "                        continue\n",
    "                    \n",
    "                    if message.get('role') == 'user':\n",
    "                        questions.append({\n",
    "                            'index': idx,\n",
    "                            'question': message.get('content')\n",
    "                        })\n",
    "                        break\n",
    "                        \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"第 {idx} 行 JSON 解析失败: {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"第 {idx} 行处理失败: {e}\")\n",
    "                continue\n",
    "    \n",
    "    return questions\n",
    "\n",
    "print(f\"开始读取数据集: {DATASET_PATH}\")\n",
    "questions = load_dataset(DATASET_PATH, SAMPLE_SIZE)\n",
    "print(f\"\\n成功读取 {len(questions)} 条问题\")\n",
    "if questions:\n",
    "    print(f\"示例问题: {questions[0]['question'][:50]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6b481de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API调用函数已更新（集成限流和智能重试）\n"
     ]
    }
   ],
   "source": [
    "#定义api调用函数\n",
    "def call_qwen3_api(question, api_key, api_url, max_retries=MAX_RETRIES):\n",
    "    \"\"\"\n",
    "    调用Qwen3 API进行推理（增强版 - 集成限流和智能重试）\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"/model/qwen3-235b-a22b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": MAX_TOKENS,\n",
    "        \"temperature\": TEMPERATURE,\n",
    "        \"top_p\": TOP_P,\n",
    "        \"stream\": False,\n",
    "        \"chat_template_kwargs\": {\n",
    "            \"enable_thinking\": ENABLE_THINKING\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    api_start_time = time.time()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # 🔥 QPS限流：在发送请求前等待令牌\n",
    "            rate_limiter.wait()\n",
    "            \n",
    "            # 发送请求\n",
    "            response = requests.post(\n",
    "                api_url, \n",
    "                headers=headers, \n",
    "                json=payload, \n",
    "                timeout=REQUEST_TIMEOUT\n",
    "            )\n",
    "            \n",
    "            # 🔥 特殊处理429错误\n",
    "            if response.status_code == 429:\n",
    "                if attempt < max_retries - 1:\n",
    "                    # 429错误使用更长的等待时间：5秒、10秒、15秒、20秒\n",
    "                    wait_time = min((attempt + 1) * 5, 20)\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    return {\n",
    "                        'success': False, \n",
    "                        'error': f'429限流（已重试{max_retries}次）',\n",
    "                        'api_time': time.time() - api_start_time\n",
    "                    }\n",
    "            \n",
    "            # 检查其他HTTP错误\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # 解析响应\n",
    "            result = response.json()\n",
    "            \n",
    "            # 提取回复内容\n",
    "            if 'choices' in result and len(result['choices']) > 0:\n",
    "                message = result['choices'][0]['message']\n",
    "                api_time = time.time() - api_start_time\n",
    "                \n",
    "                return {\n",
    "                    'success': True,\n",
    "                    'content': message.get('content', ''),\n",
    "                    'reasoning_content': message.get('reasoning_content', ''),\n",
    "                    'api_time': api_time\n",
    "                }\n",
    "            else:\n",
    "                return {\n",
    "                    'success': False, \n",
    "                    'error': '响应格式错误',\n",
    "                    'api_time': time.time() - api_start_time\n",
    "                }\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(2)  # 超时后等待2秒\n",
    "                continue\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': '请求超时',\n",
    "                'api_time': time.time() - api_start_time\n",
    "            }\n",
    "            \n",
    "        except requests.exceptions.ConnectionError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(3)  # 网络错误等待3秒\n",
    "                continue\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': f'网络连接错误: {str(e)[:50]}',\n",
    "                'api_time': time.time() - api_start_time\n",
    "            }\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # 其他请求异常\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = 2 ** attempt  # 指数退避\n",
    "                time.sleep(wait_time)\n",
    "                continue\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': f'请求异常: {str(e)[:50]}',\n",
    "                'api_time': time.time() - api_start_time\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\n",
    "                'success': False, \n",
    "                'error': f'未知错误: {str(e)[:50]}',\n",
    "                'api_time': time.time() - api_start_time\n",
    "            }\n",
    "    \n",
    "    return {\n",
    "        'success': False, \n",
    "        'error': f'达到最大重试次数({max_retries})',\n",
    "        'api_time': time.time() - api_start_time\n",
    "    }\n",
    "\n",
    "print(\"✅ API调用函数已更新（集成限流和智能重试）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "329b37b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 \n",
      "\n",
      "============================================================\n",
      "🚀 第一轮推理启动\n",
      "============================================================\n",
      "目标数量: 1000条\n",
      "并发配置: 20线程\n",
      "QPS限制: 15请求/秒\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:   5%|▌         | 51/1000 [00:43<06:11,  2.56it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 50 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 50条 (100.0%)                                          ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.14条/秒                                     ║\n",
      "║  ⏳ 已用时间: 0分43秒                                       ║\n",
      "║  🎯 预计剩余: 13分52秒                                      ║\n",
      "║  🌐 平均API响应: 14.49秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  10%|▉         | 99/1000 [01:42<17:35,  1.17s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 100 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 100条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 0.97条/秒                                     ║\n",
      "║  ⏳ 已用时间: 1分42秒                                       ║\n",
      "║  🎯 预计剩余: 15分24秒                                      ║\n",
      "║  🌐 平均API响应: 18.93秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  15%|█▌        | 150/1000 [02:27<11:18,  1.25it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 150 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 150条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.02条/秒                                     ║\n",
      "║  ⏳ 已用时间: 2分27秒                                       ║\n",
      "║  🎯 预计剩余: 13分54秒                                      ║\n",
      "║  🌐 平均API响应: 18.75秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  20%|██        | 200/1000 [03:19<30:22,  2.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 200 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 200条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.00条/秒                                     ║\n",
      "║  ⏳ 已用时间: 3分19秒                                       ║\n",
      "║  🎯 预计剩余: 13分16秒                                      ║\n",
      "║  🌐 平均API响应: 17.98秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  25%|██▌       | 250/1000 [03:52<07:32,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 250 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 250条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.07条/秒                                     ║\n",
      "║  ⏳ 已用时间: 3分52秒                                       ║\n",
      "║  🎯 预计剩余: 11分38秒                                      ║\n",
      "║  🌐 平均API响应: 18.06秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  30%|██▉       | 299/1000 [04:42<13:14,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 300 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 300条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 4分42秒                                       ║\n",
      "║  🎯 预计剩余: 10分58秒                                      ║\n",
      "║  🌐 平均API响应: 18.30秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  35%|███▌      | 351/1000 [05:34<07:30,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 350 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 350条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.05条/秒                                     ║\n",
      "║  ⏳ 已用时间: 5分34秒                                       ║\n",
      "║  🎯 预计剩余: 10分20秒                                      ║\n",
      "║  🌐 平均API响应: 18.58秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  40%|████      | 400/1000 [06:23<07:15,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 400 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 400条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.04条/秒                                     ║\n",
      "║  ⏳ 已用时间: 6分23秒                                       ║\n",
      "║  🎯 预计剩余: 9分34秒                                       ║\n",
      "║  🌐 平均API响应: 18.38秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  45%|████▍     | 449/1000 [06:56<03:44,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 450 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 450条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.08条/秒                                     ║\n",
      "║  ⏳ 已用时间: 6分56秒                                       ║\n",
      "║  🎯 预计剩余: 8分29秒                                       ║\n",
      "║  🌐 平均API响应: 18.31秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  50%|█████     | 500/1000 [07:44<06:24,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 500 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 500条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.08条/秒                                     ║\n",
      "║  ⏳ 已用时间: 7分44秒                                       ║\n",
      "║  🎯 预计剩余: 7分44秒                                       ║\n",
      "║  🌐 平均API响应: 18.36秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  55%|█████▌    | 550/1000 [08:38<05:30,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 550 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 550条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 8分38秒                                       ║\n",
      "║  🎯 预计剩余: 7分4秒                                        ║\n",
      "║  🌐 平均API响应: 18.58秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  60%|██████    | 600/1000 [09:29<06:25,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 600 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 600条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.05条/秒                                     ║\n",
      "║  ⏳ 已用时间: 9分29秒                                       ║\n",
      "║  🎯 预计剩余: 6分19秒                                       ║\n",
      "║  🌐 平均API响应: 18.55秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  65%|██████▌   | 650/1000 [10:10<21:38,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 650 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 650条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 10分10秒                                      ║\n",
      "║  🎯 预计剩余: 5分28秒                                       ║\n",
      "║  🌐 平均API响应: 18.29秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  70%|███████   | 700/1000 [10:53<02:09,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 700 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 700条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.07条/秒                                     ║\n",
      "║  ⏳ 已用时间: 10分53秒                                      ║\n",
      "║  🎯 预计剩余: 4分39秒                                       ║\n",
      "║  🌐 平均API响应: 18.50秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  75%|███████▌  | 750/1000 [11:48<03:22,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 750 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 750条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 11分48秒                                      ║\n",
      "║  🎯 预计剩余: 3分56秒                                       ║\n",
      "║  🌐 平均API响应: 18.69秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  80%|████████  | 800/1000 [12:38<01:44,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 800 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 800条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.05条/秒                                     ║\n",
      "║  ⏳ 已用时间: 12分38秒                                      ║\n",
      "║  🎯 预计剩余: 3分9秒                                        ║\n",
      "║  🌐 平均API响应: 18.81秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  85%|████████▌ | 850/1000 [13:26<01:50,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 850 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 850条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.05条/秒                                     ║\n",
      "║  ⏳ 已用时间: 13分26秒                                      ║\n",
      "║  🎯 预计剩余: 2分22秒                                       ║\n",
      "║  🌐 平均API响应: 18.61秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  90%|████████▉ | 899/1000 [14:01<01:23,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 900 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 900条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.07条/秒                                     ║\n",
      "║  ⏳ 已用时间: 14分1秒                                       ║\n",
      "║  🎯 预计剩余: 1分33秒                                       ║\n",
      "║  🌐 平均API响应: 18.58秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度:  95%|█████████▌| 950/1000 [14:55<00:41,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 950 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 950条 (100.0%)                                         ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 14分55秒                                      ║\n",
      "║  🎯 预计剩余: 0分47秒                                       ║\n",
      "║  🌐 平均API响应: 18.70秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "第一轮进度: 100%|██████████| 1000/1000 [15:44<00:00,  1.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "╔==========================================================╗\n",
      "║  📊 阶段报告 - 已完成 1000 条                              ║\n",
      "╠==========================================================╣\n",
      "║  ✅ 成功: 1000条 (100.0%)                                        ║\n",
      "║  ❌ 失败: 0条 (0.0%)                                           ║\n",
      "║  ⏱️  平均速度: 1.06条/秒                                     ║\n",
      "║  ⏳ 已用时间: 15分44秒                                      ║\n",
      "║  🌐 平均API响应: 18.76秒                                  ║\n",
      "║  💾 检查点已保存                                          ║\n",
      "╚==========================================================╝\n",
      "\n",
      "\n",
      "============================================================\n",
      "✅ 第一轮推理完成\n",
      "============================================================\n",
      "✅ 成功: 1000条\n",
      "❌ 失败: 0条\n",
      "📊 成功率: 100.00%\n",
      "⏱️  耗时: 15分44秒\n",
      "============================================================\n",
      "\n",
      "🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 🎬 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 批量推理（优化版 - 集成限流、智能重试、详细报告）\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "def process_single_item(item):\n",
    "    \"\"\"\n",
    "    处理单个数据项（线程安全）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        idx = item['index']\n",
    "        question = item['question']\n",
    "        \n",
    "        if not question:\n",
    "            return {'success': False, 'index': idx, 'error': '无法提取问题', 'api_time': 0}\n",
    "        \n",
    "        # 调用API（已集成限流）\n",
    "        response = call_qwen3_api(question, API_KEY, API_URL)\n",
    "        \n",
    "        if response['success']:\n",
    "            # 构建对话格式\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['content']\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # 保留思考内容\n",
    "            if response.get('reasoning_content'):\n",
    "                conversation.append({\n",
    "                    \"reasoning_content\": response['reasoning_content']\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'index': idx,\n",
    "                'data': conversation,\n",
    "                'api_time': response.get('api_time', 0)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'index': idx,\n",
    "                'error': response.get('error', '未知错误'),\n",
    "                'api_time': response.get('api_time', 0)\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'index': item.get('index', -1),\n",
    "            'error': f'处理异常: {str(e)[:50]}',\n",
    "            'api_time': 0\n",
    "        }\n",
    "\n",
    "\n",
    "def print_stage_report(stats, total_target):\n",
    "    \"\"\"打印阶段报告\"\"\"\n",
    "    print(\"\\n\")\n",
    "    print(\"╔\" + \"=\"*58 + \"╗\")\n",
    "    print(f\"║  📊 阶段报告 - 已完成 {stats['total']} 条{' '*30}║\")\n",
    "    print(\"╠\" + \"=\"*58 + \"╣\")\n",
    "    print(f\"║  ✅ 成功: {stats['success']}条 ({stats['success_rate']:.1f}%){' '*(44-len(str(stats['success'])))}║\")\n",
    "    print(f\"║  ❌ 失败: {stats['failed']}条 ({100-stats['success_rate']:.1f}%){' '*(44-len(str(stats['failed'])))}║\")\n",
    "    print(f\"║  ⏱️  平均速度: {stats['speed']:.2f}条/秒{' '*37}║\")\n",
    "    \n",
    "    elapsed_min = int(stats['elapsed'] // 60)\n",
    "    elapsed_sec = int(stats['elapsed'] % 60)\n",
    "    print(f\"║  ⏳ 已用时间: {elapsed_min}分{elapsed_sec}秒{' '*(42-len(str(elapsed_min))-len(str(elapsed_sec)))}║\")\n",
    "    \n",
    "    if stats['total'] < total_target:\n",
    "        remaining = total_target - stats['total']\n",
    "        eta_seconds = remaining / stats['speed'] if stats['speed'] > 0 else 0\n",
    "        eta_min = int(eta_seconds // 60)\n",
    "        eta_sec = int(eta_seconds % 60)\n",
    "        print(f\"║  🎯 预计剩余: {eta_min}分{eta_sec}秒{' '*(42-len(str(eta_min))-len(str(eta_sec)))}║\")\n",
    "    \n",
    "    if stats['avg_api_time'] > 0:\n",
    "        print(f\"║  🌐 平均API响应: {stats['avg_api_time']:.2f}秒{' '*34}║\")\n",
    "    \n",
    "    print(f\"║  💾 检查点已保存{' '*42}║\")\n",
    "    print(\"╚\" + \"=\"*58 + \"╝\")\n",
    "    print()\n",
    "\n",
    "\n",
    "def batch_inference_concurrent(questions, max_workers, round_name=\"第一轮\"):\n",
    "    \"\"\"\n",
    "    并发批量推理（优化版）\n",
    "    \n",
    "    参数:\n",
    "        questions: 问题列表\n",
    "        max_workers: 最大并发线程数\n",
    "        round_name: 轮次名称\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    failed_items = []\n",
    "    \n",
    "    # 重置统计器\n",
    "    stats_collector.reset()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"🚀 {round_name}推理启动\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"目标数量: {len(questions)}条\")\n",
    "    print(f\"并发配置: {max_workers}线程\")\n",
    "    print(f\"QPS限制: {MAX_QPS}请求/秒\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    # 使用线程池执行并发请求\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 提交所有任务\n",
    "        future_to_item = {\n",
    "            executor.submit(process_single_item, item): item \n",
    "            for item in questions\n",
    "        }\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        with tqdm(\n",
    "            total=len(questions), \n",
    "            desc=f\"{round_name}进度\",\n",
    "            bar_format='{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]'\n",
    "        ) as pbar:\n",
    "            \n",
    "            for future in as_completed(future_to_item):\n",
    "                result = future.result()\n",
    "                \n",
    "                if result['success']:\n",
    "                    results.append(result['data'])\n",
    "                    stats_collector.add_success(result.get('api_time', 0))\n",
    "                else:\n",
    "                    failed_items.append({\n",
    "                        'index': result['index'],\n",
    "                        'error': result['error']\n",
    "                    })\n",
    "                    stats_collector.add_failure(result['error'])\n",
    "                    \n",
    "                    # 打印失败信息（简洁版）\n",
    "                    print(f\"\\n⚠️  第{result['index']}条失败: {result['error']}\")\n",
    "                \n",
    "                # 更新进度条\n",
    "                stats = stats_collector.get_stats()\n",
    "                pbar.set_postfix({\n",
    "                    '✅': stats['success'],\n",
    "                    '❌': stats['failed'],\n",
    "                    '成功率': f\"{stats['success_rate']:.1f}%\"\n",
    "                })\n",
    "                pbar.update(1)\n",
    "                \n",
    "                # 每N条打印阶段报告\n",
    "                if stats['total'] % REPORT_INTERVAL == 0 and stats['total'] > 0:\n",
    "                    # 保存检查点\n",
    "                    save_checkpoint(results, OUTPUT_PATH)\n",
    "                    # 打印报告\n",
    "                    print_stage_report(stats, len(questions))\n",
    "    \n",
    "    # 最终保存\n",
    "    save_checkpoint(results, OUTPUT_PATH)\n",
    "    \n",
    "    # 最终统计\n",
    "    stats = stats_collector.get_stats()\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"✅ {round_name}推理完成\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"✅ 成功: {len(results)}条\")\n",
    "    print(f\"❌ 失败: {len(failed_items)}条\")\n",
    "    print(f\"📊 成功率: {stats['success_rate']:.2f}%\")\n",
    "    elapsed_min = int(stats['elapsed'] // 60)\n",
    "    elapsed_sec = int(stats['elapsed'] % 60)\n",
    "    print(f\"⏱️  耗时: {elapsed_min}分{elapsed_sec}秒\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return results, failed_items\n",
    "\n",
    "\n",
    "def save_checkpoint(results, output_path):\n",
    "    \"\"\"\n",
    "    保存检查点（线程安全）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 保存到临时文件，然后重命名（原子操作）\n",
    "        temp_path = f\"{output_path}.tmp\"\n",
    "        with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "            for conversation in results:\n",
    "                f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 重命名为最终文件\n",
    "        import os\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        os.rename(temp_path, output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ 保存检查点失败: {e}\")\n",
    "\n",
    "\n",
    "# ==================== 执行第一轮推理 ====================\n",
    "\n",
    "print(\"\\n\" + \"🎬 \"*20)\n",
    "results, failed_items = batch_inference_concurrent(\n",
    "    questions, \n",
    "    max_workers=MAX_WORKERS,\n",
    "    round_name=\"第一轮\"\n",
    ")\n",
    "print(\"🎬 \"*20 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8f683d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "\n",
      "开始并发推理，共 1000 条数据\n",
      "并发线程数: 20\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   0%|          | 1/1000 [00:24<6:50:29, 24.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 4 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   0%|          | 3/1000 [00:25<1:34:05,  5.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 7 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 15 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   1%|          | 9/1000 [00:25<17:37,  1.07s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 14 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 13 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 2 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 10 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 9 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 12 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   1%|          | 11/1000 [00:25<12:09,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 16 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 0 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 3 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   2%|▏         | 16/1000 [00:25<05:37,  2.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 19 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 11 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 5 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 6 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 17 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   2%|▏         | 20/1000 [00:41<36:45,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 23 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   2%|▏         | 23/1000 [00:43<24:07,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 24 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 26 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 25 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 28 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   3%|▎         | 30/1000 [00:43<08:34,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 22 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 29 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 27 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 31 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 32 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 33 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 35 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 34 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 36 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 30 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   4%|▎         | 35/1000 [00:45<07:33,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 38 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 39 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   4%|▎         | 37/1000 [00:51<15:56,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 40 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 41 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   4%|▍         | 39/1000 [00:52<13:22,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 43 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   4%|▍         | 41/1000 [00:54<14:15,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 45 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 44 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 42 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   4%|▍         | 45/1000 [00:55<07:53,  2.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 47 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 46 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 48 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   5%|▍         | 47/1000 [00:55<05:52,  2.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 51 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 49 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 50 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   5%|▍         | 49/1000 [00:56<06:01,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 54 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   5%|▌         | 52/1000 [01:01<14:19,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 55 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   5%|▌         | 53/1000 [01:04<20:40,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 56 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 55/1000 [01:04<14:02,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 57 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 56/1000 [01:10<31:32,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 59 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 57/1000 [01:10<26:59,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 61 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 58/1000 [01:11<21:03,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 60 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 60/1000 [01:12<14:57,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 63 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 62 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▌         | 62/1000 [01:13<10:10,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 64 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▋         | 63/1000 [01:13<09:13,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 66 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▋         | 64/1000 [01:13<07:54,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 65 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   6%|▋         | 65/1000 [01:14<06:46,  2.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 67 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   7%|▋         | 66/1000 [01:14<06:18,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 68 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   7%|▋         | 68/1000 [01:22<28:45,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 70 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 71 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   7%|▋         | 70/1000 [01:25<28:44,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 72 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   7%|▋         | 71/1000 [01:26<22:50,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 73 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   7%|▋         | 72/1000 [01:27<18:18,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 74 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   8%|▊         | 76/1000 [01:43<58:23,  3.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 89 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   8%|▊         | 77/1000 [01:43<43:43,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 90 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   8%|▊         | 78/1000 [01:46<40:29,  2.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 94 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   8%|▊         | 79/1000 [01:51<51:57,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 96 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   8%|▊         | 80/1000 [01:54<50:55,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 97 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   9%|▉         | 90/1000 [02:07<18:54,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 99 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 100 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:   9%|▉         | 93/1000 [02:09<10:53,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 101 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  10%|▉         | 97/1000 [02:17<23:18,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 103 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  10%|▉         | 99/1000 [02:19<18:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 104 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 105 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  10%|█         | 102/1000 [02:25<28:08,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 106 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  10%|█         | 103/1000 [02:27<26:24,  1.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 107 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  10%|█         | 105/1000 [02:28<18:51,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 108 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  11%|█         | 107/1000 [02:30<15:39,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 110 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  11%|█         | 108/1000 [02:34<26:25,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 113 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  11%|█         | 112/1000 [02:38<23:10,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 117 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 118 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  11%|█▏        | 114/1000 [02:39<13:11,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 119 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  12%|█▏        | 115/1000 [02:41<16:58,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 123 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  12%|█▏        | 117/1000 [02:45<22:16,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 132 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 131 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 133 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  12%|█▏        | 122/1000 [02:51<21:17,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 136 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  12%|█▎        | 125/1000 [02:53<11:37,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 138 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 137 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  13%|█▎        | 128/1000 [02:53<07:06,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 139 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  13%|█▎        | 130/1000 [02:56<11:51,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 140 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  13%|█▎        | 134/1000 [02:59<10:44,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 141 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▎        | 135/1000 [02:59<08:50,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 142 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▎        | 136/1000 [03:00<07:44,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 144 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▎        | 137/1000 [03:01<11:02,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 146 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▍        | 138/1000 [03:01<09:01,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 147 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▍        | 139/1000 [03:02<08:38,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 148 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▍        | 140/1000 [03:02<07:21,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 149 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▍        | 141/1000 [03:03<07:32,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 150 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  14%|█▍        | 143/1000 [03:03<05:31,  2.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 151 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  15%|█▍        | 146/1000 [03:05<06:32,  2.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 152 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 153 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 154 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  15%|█▍        | 148/1000 [03:06<07:59,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 156 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  15%|█▌        | 150/1000 [03:07<06:13,  2.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 157 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  15%|█▌        | 152/1000 [03:08<07:35,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 158 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  15%|█▌        | 154/1000 [03:09<05:02,  2.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 159 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 155/1000 [03:09<06:23,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 162 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 163 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 157/1000 [03:11<08:20,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 164 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 158/1000 [03:11<07:06,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 166 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 165 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 160/1000 [03:12<06:02,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 167 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 161/1000 [03:13<07:52,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 168 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▌        | 162/1000 [03:13<07:07,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 169 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▋        | 163/1000 [03:14<08:08,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 171 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▋        | 164/1000 [03:15<08:47,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 173 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  16%|█▋        | 165/1000 [03:16<11:50,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 174 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 175 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 167/1000 [03:16<07:29,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 176 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 168/1000 [03:17<06:45,  2.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 178 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 170/1000 [03:17<06:21,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 179 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 171/1000 [03:18<05:35,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 180 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 181 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 173/1000 [03:20<08:57,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 182 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  17%|█▋        | 174/1000 [03:21<10:39,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 183 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 175/1000 [03:22<13:50,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 185 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 176/1000 [03:23<11:10,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 184 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 177/1000 [03:23<10:13,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 186 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 178/1000 [03:24<08:51,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 187 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 179/1000 [03:25<09:46,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 189 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 180/1000 [03:25<08:07,  1.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 191 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 181/1000 [03:25<07:55,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 192 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  18%|█▊        | 185/1000 [03:30<11:14,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 195 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 196 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  19%|█▊        | 186/1000 [03:30<09:01,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 197 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  19%|█▉        | 188/1000 [03:32<11:02,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 198 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  19%|█▉        | 189/1000 [03:32<09:00,  1.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 199 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  19%|█▉        | 190/1000 [03:33<07:38,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 200 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  19%|█▉        | 191/1000 [03:33<08:24,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 201 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  21%|██        | 210/1000 [04:11<18:00,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 227 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  21%|██▏       | 213/1000 [04:13<09:47,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 228 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 216/1000 [04:16<12:22,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 231 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 232 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 217/1000 [04:17<10:32,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 233 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 221/1000 [04:19<10:07,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 234 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 222/1000 [04:20<08:02,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 235 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 223/1000 [04:20<06:33,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 236 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  22%|██▏       | 224/1000 [04:21<10:20,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 237 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  23%|██▎       | 226/1000 [04:22<08:21,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 238 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  23%|██▎       | 227/1000 [04:23<06:41,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 239 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  23%|██▎       | 228/1000 [04:24<08:10,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 240 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  23%|██▎       | 229/1000 [04:24<06:50,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 241 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  23%|██▎       | 231/1000 [04:25<05:47,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 242 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  24%|██▎       | 235/1000 [04:28<07:36,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 244 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 245 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  24%|██▍       | 239/1000 [04:28<02:50,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 246 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 247 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  24%|██▍       | 241/1000 [04:29<04:12,  3.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 248 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  24%|██▍       | 242/1000 [04:30<05:37,  2.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 249 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 250 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  25%|██▍       | 246/1000 [04:34<10:17,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 251 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  26%|██▋       | 264/1000 [05:15<21:15,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 281 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  27%|██▋       | 268/1000 [05:20<16:42,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 284 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  27%|██▋       | 269/1000 [05:21<16:12,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 285 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  27%|██▋       | 271/1000 [05:23<12:34,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 286 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  27%|██▋       | 272/1000 [05:25<15:17,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 287 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  27%|██▋       | 273/1000 [05:25<13:24,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 288 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 275/1000 [05:27<12:44,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 289 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 276/1000 [05:27<10:02,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 290 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 279/1000 [05:30<08:50,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 291 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 280/1000 [05:30<07:40,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 292 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 281/1000 [05:33<14:36,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 294 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  28%|██▊       | 282/1000 [05:33<11:00,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 295 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  30%|██▉       | 299/1000 [06:10<26:00,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 316 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  30%|███       | 301/1000 [06:13<21:04,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 317 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  30%|███       | 303/1000 [06:13<12:35,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 318 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  30%|███       | 305/1000 [06:14<08:12,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 319 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  31%|███       | 308/1000 [06:16<08:03,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 320 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  31%|███       | 309/1000 [06:16<06:28,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 321 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  31%|███       | 310/1000 [06:17<05:27,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 322 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  31%|███▏      | 313/1000 [06:17<03:23,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 323 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  31%|███▏      | 314/1000 [06:18<03:35,  3.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 324 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▏      | 315/1000 [06:18<04:20,  2.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 325 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▏      | 317/1000 [06:20<07:52,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 326 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▏      | 320/1000 [06:20<03:54,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 327 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▏      | 321/1000 [06:22<06:30,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 328 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▏      | 324/1000 [06:22<03:47,  2.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 329 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 330 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  32%|███▎      | 325/1000 [06:22<03:32,  3.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 331 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  33%|███▎      | 327/1000 [06:24<06:21,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 334 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  33%|███▎      | 328/1000 [06:25<07:46,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 335 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  33%|███▎      | 331/1000 [06:28<08:16,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 336 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  33%|███▎      | 334/1000 [06:28<04:45,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 338 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 337 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 339 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▎      | 337/1000 [06:30<06:06,  1.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 340 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▍      | 338/1000 [06:31<06:10,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 342 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 343 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 341 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▍      | 341/1000 [06:31<03:43,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 344 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▍      | 342/1000 [06:32<03:46,  2.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 345 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▍      | 343/1000 [06:33<06:59,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 346 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  34%|███▍      | 344/1000 [06:35<10:40,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 256 条推理失败: 请求超时\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▌      | 356/1000 [07:00<11:29,  1.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 370 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▌      | 357/1000 [07:00<09:50,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 371 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▌      | 359/1000 [07:01<08:05,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 372 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▌      | 360/1000 [07:02<07:26,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 373 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▌      | 362/1000 [07:02<05:14,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 374 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▋      | 363/1000 [07:03<06:31,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 375 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  36%|███▋      | 364/1000 [07:05<08:17,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 376 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 377 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  37%|███▋      | 368/1000 [07:06<04:28,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 378 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 379 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  37%|███▋      | 371/1000 [07:07<03:55,  2.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 380 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  37%|███▋      | 372/1000 [07:07<03:24,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 381 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  37%|███▋      | 373/1000 [07:08<04:35,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 382 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  37%|███▋      | 374/1000 [07:09<06:45,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 383 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 384 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 376/1000 [07:09<04:31,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 385 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 378/1000 [07:10<04:20,  2.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 386 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 379/1000 [07:10<03:44,  2.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 387 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 380/1000 [07:12<06:50,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 388 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 389 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 384/1000 [07:12<03:11,  3.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 390 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 391 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  38%|███▊      | 385/1000 [07:15<08:53,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 392 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▊      | 386/1000 [07:16<09:56,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 393 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▉      | 388/1000 [07:16<06:09,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 394 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▉      | 390/1000 [07:17<05:40,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 395 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▉      | 391/1000 [07:18<05:45,  1.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 396 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▉      | 393/1000 [07:18<04:20,  2.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 397 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 398 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  39%|███▉      | 394/1000 [07:20<08:35,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 399 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 400 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|███▉      | 396/1000 [07:21<05:37,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 401 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|███▉      | 397/1000 [07:21<05:19,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 403 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|███▉      | 399/1000 [07:25<12:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 405 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 406 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|████      | 401/1000 [07:26<09:38,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 408 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|████      | 402/1000 [07:27<07:59,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 409 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|████      | 403/1000 [07:27<07:46,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 410 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|████      | 404/1000 [07:28<07:10,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 411 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  40%|████      | 405/1000 [07:28<05:47,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 412 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  41%|████      | 406/1000 [07:30<07:59,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 414 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  41%|████      | 407/1000 [07:30<06:23,  1.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 415 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  41%|████      | 408/1000 [07:32<09:41,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 417 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  41%|████      | 409/1000 [07:33<11:47,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 418 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 427/1000 [08:10<10:01,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 442 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 428/1000 [08:12<13:30,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 445 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 430/1000 [08:13<09:05,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 446 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 432/1000 [08:15<10:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 447 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 433/1000 [08:16<08:53,  1.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 448 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  43%|████▎     | 434/1000 [08:16<07:11,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 449 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▎     | 437/1000 [08:19<07:17,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 451 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 438/1000 [08:20<07:14,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 452 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 439/1000 [08:20<06:55,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 453 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 440/1000 [08:21<06:54,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 454 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 441/1000 [08:22<08:03,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 455 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 442/1000 [08:23<08:06,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 456 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 443/1000 [08:24<07:53,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 457 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  44%|████▍     | 445/1000 [08:25<07:36,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 459 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  45%|████▍     | 447/1000 [08:27<07:20,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 460 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  45%|████▍     | 449/1000 [08:28<06:54,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 461 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  45%|████▌     | 453/1000 [08:29<03:46,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 462 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  45%|████▌     | 454/1000 [08:30<04:50,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 463 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  46%|████▌     | 456/1000 [08:32<05:44,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 464 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  46%|████▌     | 457/1000 [08:33<07:29,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 465 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  47%|████▋     | 472/1000 [09:05<16:26,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 489 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  47%|████▋     | 473/1000 [09:06<14:20,  1.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 490 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  47%|████▋     | 474/1000 [09:09<16:04,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 491 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 475/1000 [09:10<13:47,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 492 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 479/1000 [09:12<08:03,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 493 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 481/1000 [09:13<06:47,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 494 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 495 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 483/1000 [09:15<06:54,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 496 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 484/1000 [09:15<05:58,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 497 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  48%|████▊     | 485/1000 [09:17<07:52,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 499 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  49%|████▊     | 486/1000 [09:17<07:06,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 501 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 500 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  49%|████▉     | 490/1000 [09:20<06:48,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 502 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  49%|████▉     | 492/1000 [09:21<05:36,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 402 条推理失败: 请求超时\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  49%|████▉     | 494/1000 [09:22<03:57,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 503 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  50%|████▉     | 495/1000 [09:23<04:58,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 504 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  50%|████▉     | 498/1000 [09:24<03:28,  2.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 505 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 506 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  50%|█████     | 501/1000 [09:25<04:13,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 507 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  50%|█████     | 502/1000 [09:27<06:21,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 508 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  50%|█████     | 504/1000 [09:29<07:20,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 509 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 506/1000 [09:30<05:41,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 510 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 511 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "\n",
      "第 512 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 508/1000 [09:30<04:01,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 513 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 509/1000 [09:32<06:50,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 514 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 510/1000 [09:33<05:52,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 515 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 511/1000 [09:33<04:44,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 516 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  51%|█████     | 512/1000 [09:34<04:40,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "第 518 条推理失败: 请求异常: 429 Client Error: Too Many Requests for url: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度:  53%|█████▎    | 526/1000 [10:10<09:09,  1.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 95\u001b[0m, in \u001b[0;36mbatch_inference_concurrent\u001b[0;34m(questions, max_workers)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(questions), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m推理进度\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(future_to_item):\n\u001b[1;32m     96\u001b[0m         result \u001b[38;5;241m=\u001b[39m future\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:245\u001b[0m, in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[1;32m    242\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) futures unfinished\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\n\u001b[1;32m    243\u001b[0m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[0;32m--> 245\u001b[0m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m waiter\u001b[38;5;241m.\u001b[39mlock:\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 607\u001b[0m     signaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m     \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m     gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 145\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# 执行并发推理\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m--> 145\u001b[0m results, failed_items \u001b[38;5;241m=\u001b[39m \u001b[43mbatch_inference_concurrent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m60\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 86\u001b[0m, in \u001b[0;36mbatch_inference_concurrent\u001b[0;34m(questions, max_workers)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m并发线程数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_workers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# 使用线程池执行并发请求\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39mmax_workers) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# 提交所有任务\u001b[39;00m\n\u001b[1;32m     88\u001b[0m     future_to_item \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m         executor\u001b[38;5;241m.\u001b[39msubmit(process_single_item, item): item \n\u001b[1;32m     90\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m questions\n\u001b[1;32m     91\u001b[0m     }\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;66;03m# 使用tqdm显示进度\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py:649\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[0;32m--> 649\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[0;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1096\u001b[0m, in \u001b[0;36mThread.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:1116\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1117\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m   1118\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## 批量推理（并发版本 - 60线程）\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "\n",
    "# 线程安全的结果存储\n",
    "results_lock = threading.Lock()\n",
    "results = []\n",
    "failed_items = []\n",
    "\n",
    "def process_single_item(item):\n",
    "    \"\"\"\n",
    "    处理单个数据项（线程安全）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 根据您使用的是第一部分还是第二部分代码，选择对应的提取方式\n",
    "        \n",
    "        # 第一部分代码用这个：\n",
    "        idx = item['index']\n",
    "        question = item['question']\n",
    "        \n",
    "        # 第二部分代码用这个：\n",
    "        # idx = item[0]  # 如果传入的是 (idx, data_item)\n",
    "        # question = extract_question(item[1])\n",
    "        \n",
    "        if not question:\n",
    "            return {'success': False, 'index': idx, 'error': '无法提取问题'}\n",
    "        \n",
    "        # 调用API\n",
    "        response = call_qwen3_api(question, API_KEY, API_URL)\n",
    "        \n",
    "        if response['success']:\n",
    "            # 构建对话格式\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": question\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": response['content']\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # 如果有reasoning_content\n",
    "            if response.get('reasoning_content'):\n",
    "                conversation.append({\n",
    "                    \"reasoning_content\": response['reasoning_content']\n",
    "                })\n",
    "            \n",
    "            return {\n",
    "                'success': True,\n",
    "                'index': idx,\n",
    "                'data': conversation\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'success': False,\n",
    "                'index': idx,\n",
    "                'error': response.get('error', '未知错误')\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'index': item.get('index', -1),\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "\n",
    "def batch_inference_concurrent(questions, max_workers=60):\n",
    "    \"\"\"\n",
    "    并发批量推理\n",
    "    \n",
    "    参数:\n",
    "        questions: 问题列表\n",
    "        max_workers: 最大并发线程数\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    failed_items = []\n",
    "    \n",
    "    print(f\"\\n开始并发推理，共 {len(questions)} 条数据\")\n",
    "    print(f\"并发线程数: {max_workers}\\n\")\n",
    "    \n",
    "    # 使用线程池执行并发请求\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # 提交所有任务\n",
    "        future_to_item = {\n",
    "            executor.submit(process_single_item, item): item \n",
    "            for item in questions\n",
    "        }\n",
    "        \n",
    "        # 使用tqdm显示进度\n",
    "        with tqdm(total=len(questions), desc=\"推理进度\") as pbar:\n",
    "            for future in as_completed(future_to_item):\n",
    "                result = future.result()\n",
    "                \n",
    "                if result['success']:\n",
    "                    results.append(result['data'])\n",
    "                else:\n",
    "                    failed_items.append({\n",
    "                        'index': result['index'],\n",
    "                        'error': result['error']\n",
    "                    })\n",
    "                    print(f\"\\n第 {result['index']} 条推理失败: {result['error']}\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                \n",
    "                # 每完成100条保存一次（防止中断丢失数据）\n",
    "                if len(results) % 100 == 0:\n",
    "                    save_checkpoint(results, OUTPUT_PATH)\n",
    "    \n",
    "    print(f\"\\n推理完成！\")\n",
    "    print(f\"成功: {len(results)} 条\")\n",
    "    print(f\"失败: {len(failed_items)} 条\")\n",
    "    \n",
    "    return results, failed_items\n",
    "\n",
    "\n",
    "def save_checkpoint(results, output_path):\n",
    "    \"\"\"\n",
    "    保存检查点（线程安全）\n",
    "    \"\"\"\n",
    "    try:\n",
    "        Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # 保存到临时文件，然后重命名（原子操作）\n",
    "        temp_path = f\"{output_path}.tmp\"\n",
    "        with open(temp_path, 'w', encoding='utf-8') as f:\n",
    "            for conversation in results:\n",
    "                f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        # 重命名为最终文件\n",
    "        import os\n",
    "        if os.path.exists(output_path):\n",
    "            os.remove(output_path)\n",
    "        os.rename(temp_path, output_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n保存检查点失败: {e}\")\n",
    "\n",
    "\n",
    "# 执行并发推理\n",
    "print(\"=\"*60)\n",
    "results, failed_items = batch_inference_concurrent(questions, max_workers=MAX_WORKERS)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2308f499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始批量推理，共 5 条数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "推理进度: 100%|██████████| 5/5 [02:12<00:00, 26.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "推理完成！成功: 5 条，失败: 0 条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#批量推理-ss\n",
    "# 存储结果\n",
    "results = []\n",
    "failed_count = 0\n",
    "\n",
    "print(f\"开始批量推理，共 {len(questions)} 条数据...\")\n",
    "\n",
    "for item in tqdm(questions, desc=\"推理进度\"):\n",
    "    question = item['question']\n",
    "    \n",
    "    # 调用API\n",
    "    response = call_qwen3_api(question, API_KEY, API_URL)\n",
    "    \n",
    "    if response['success']:\n",
    "        # 构建对话格式\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": question\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": response['content']\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # 如果有reasoning_content，也可以选择保存\n",
    "        if response.get('reasoning_content'):\n",
    "            conversation.append({\n",
    "                \"reasoning_content\": response['reasoning_content']\n",
    "            })\n",
    "        \n",
    "        results.append(conversation)\n",
    "    else:\n",
    "        failed_count += 1\n",
    "        print(f\"\\n第 {item['index']} 条推理失败: {response['error']}\")\n",
    "    \n",
    "    # 添加延迟避免请求过快\n",
    "    time.sleep(0.5)\n",
    "\n",
    "print(f\"\\n推理完成！成功: {len(results)} 条，失败: {failed_count} 条\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2b1dd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "结果已保存到: /data/private/outputs/qwen3_inference_1000_samples.jsonl\n",
      "共保存 1000 条对话数据\n"
     ]
    }
   ],
   "source": [
    "#保存结果\n",
    "# 确保输出目录存在\n",
    "output_path = Path(OUTPUT_PATH)\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 保存为JSONL格式\n",
    "with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    for conversation in results:\n",
    "        f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(f\"结果已保存到: {OUTPUT_PATH}\")\n",
    "print(f\"共保存 {len(results)} 条对话数据\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79efee0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共有 5 条推理结果\n",
      "\n",
      "前5条推理结果（完整内容）：\n",
      "\n",
      "================================================================================\n",
      "第 1 条推理结果\n",
      "================================================================================\n",
      "\n",
      "【问题】\n",
      "能给我讲一个寓意深刻的故事吗？\n",
      "\n",
      "【回答】\n",
      "好的，我为你创作了一个关于机械森林的寓言故事。这个故事探讨了科技与自然的关系，以及人类如何在时代洪流中守护希望。请看看这个故事能否带给你启发。\n",
      "\n",
      "---\n",
      "\n",
      "**《会开花的树》**\n",
      "\n",
      "爷爷的机械义肢在金属森林里发出清脆的碰撞声，我紧紧攥着他冰凉的手，看那些银白色的仿真树木在暮色里收起叶片。\n",
      "\n",
      "\"就是这里。\"爷爷蹲下身，胸腔里的燃料电池发出轻微的嗡鸣。他翻开一片伪装成落叶的太阳能板，泥土的气息突然从地底涌出来——那里埋着个生锈的铁皮盒子。\n",
      "\n",
      "我的电子义眼自动调焦，虹膜上跳动着数据流：盒子储藏温度26℃，湿度75%，储存时长显示已经超过了八十年。当爷爷颤抖着打开盒盖时，十二颗榛子大小的种子正在鹅绒衬布上沉睡。\n",
      "\n",
      "\"这是你太奶奶从旧地球带来的橡果。\"爷爷把种子倒进我的机械手掌，传感器显示它们的心跳每分钟只有三次，\"那时候树木会自己开花。\"\n",
      "\n",
      "我触碰颈后的神经接口，记忆芯片里立刻跳出全息影像：粉白的花瓣像落雪铺满山谷，穿碎花裙的少女在花雨中转圈，她发梢沾着的不是数据流，而是真正的桃花瓣。\n",
      "\n",
      "金属森林突然剧烈震颤，巡逻的蜘蛛探测器从枝桠间垂下红光。爷爷把我推进废弃的地下管道，他的机械臂在关闭闸门时被激光削去半截：\"记住，当种子发芽的时候......\"\n",
      "\n",
      "我抱着铁盒在污水管道里蜷缩了三天三夜。智能追踪系统搜索的是人类体温，而我的电子骨骼始终维持着18℃的恒温。当警报终于解除时，我的机械胃里只剩下一组被腐蚀的电池。\n",
      "\n",
      "在废弃的核电站地下室，我用冷却液浇灌种子。第一千零四十七天，当最后一点能源即将耗尽时，芽尖终于顶破了金属地板。淡金色的光线从裂缝里涌进来——不是太阳，而是三百架无人机正悬停在城市上空。\n",
      "\n",
      "它们携带着从月球采回的冰晶，在贫瘠的土地上投下最后一场雨。我看着玻璃穹顶外闪烁的星群，突然明白爷爷没说完的话：真正的魔法不是光合作用，是让不可能的事物在计算误差里生根。\n",
      "\n",
      "今早，第一朵花在钢铁枝桠间绽放。花瓣是半透明的数据流，却引来了真正的蜜蜂。\n",
      "\n",
      "---\n",
      "\n",
      "**故事核心：**\n",
      "- 被遗忘的种子象征被科技社会抛弃的自然法则\n",
      "- 恒温机械体与冷血生物的对比暗喻人性温度\n",
      "- 数据流组成的花朵暗示科技与自然终将融合\n",
      "- 三百架无人机降雨对应诺亚方舟的现代诠释\n",
      "\n",
      "希望这个故事能在钢筋水泥的缝隙里，为你栽下一朵会发光的花。\n",
      "\n",
      "【思考过程】\n",
      "好的，用户让我讲一个寓意深刻的故事。首先，我需要确定用户的需求是什么。他们可能想要一个有深度的故事，可能用于启发思考，或者作为生活中的借鉴。接下来，我得考虑故事的主题。通常寓意深刻的故事会涉及人性、道德、生命的意义等主题。比如，自然与人类的关系、科技与传统的冲突，或者个人成长中的挑战。\n",
      "\n",
      "然后，用户可能希望故事有情感共鸣，所以需要塑造有深度的角色和情节。可能需要一个转折点，让读者在结局时有所领悟。考虑到现代科技对生活的影响，或许可以围绕科技与自然和谐共处的主题展开，这样既有现实意义又能引发思考。\n",
      "\n",
      "接下来是故事的结构。可能需要一个设定，比如未来世界，科技高度发达，但自然环境被破坏。主角可以是一个孩子，代表纯真和希望，与爷爷这样的传统角色互动，带出传承的重要性。加入象征元素，比如会开花的树，作为希望的象征，增强故事的感染力。\n",
      "\n",
      "还要注意故事的节奏，开头描绘环境，中间发展冲突，高潮部分出现转折，结尾点明寓意。语言要生动，细节描写要到位，比如机械森林的冰冷、爷爷的温暖动作，以及种子发芽的奇迹，这些都能增强画面感。\n",
      "\n",
      "最后，要确保寓意明确但不过于直白。通过主角的经历和变化，让读者自行领悟主题，比如真正的魔法是爱与坚持，科技无法替代自然，或者希望需要时间和耐心培育。这样故事既有深度，又留给读者思考的空间。\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "第 2 条推理结果\n",
      "================================================================================\n",
      "\n",
      "【问题】\n",
      "证明：如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。\n",
      "\n",
      "【回答】\n",
      "### 证明：对于任意两个 $ n \\times n $ 阶方阵 $ A $ 和 $ B $，有 $ \\text{tr}(AB) = \\text{tr}(BA) $\n",
      "\n",
      "---\n",
      "\n",
      "#### 背景知识\n",
      "\n",
      "矩阵的**迹**（trace）定义为矩阵主对角线元素之和。对于任意两个 $ n \\times n $ 阶矩阵 $ A $ 和 $ B $，即使 $ AB \\neq BA $，也有一个重要的矩阵迹性质：\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\text{tr}(BA)\n",
      "$$\n",
      "\n",
      "这一性质被称为**迹的循环不变性**。它不依赖于 $ A $ 和 $ B $ 是否可交换，因此即使在 $ AB = BA $ 的情况下，该性质依然成立。\n",
      "\n",
      "---\n",
      "\n",
      "#### 详细证明\n",
      "\n",
      "设 $ A $ 和 $ B $ 是两个 $ n \\times n $ 阶矩阵。我们从迹的定义出发，逐步展开 $ \\text{tr}(AB) $ 和 $ \\text{tr}(BA) $，并比较它们的表达式。\n",
      "\n",
      "---\n",
      "\n",
      "**1. 展开 $ \\text{tr}(AB) $**\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\sum_{i=1}^n (AB)_{ii}\n",
      "$$\n",
      "\n",
      "其中，矩阵 $ AB $ 的第 $ i $ 行第 $ i $ 列元素为：\n",
      "\n",
      "$$\n",
      "(AB)_{ii} = \\sum_{k=1}^n A_{ik}B_{ki}\n",
      "$$\n",
      "\n",
      "因此：\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\sum_{i=1}^n \\sum_{k=1}^n A_{ik}B_{ki}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "**2. 展开 $ \\text{tr}(BA) $**\n",
      "\n",
      "$$\n",
      "\\text{tr}(BA) = \\sum_{i=1}^n (BA)_{ii}\n",
      "$$\n",
      "\n",
      "其中，矩阵 $ BA $ 的第 $ i $ 行第 $ i $ 列元素为：\n",
      "\n",
      "$$\n",
      "(BA)_{ii} = \\sum_{k=1}^n B_{ik}A_{ki}\n",
      "$$\n",
      "\n",
      "因此：\n",
      "\n",
      "$$\n",
      "\\text{tr}(BA) = \\sum_{i=1}^n \\sum_{k=1}^n B_{ik}A_{ki}\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "**3. 比较两个表达式**\n",
      "\n",
      "注意到 $ \\text{tr}(AB) $ 和 $ \\text{tr}(BA) $ 的表达式分别为：\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\sum_{i=1}^n \\sum_{k=1}^n A_{ik}B_{ki}\n",
      "$$\n",
      "$$\n",
      "\\text{tr}(BA) = \\sum_{i=1}^n \\sum_{k=1}^n B_{ik}A_{ki}\n",
      "$$\n",
      "\n",
      "由于标量乘法的交换性，$ A_{ik}B_{ki} = B_{ki}A_{ik} $。因此，我们可以将 $ \\text{tr}(AB) $ 中的项重新排列顺序，得到：\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\sum_{k=1}^n \\sum_{i=1}^n B_{ki}A_{ik} = \\text{tr}(BA)\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "#### 结论\n",
      "\n",
      "我们得出：\n",
      "\n",
      "$$\n",
      "\\text{tr}(AB) = \\text{tr}(BA)\n",
      "$$\n",
      "\n",
      "这一性质对任意 $ n \\times n $ 阶矩阵 $ A $ 和 $ B $ 都成立，无论它们是否可交换（即是否满足 $ AB = BA $）。因此，题目中给出的条件 $ AB = BA $ 是**多余的**，但结论依然成立。\n",
      "\n",
      "---\n",
      "\n",
      "### 最终答案\n",
      "\n",
      "$$\n",
      "\\boxed{\\text{tr}(AB) = \\text{tr}(BA)}\n",
      "$$\n",
      "\n",
      "【思考过程】\n",
      "嗯，好的，我现在要证明如果A和B都是n阶方阵，并且满足AB=BA的话，那么它们的迹tr(AB)等于tr(BA)。不过题目里已经说了AB=BA，所以可能这个条件是不是多余的呢？或者说是不是题目本来应该是即使AB不等于BA的情况下，tr(AB)=tr(BA)，但这里特别强调当AB=BA的时候成立？不过我记得迹的一个性质本来就是不管矩阵相乘的顺序如何，迹都是相等的，也就是说即使AB不等于BA，tr(AB)=tr(BA)也是成立的。那这里题目给出AB=BA的条件，是不是有什么特别的地方需要考虑呢？\n",
      "\n",
      "不过可能我记错了，先不管这个，先仔细想想。题目说如果A和B是n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。首先，根据迹的基本性质，对于任意两个矩阵A和B，不管是否满足AB=BA，tr(AB)=tr(BA)都是成立的。所以即使题目中没有给出AB=BA的条件，结论也是成立的。但题目中特别给出了这个条件，可能是为了某种特定的证明方法，或者可能题目原本有其他意图？\n",
      "\n",
      "不过不管怎样，既然题目要求证明当AB=BA时tr(AB)=tr(BA)，那么或许需要结合交换性的条件来给出另一种证明方法。不过常规的结论是tr(AB)=tr(BA)对任何A,B都成立，所以这里可能题目有点多余，或者可能题目原本想考察别的东西？\n",
      "\n",
      "不过先不管这个，让我先回忆一下常规的证明方法。迹的定义是矩阵主对角线元素之和，所以tr(AB)就是AB乘积后的主对角线元素之和。而AB的第i个对角线元素是A的第i行和B的第i列对应元素乘积之和，也就是Σ_{k=1}^n A_{ik}B_{ki}。然后tr(AB)=Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki}。\n",
      "\n",
      "同样地，tr(BA)=Σ_{i=1}^n Σ_{k=1}^n B_{ik}A_{ki}。这时候，如果交换求和顺序的话，可能可以证明这两个表达式是相等的。例如，tr(AB)=Σ_{i,k}A_{ik}B_{ki}=Σ_{k,i}B_{ki}A_{ik}=Σ_{k,i}B_{ki}A_{ik}_{ik}，也就是tr(BA)。因此，不管A和B是否交换，迹都是相等的。\n",
      "\n",
      "所以，看来这个结论实际上对于任何A和B都是成立的，不需要AB=BA的条件。那题目里给出的条件AB=BA是不是有问题呢？或者可能题目原本是要证明tr(AB)=tr(BA)，但不管怎样，既然常规证明已经足够，或许题目的条件是多余的？\n",
      "\n",
      "不过可能题目希望用AB=BA的条件来给出另外一种证明方法，比如通过特征值或者其他性质？\n",
      "\n",
      "因为当A和B可交换的时候，它们可以同时上三角化，也就是说存在一个可逆矩阵P，使得P^{-1}AP和P^{-1}BP都是上三角矩阵。这种情况下，AB和BA的迹可能更容易计算。例如，如果A和B都可以对角化并且可交换的话，它们可以同时对角化，那么AB和BA的迹显然相等，因为它们的乘积在对角化的情况下对角线元素都是各自特征值的乘积，顺序不同而已。不过即使不能对角化，如果可以同时上三角化的话，迹也是可以比较的。\n",
      "\n",
      "不过这可能是一种不同的证明思路，但不管怎样，常规证明已经足够。不过可能题目希望我们结合可交换的条件来证明，不过不管怎样，结论是对的，不管是否有AB=BA的条件，所以这里可能题目给出这个条件只是作为已知条件，或者可能是为了在某个特定上下文中的应用。\n",
      "\n",
      "不过既然题目要求证明当A和B可交换时tr(AB)=tr(BA)，那么也许可以这样回答：实际上，无需A和B可交换，这个等式总成立。但根据题目要求，我们来证明当AB=BA时结论成立。\n",
      "\n",
      "不过为了稳妥起见，可能应该按照题目的条件来，给出一个使用AB=BA作为条件的证明，虽然实际上不需要这个条件。不过可能题目的意图是其他，比如tr(AB)的某种性质？\n",
      "\n",
      "再仔细想想，可能题目有误，或者我哪里有问题？\n",
      "\n",
      "比如，是否存在某些情况下，当AB不等于BA时，tr(AB)不等于tr(BA)？不过根据已知结论，这种情况不存在。因此，可能题目在条件上是多余的，但结论是对的。\n",
      "\n",
      "不过既然题目要求证明当AB=BA时tr(AB)=tr(BA)，那我可以直接说，根据迹的循环性质，对于任何矩阵A和B，tr(AB)=tr(BA)总成立，无论是否交换。所以即使题目中的条件可能多余，结论是对的。\n",
      "\n",
      "不过可能老师或者出题人想让学生使用AB=BA的条件来给出一个更直接的证明，比如tr(AB)=tr(BA)因为AB=BA，所以它们的迹当然相等。也就是说，如果两个矩阵相等，它们的迹自然相等。这时候tr(AB)=tr(BA)直接来自于AB=BA，因为两个矩阵相同，所以它们的迹当然一样。\n",
      "\n",
      "哦，对了，这可能才是题目的意思。题目是说如果A和B是方阵，并且AB=BA，那么tr(AB)=tr(BA)。不过在这种情况下，因为AB和BA本身是相同的矩阵，所以它们的迹自然相同。但是这显然有问题，因为如果AB=BA的话，那么作为矩阵相等的话，它们的迹当然相等。但问题在于，题目中的结论tr(AB)=tr(BA)在这种情况下是显然的，因为矩阵本身相等，所以迹相等。但如果是这样的话，这个结论适用于任何满足AB=BA的矩阵，而实际上即使AB≠BA，迹还是相等的。\n",
      "\n",
      "这说明题目可能存在一定的矛盾，或者题目的意图被误解了。可能用户输入的时候有错误？\n",
      "\n",
      "不过仔细看一下题目：“证明：如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。”所以这里的问题是，当AB=BA时，证明tr(AB)=tr(BA)，但根据矩阵相等的情况下，它们的迹当然相等，所以这几乎是定义上的事情。也就是说，如果两个矩阵相等，那么它们的迹相等，所以tr(AB)=tr(BA)直接由AB=BA得出。这似乎是一个两行的证明。\n",
      "\n",
      "但另一方面，常规的结论是无论AB和BA是否相等，它们的迹都是相等的，这时候tr(AB)=tr(BA)，而这里的题目可能混淆了两种情况。也就是说，用户可能本来想问的是常规的情况，但加上了一个不必要的条件，或者可能想问当AB=BA时，tr(AB)=tr(BA)是否成立，而答案当然是肯定的，但原因不同。\n",
      "\n",
      "这个时候，我需要明确题目的意图。如果题目给出AB=BA的条件，那么结论直接来自于矩阵相等的定义，即如果两个矩阵相等，它们所有的对应元素都相等，包括迹。因此，tr(AB)=tr(BA)是直接成立的。但如果是这样的话，这个题目似乎太简单了，可能用户的问题有误。\n",
      "\n",
      "另一种可能是，用户本来想问的是tr(AB)=tr(BA)的证明，而不管是否交换，这时候AB和BA虽然不一定相等，但它们的迹相等。可能在原题中存在一些排版错误或者条件误植。\n",
      "\n",
      "在这种情况下，我需要考虑用户的需求。根据用户给出的问题：“证明：如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。”因此，用户可能在学习过程中遇到了这个问题，或者题目中给出了这个条件，所以需要结合该条件进行证明。\n",
      "\n",
      "如果按照题目给出的条件，AB=BA，那么tr(AB)=tr(BA)直接由矩阵相等的性质得出，即如果两个矩阵相等，它们的迹也相等。因此，证明非常简短：\n",
      "\n",
      "证明：因为AB=BA，所以tr(AB)=tr(BA)，得证。\n",
      "\n",
      "但这似乎太简单了，可能不是题目的本意。因此，可能用户在这里存在对题目的误解，或者题目本身存在错误。\n",
      "\n",
      "另一种可能是，题目原本是要证明tr(AB)=tr(BA)，而AB=BA是这个证明过程中的一个中间步骤，但可能并非必要条件。这时候可能需要更仔细地检查。\n",
      "\n",
      "考虑到用户可能需要的是常规的证明，即对于任何A和B，tr(AB)=tr(BA)，即使他们不可交换，不过题目中给出了AB=BA的条件，但可能用户自己也不太清楚，所以需要明确回答。\n",
      "\n",
      "现在，我需要综合考虑这两种可能性：\n",
      "\n",
      "可能性一：题目正确，条件AB=BA存在，结论tr(AB)=tr(BA)直接由矩阵相等的定义得出。\n",
      "\n",
      "可能性二：题目存在错误，应该是不加AB=BA的条件，证明tr(AB)=tr(BA)。\n",
      "\n",
      "根据我的经验，可能性二的可能性更大，因为迹的性质tr(AB)=tr(BA)是著名的结论，通常在教科书中出现时不带AB=BA的条件。因此，可能用户误加了这个条件，或者题目存在排版错误。\n",
      "\n",
      "不过，为了全面考虑，我应该分别处理这两种情况：\n",
      "\n",
      "首先，针对可能性一：如果题目确实要求基于AB=BA的条件来证明tr(AB)=tr(BA)，那么证明如下：\n",
      "\n",
      "证明：已知AB=BA，因此矩阵AB和矩阵BA完全相同，所以它们的迹也必须相同。因此，tr(AB)=tr(BA)。证毕。\n",
      "\n",
      "不过这可能显得太简单，但逻辑上是正确的。\n",
      "\n",
      "针对可能性二：如果题目漏掉了条件，或者条件被误植，正确的结论是对于任何A,B，tr(AB)=tr(BA)，证明如下：\n",
      "\n",
      "证明：迹的定义是矩阵对角线元素之和。考虑AB的第i个对角线元素是Σ_{k=1}^n A_{ik}B_{ki}，因此tr(AB)=Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki}。同样地，tr(BA)=Σ_{i=1}^n Σ_{k=1}^n B_{ik}A_{ki}。注意到这里的表达式实际上是相同的，只是交换了求和的索引变量i和k。具体来说，如果我们交换i和k的位置，tr(AB)中的项A_{ik}B_{ki}对应到tr(BA)中的项B_{ki}A_{ik}，也就是相同的数乘积交换，因此求和结果相同。因此tr(AB)=tr(BA)。\n",
      "\n",
      "或者，可以利用矩阵乘法的迹的性质，使用线性代数中的定理，即tr(AB)=tr(BA)对于任何A,B成立。\n",
      "\n",
      "现在，再回到用户的问题，题目确实提到了AB=BA的条件，因此可能性一的可能性存在，但根据我的知识，这可能不是题目的意图，因为如果这样的话，问题没有意义，因为迹相等显然成立。因此，可能用户的问题存在条件多余的情况，或者可能题目原本是其他形式，比如tr(A+B)=tr(A)+tr(B)之类的，但不太清楚。\n",
      "\n",
      "或者，可能题目原本是证明det(AB)=det(BA)，而当det存在时，但这个对于方阵总是成立的，不需要AB=BA的条件，因为det(AB)=det(A)det(B)=det(B)det(A)=det(BA)，所以同样不需要条件。\n",
      "\n",
      "不过回到原题，无论如何，我需要根据用户给出的问题来回答，即在AB=BA的情况下证明tr(AB)=tr(BA)。这时候，有两种可能的解答方式：\n",
      "\n",
      "1. 直接因为AB=BA，所以迹相等；\n",
      "\n",
      "2. 即使没有AB=BA的条件，迹也相等，所以结论成立。\n",
      "\n",
      "但用户的问题可能希望使用第一种方式，但这样结论显得过于直接，而实际上这个等式总是成立的，不管有没有这个条件。\n",
      "\n",
      "因此，可能存在题目的错误，或者用户的输入错误。在这种情况下，我应该指出这点，并给出两种情况下的证明。\n",
      "\n",
      "不过根据用户的问题陈述：“证明：如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。”所以用户可能在学习过程中遇到了这个问题，并且可能对迹的性质不太熟悉，因此我需要给出一个正确的证明，不管条件是否有必要。\n",
      "\n",
      "在这种情况下，我需要考虑用户可能的真实需求：也许他们需要证明tr(AB)=tr(BA)，而AB=BA的条件可能无关紧要，或者他们被要求在交换的情况下证明，但无论如何，我需要给出一个正确的推导过程。\n",
      "\n",
      "可能用户认为在交换的条件下，tr(AB)=tr(BA)需要某种特定的证明方法，而不是常规的迹的循环性质，因此需要详细展开。\n",
      "\n",
      "不过现在，我需要按照可能性来回答：\n",
      "\n",
      "首先，如果题目确实要求利用AB=BA这个条件，那么证明就是tr(AB)=tr(BA)，因为矩阵相等；\n",
      "\n",
      "其次，如果题目意图是证明对于任何A,B，tr(AB)=tr(BA)，不管是否交换，那么给出迹定义的证明；\n",
      "\n",
      "考虑到用户可能混淆了两种情况，我应该指出通常情况下tr(AB)=tr(BA)无需交换条件，但根据题目给出的条件，可以更直接地得出结论。\n",
      "\n",
      "可能的回答结构：\n",
      "\n",
      "实际上，即使矩阵A和B不可交换，即AB≠BA，等式tr(AB)=tr(BA)仍然成立。这是一个关于矩阵迹的一般性质。不过，题目中给出了AB=BA的条件，这时候结论更加显然，因为当两个矩阵相等时，它们的迹必然相等。因此，tr(AB)=tr(BA)直接由AB=BA推出。不过，为了全面说明，这里给出两种情况的证明：\n",
      "\n",
      "1. 一般情况下tr(AB)=tr(BA)，无需AB=BA的条件；\n",
      "\n",
      "2. 特殊情况下，当AB=BA时，结论显然成立。\n",
      "\n",
      "不过可能题目的意图是希望我们使用迹的一些其他性质来证明，而不仅仅是引用矩阵相等的结论。比如，可能在某些教材中，先证明当AB=BA时的一些性质，然后进一步推广，所以需要特定的步骤。\n",
      "\n",
      "不过，为了确保正确性，我需要详细写出迹的一般证明：\n",
      "\n",
      "迹的乘积交换性的证明：\n",
      "\n",
      "考虑n×n矩阵A和B，那么tr(AB)=Σ_{i=1}^n (AB)_{ii}=Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki}\n",
      "\n",
      "同样地，tr(BA)=Σ_{i=1}^n (BA)_{ii}=Σ_{i=1}^n Σ_{k=1}^n B_{ik}A_{ki}\n",
      "\n",
      "现在，我们可以交换求和顺序，得到：\n",
      "\n",
      "Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki}=Σ_{k=1}^n Σ_{i=1}^n B_{ki}A_{ik}=Σ_{k=1}^n Σ_{i=1}^n B_{ki}A_{ik}=Σ_{k=1}^n (BA)_{kk}=tr(BA)\n",
      "\n",
      "这里，第二个等式通过将指标i和k交换，也就是令原来的k作为i，i作为k，但实际上求和变量是哑变量，因此可以重命名：\n",
      "\n",
      "令在tr(AB)中，项为A_{ik}B_{ki}，对i和k都从1到n求和，这相当于对所有可能的i和k的组合进行求和，因此无论怎样交换i和k的顺序，结果都是一样的。因此，tr(AB)=tr(BA)\n",
      "\n",
      "因此，不管A和B是否可交换，结论都成立。\n",
      "\n",
      "回到题目，既然题目给出了AB=BA的条件，但结论在一般情况下都成立，所以这个条件可能是多余的，但结论是对的。\n",
      "\n",
      "因此，总结：\n",
      "\n",
      "题目中的条件AB=BA并不是必需的，但当满足这个条件时，结论tr(AB)=tr(BA)自然成立，因为矩阵相等的情况下它们的迹当然相等。不过实际上即使没有这个条件，结论也是成立的。因此，证明可以通过引用迹的普遍性质，或者通过直接应用矩阵相等的性质。\n",
      "\n",
      "不过，如果题目明确要求使用AB=BA的条件，可能预期的是前一种简短证明，否则是后一种。\n",
      "\n",
      "考虑到用户可能的需求，这里给出两种证明中的一种，可能更倾向于普遍情况的证明，因为题目中的条件可能是多余的。\n",
      "\n",
      "因此，最终的回答应该包括迹作为线性算子的性质，以及交换求和顺序的证明。\n",
      "\n",
      "现在，我需要用中文组织这个证明过程，可能按照如下步骤：\n",
      "\n",
      "首先写出tr(AB)的表达式，然后交换求和顺序得到tr(BA)，从而证明两者相等，而无需用到AB=BA的条件。然后指出题目中的条件是多余的，但结论成立。\n",
      "\n",
      "或者，如果必须结合AB=BA的条件，那么直接说明矩阵相等导致迹相等。\n",
      "\n",
      "但考虑到常规的迹的性质更为重要，这里可能应该给出普遍性的证明，然后指出题目中的条件其实不需要。\n",
      "\n",
      "综上，我认为用户可能想问的是常规情况，所以回答如下：\n",
      "\n",
      "“即使不假设AB=BA，我们都有tr(AB)=tr(BA)。证明如下：迹是矩阵的对角线元素之和，因此tr(AB)=Σ_{i=1}^n Σ_{j=1}^n A_{ij}B_{ji}，而tr(BA)=Σ_{j=1}^n Σ_{i=1}^n B_{ji}A_{ij}，两者显然相等，因为和式的项相同，仅求和顺序不同。”\n",
      "\n",
      "但为了符合题目中的条件，可能需要补充说：特别地，当AB=BA时，结论自然成立。\n",
      "\n",
      "因此，综合起来，最终的答案应该是：\n",
      "\n",
      "证明：对于任意两个n阶方阵A和B，无论是否交换，都有tr(AB)=tr(BA)。这是因为迹的定义为对角线元素之和，具体来说：\n",
      "\n",
      "tr(AB) = Σ_{i=1}^n (AB)_{ii} = Σ_{i=1}^n (Σ_{j=1}^n A_{ij}B_{ji}}) = Σ_{i=1}^n Σ_{j=1}^n A_{ij}B_{ji}\n",
      "\n",
      "同样地，\n",
      "\n",
      "tr(BA) = Σ_{i=1}^n (BA)_{ii} = Σ_{i=1}^n (Σ_{j=1}^n B_{ij}A_{ji}}) = Σ_{i=1}^n Σ_{j=1}^n B_{ij}A_{ji}\n",
      "\n",
      "注意到A_{ij}和B_{ji}的乘积与求和顺序无关，交换i和j的顺序，我们可以将第二个表达式重写为：\n",
      "\n",
      "Σ_{j=1}^n Σ_{i=1}^n B_{ji}A_{ij} = Σ_{i=1}^n Σ_{j=1}^n A_{ij}B_{ji} = tr(AB)\n",
      "\n",
      "因此，tr(AB)=tr(BA)。特别地，当A和B可交换时，即AB=BA，该等式当然成立，此时tr(AB)=tr(BA)是更一般性结论的一个特例。\n",
      "\n",
      "不过题目中给出的条件AB=BA虽然对于这个等式的成立并非必要，但在此条件下结论仍然正确。\n",
      "\n",
      "不过，如果题目明确希望使用AB=BA的条件进行证明，可以这样简短地回答：\n",
      "\n",
      "因为AB=BA，所以这两个矩阵相等，因此它们的所有对应元素都相等，包括迹，即tr(AB)=tr(BA)。\n",
      "\n",
      "但这样的证明虽然正确，却忽略了迹本身的一个重要性质，即即使矩阵乘积不可交换，它们的迹仍然相等，这可能才是题目的考察点。\n",
      "\n",
      "因此，考虑到教育意义，给出普遍情况的证明更好，同时指出题目中的条件是多余的。\n",
      "\n",
      "综上，我认为正确的方法是证明一般情况下的tr(AB)=tr(BA)，然后指出题目中的条件虽然成立，但并非必要。因此，在答案中给出详细的迹的求和证明，说明无论A和B是否可交换，结论成立，因此题目中的情况当然满足。\n",
      "\n",
      "现在，将上述思考整理成正式的证明步骤：\n",
      "\n",
      "首先，展开tr(AB)为双重和的形式；\n",
      "\n",
      "其次，重新排列求和顺序得到tr(BA)；\n",
      "\n",
      "最后，得出结论，无需AB=BA的条件，因此题目中的结论在更弱的条件下成立。\n",
      "\n",
      "可能用户的题目存在错误，但答案正确。\n",
      "\n",
      "现在，我需要用中文写出这个过程，确保清晰正确。\n",
      "\n",
      "**答案**\n",
      "\\boxed{tr(AB) = tr(BA)}\n",
      "\n",
      "**证明：**\n",
      "\n",
      "矩阵A和B都是n阶方阵，迹的定义为矩阵主对角线元素之和。考虑矩阵乘积AB的迹：\n",
      "\n",
      "$$\n",
      "tr(AB) = \\sum_{i=1}^n (AB)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n A_{ik}B_{ki}\n",
      "$$\n",
      "\n",
      "同样地，对于矩阵乘积BA的迹：\n",
      "\n",
      "$$\n",
      "tr(BA) = \\sum_{i=1}^n (BA)_{ii} = \\sum_{i=1}^n \\sum_{k=1}^n B_{ik}A_{ki}\n",
      "$$\n",
      "\n",
      "注意到在tr(AB)的表达式中，每个项A_{ik}B_{ki}对应tr(BA)中的项B_{ki}A_{ik}（令k和i互换角色即可）。由于标量乘法的交换性，A_{ik}B_{ki} = B_{ki}A_{ik}，并且双重求和的顺序可以交换而不影响结果，因此：\n",
      "\n",
      "$$\n",
      "tr(AB) = \\sum_{i=1}^n \\sum_{k=1}^n A_{ik}B_{ki} = \\sum_{k=1}^n \\sum_{i=1}^n B_{ki}A_{ik} = tr(BA)\n",
      "$$\n",
      "\n",
      "因此，无论A和B是否满足AB=BA，总有tr(AB)=tr(BA)。题目中给出的条件AB=BA在此情况下并非必要，但结论依然成立。\n",
      "\n",
      "证毕。\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "第 3 条推理结果\n",
      "================================================================================\n",
      "\n",
      "【问题】\n",
      "大瓶芒果汁容量为30盎司，每瓶售价2700比塞塔。小瓶容量为6盎司，每瓶售价600比塞塔。购买一大瓶而不是同样体积的小瓶可以节省多少比塞塔？\n",
      "\n",
      "【回答】\n",
      "为了比较购买一大瓶芒果汁与购买等体积小瓶芒果汁的花费差异，我们可以从以下几个方面进行分析：\n",
      "\n",
      "---\n",
      "\n",
      "### 1. **明确目标**\n",
      "题目要求我们计算 **购买一大瓶芒果汁** 相较于 **购买等体积的小瓶芒果汁** 可以节省多少钱。这里的“等体积”指的是总容量相同。\n",
      "\n",
      "---\n",
      "\n",
      "### 2. **单位换算与数量计算**\n",
      "- 大瓶容量：30 盎司，售价：2700 比塞塔\n",
      "- 小瓶容量：6 盎司，售价：600 比塞塔\n",
      "\n",
      "要获得 30 盎司的芒果汁：\n",
      "- 需要购买的小瓶数量：  \n",
      "  $$\n",
      "  \\frac{30}{6} = 5 \\text{ 瓶}\n",
      "  $$\n",
      "\n",
      "---\n",
      "\n",
      "### 3. **计算总花费**\n",
      "- 5 瓶小瓶的总价格：  \n",
      "  $$\n",
      "  5 \\times 600 = 3000 \\text{ 比塞塔}\n",
      "  $$\n",
      "- 1 瓶大瓶的价格：  \n",
      "  $$\n",
      "  2700 \\text{ 比塞塔}\n",
      "  $$\n",
      "\n",
      "---\n",
      "\n",
      "### 4. **计算节省金额**\n",
      "- 购买大瓶比购买相同体积小瓶节省的金额为：  \n",
      "  $$\n",
      "  3000 - 2700 = 300 \\text{ 比塞塔}\n",
      "  $$\n",
      "\n",
      "---\n",
      "\n",
      "### 5. **验证（每盎司价格法）**\n",
      "- 大瓶每盎司价格：  \n",
      "  $$\n",
      "  \\frac{2700}{30} = 90 \\text{ 比塞塔/盎司}\n",
      "  $$\n",
      "- 小瓶每盎司价格：  \n",
      "  $$\n",
      "  \\frac{600}{6} = 100 \\text{ 比塞塔/盎司}\n",
      "  $$\n",
      "- 每盎司节省：  \n",
      "  $$\n",
      "  100 - 90 = 10 \\text{ 比塞塔/盎司}\n",
      "  $$\n",
      "- 30 盎司节省总额：  \n",
      "  $$\n",
      "  30 \\times 10 = 300 \\text{ 比塞塔}\n",
      "  $$\n",
      "\n",
      "---\n",
      "\n",
      "### 最终答案\n",
      "\n",
      "$$\n",
      "\\boxed{300}\n",
      "$$\n",
      "\n",
      "【思考过程】\n",
      "嗯，好的，我现在要解决这个问题。题目是说大瓶芒果汁是30盎司，售价2700比塞塔；小瓶是6盎司，每瓶600比塞塔。问题是买一大瓶而不是同样体积的小瓶的话，可以省多少钱。对吧？\n",
      "\n",
      "首先，我需要明确题目在问什么。题目问的是购买一大瓶而不是同样体积的小瓶所节省的钱。这里的“同样体积”应该是指总容量相同的情况下，比较购买大瓶和小瓶的花费差异，然后算出节省了多少。也就是说，如果我要得到和一大瓶同样多的芒果汁，如果我买小瓶的话需要买几瓶，然后计算两种购买方式的差价，对吗？\n",
      "\n",
      "首先，大瓶的容量是30盎司，售价2700。小瓶是6盎司，售价600。那我要比较的是，买相当于30盎司的大瓶的话，如果买小瓶的话需要多少钱，然后比较两者的差价。这样节省的钱就是用大瓶的价格减去如果买小瓶的话的总价格，对吧？\n",
      "\n",
      "不过可能需要注意一下，这里是否需要使用整数瓶的小瓶来凑到30盎司。比如，30盎司的话，小瓶每瓶6盎司，所以需要买多少瓶呢？30除以6等于5瓶，对吧？所以需要5瓶小瓶才能得到30盎司的芒果汁。那这时候买五瓶小瓶的总价格是多少呢？5乘以600，等于3000比塞塔。而买一大瓶的话只需要2700，所以节省的钱就是3000减去2700，等于300比塞塔。所以答案应该是300比塞塔？\n",
      "\n",
      "不过，让我再仔细检查一下，有没有哪里考虑不周的地方。首先，我需要确认的是，是否必须用整数瓶的小瓶来凑到30盎司，题目里没有说明是否可以购买部分瓶装的小瓶芒果汁，所以应该假设只能买整数瓶的小瓶。这样的话，买五瓶小瓶刚好是30盎司，总费用是5×600=3000，而大瓶是2700，所以确实节省了300。不过有没有可能题目中的“同样体积”指的是刚好相同的体积，或者可能需要考虑其他情况？\n",
      "\n",
      "或者，是否存在另一种情况，比如购买大瓶和小瓶的总容量相同的话，但可能不同的计算方式？比如，比如题目有没有可能问的是每盎司的价格差异，但题目明确说是购买一大瓶而不是同样体积的小瓶，所以应该是总容量一样时的总价差。\n",
      "\n",
      "不过再仔细想想，有没有可能我的计算哪里错了呢？比如，是否小瓶的单价更便宜？或者题目有没有可能问的是每盎司的节省，不过题目说“购买一大瓶而不是同样体积的小瓶可以节省多少”，所以应该就是总差价。比如买一大瓶代替购买相当于30盎司的小瓶装的话，也就是五瓶小瓶，花费差就是节省的钱，对吧？\n",
      "\n",
      "不过为了确保正确，再从另一个角度考虑，比如计算每盎司的价格，然后比较。\n",
      "\n",
      "大瓶的每盎司价格是2700 / 30 = 90比塞塔每盎司。\n",
      "\n",
      "小瓶的每盎司价格是600 / 6 = 100比塞塔每盎司。\n",
      "\n",
      "所以每盎司差价是10元，买30盎司的话节省的总量就是每盎司省10元乘以30盎司，等于300，和之前的结果一致。嗯，没错，两种方法都得到一样的结果。所以应该是对的。\n",
      "\n",
      "不过再想想，题目是否有可能问的是买大瓶比买小瓶（小瓶的数量等于大瓶的数量）？不过题目里说“而不是同样体积的小瓶”，所以应该是体积相同的情况下比较价格。也就是说，对于30盎司的量来说，买大瓶是2700，而买小瓶的话需要五瓶，总共3000，所以节省了3000-2700=300。是的，没错。\n",
      "\n",
      "看来两种方法都得到同样的结果，所以答案应该是300比塞塔。不过再检查一次计算过程，避免简单的算术错误。\n",
      "\n",
      "首先，30盎司的大瓶，价格2700。小瓶是6盎司，600元一瓶。\n",
      "\n",
      "五瓶小瓶的话，6×5=30盎司，总价格是5×600=3000。两者之差是3000-2700=300。是的，没错。\n",
      "\n",
      "或者，每盎司大瓶便宜10元，总共30盎司，所以30×10=300。没错。\n",
      "\n",
      "所以答案应该是300比塞塔。\n",
      "\n",
      "不过，有没有可能哪里漏看了单位或者转换的问题？题目中的盎司都是同一个单位，比塞塔也是同一货币单位，所以应该没问题。\n",
      "\n",
      "总之，我认为正确答案是300比塞塔，购买大瓶可以节省这个数额。\n",
      "\n",
      "**答案**\n",
      "\\boxed{300}\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "第 4 条推理结果\n",
      "================================================================================\n",
      "\n",
      "【问题】\n",
      "能否创造一个反派角色的详细背景故事给我听听？\n",
      "\n",
      "【回答】\n",
      "好的！以下是一个原创反派角色的详细背景故事，包含世界观设定、性格矛盾点和关键事件：\n",
      "\n",
      "---\n",
      "\n",
      "### **角色姓名**：维克托·黑棘（Victor Blackthorn）  \n",
      "**称号**：\"血色挽歌\" / \"猩红法则的执刑者\"  \n",
      "**年龄**：37岁（外貌凝固在28岁）  \n",
      "**核心动机**：用血与火重塑一个\"绝对秩序\"的世界，以抹去他童年目睹的混乱与背叛。\n",
      "\n",
      "---\n",
      "\n",
      "### **世界观设定**  \n",
      "故事发生在架空中世纪奇幻大陆\"埃瑟兰\"，这里存在三大魔法学院、七大家族议会和游离在法律外的\"灰烬教团\"。魔法能量源自\"星尘矿脉\"，而维克托的家族\"黑棘\"曾是守护矿脉的古老贵族，却在他幼年时因拒绝向王权低头而被灭族。\n",
      "\n",
      "---\n",
      "\n",
      "### **关键背景故事**  \n",
      "#### **1. 血色摇篮（童年创伤）**  \n",
      "- **家族覆灭**：维克托出生当夜，黑棘城堡被星尘矿脉的贪婪者围攻。他被母亲用禁术\"血肉傀儡\"藏进一具死尸体内，透过眼眶缝隙目睹母亲被撕碎。母亲最后一句话是：\"记住，弱者的血永远要为强者的棋局祭旗。\"  \n",
      "- **诅咒胎记**：因禁术反噬，他右眼变成血红色，眼皮刻着家族徽章的荆棘纹路，遇情绪波动时会渗血。\n",
      "\n",
      "#### **2. 魔法学院的蟑螂（人格扭曲期）**  \n",
      "- **伪装身份入学**：化名\"维克\"进入三大魔法学院之一的\"霜语学院\"，隐瞒血统靠偷窃星尘粉末维生，因过度使用禁术导致左手皮肤石化。  \n",
      "- **霸凌与觉醒**：因出身低微被贵族学生当\"活体实验品\"，在一次濒死体验中觉醒\"血肉重构\"能力——将伤口转化为武器。他割下施暴者的舌头缝合自己的伤口，并留下著名台词：\"疼痛是最好的教师，而你们…只是教材。\"  \n",
      "- **禁忌研究**：偷取学院禁书《血契通灵术》，发现星尘能量本质是\"凝固的亡魂尖叫\"，从此厌恶传统魔法体系。\n",
      "\n",
      "#### **3. 血色黎明事件（黑化转折点）**  \n",
      "- **复仇仪式**：25岁时带领十二名被学院压迫的学生组成\"猩红议会\"，用血魔法将霜语学院沉入地底。他亲手扯断院长的脊椎制成法杖，却在核心密室发现母亲当年被篡改的灭族诏书——家族本可投降却选择反抗。  \n",
      "- **第一次自我怀疑**：仪式中一名平民仆役女孩为保护他而死，女孩的血触发他童年记忆，导致血魔法失控焚毁半个城镇。他开始在日记本写：\"纯洁的牺牲者…是我公式里的变量吗？\"\n",
      "\n",
      "---\n",
      "\n",
      "### **性格矛盾点**  \n",
      "- **极端理性与感性执念**：能精准计算星尘能量流动，却执着于复刻母亲的死亡场景（所有基地都建造在荆棘花盛开的血色沼泽）。  \n",
      "- **秩序狂热与破坏欲**：建立\"猩红法庭\"审判腐败贵族，但判决方式永远是将犯人制成血魔法傀儡。  \n",
      "- **对\"弱者\"的复杂态度**：会为保护一名贫民窟儿童屠城，却在事后冷笑着说：\"你该感谢我赐予你成为故事的资格。\"\n",
      "\n",
      "---\n",
      "\n",
      "### **能力与弱点**  \n",
      "- **血契通灵**：以自身血液为媒介操控亡者，代价是每使用一次就会失去一种情感记忆。  \n",
      "- **猩红领域**：半径三公里内所有液体变为致命血酸，但需要至少三名活人献祭维持。  \n",
      "- **致命弱点**：因童年禁术残留，无法触碰\"月光蔷薇\"的花粉，接触后能力会暴走反噬。\n",
      "\n",
      "---\n",
      "\n",
      "### **深层秘密（可扩展剧情线）**  \n",
      "- **母亲的真相**：维克托母亲实为\"灰烬教团\"高层，当年拒绝交出星尘矿脉控制权引发灭族。  \n",
      "- **星尘诅咒**：长期使用星尘能量导致他肉体不朽，但灵魂正在被矿脉中的古神意识侵蚀。  \n",
      "- **双面间谍**：猩红议会核心成员\"苍白修女\"实为教会卧底，暗中收集他屠杀平民的证据。\n",
      "\n",
      "---\n",
      "\n",
      "### **经典台词**  \n",
      "\"你们称呼我为恶魔？真可笑——是你们跪着把恶魔的王座打造得如此华丽。\"  \n",
      "\"法律是血写的，历史是血写的，连你们的眼泪…都将为我的公式证明。\"  \n",
      "\n",
      "---\n",
      "\n",
      "这个角色可以衍生出多种故事线：  \n",
      "- 与童年救他的平民女孩后裔相遇（对方成为圣殿骑士）  \n",
      "- 发现星尘矿脉深处沉睡着比他更古老的血魔法生物  \n",
      "- 被教会利用来清除其他反叛势力，最终反噬雇主  \n",
      "\n",
      "需要补充其他维度可以随时告诉我！\n",
      "\n",
      "【思考过程】\n",
      "嗯，用户让我创造一个反派角色的详细背景故事。首先，我需要确定用户的需求是什么。他们可能是在寻找灵感，或者为小说、游戏、剧本创作角色。反派角色通常需要有深度，不能只是单纯的坏人，所以得设计一个复杂的背景，让他的行为有合理性。\n",
      "\n",
      "接下来，我得考虑反派的动机。常见的动机有复仇、权力欲望、保护某人或某物，或者被过去的创伤所驱使。用户可能希望这个反派有真实感，所以需要赋予他一些人性化的特质，让读者或观众能产生共鸣。\n",
      "\n",
      "然后，设定世界观。用户没有指定类型，可能是奇幻、科幻或现代。考虑到用户提到“魔法学院”和“血色黎明”，可能偏向奇幻题材。所以背景设定在一个有魔法和古老家族的世界里，这样能增加戏剧性和冲突点。\n",
      "\n",
      "角色名字和称号也很重要。需要听起来有威慑力，同时反映他的背景或能力。比如“维克托·黑棘”和称号“血色挽歌”，既有贵族气息，又带有血腥和悲剧色彩。\n",
      "\n",
      "家庭背景方面，被家族抛弃和母亲被杀，这样的设定能解释他的仇恨和动机。加入魔法学院遭受欺凌，进一步塑造他的扭曲心理。关键事件如母亲的死亡和学院的背叛，这些转折点能推动他的黑化过程。\n",
      "\n",
      "能力方面，血魔法既有视觉冲击力，又与他的背景相关联。弱点的设计要合理，比如依赖血液和反噬，增加角色的脆弱性和故事的张力。\n",
      "\n",
      "最后，深层动机可能需要更隐秘的原因，比如复活母亲或对抗更大的威胁，这样能让角色更有层次。同时，与其他势力的关联可以扩展故事的广度，为后续发展留空间。\n",
      "\n",
      "需要确保故事有足够的细节，让角色立体，同时保持逻辑连贯。可能用户还希望这个反派有独特的个性和魅力，所以加入一些矛盾点，比如对弱者的同情，但手段极端，这样增加复杂性。总之，综合这些元素，构建一个既可信又有吸引力的反派背景。\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "第 5 条推理结果\n",
      "================================================================================\n",
      "\n",
      "【问题】\n",
      "在大学里应该谈恋爱吗？\n",
      "\n",
      "【回答】\n",
      "在大学期间是否谈恋爱是一个因人而异的选择，没有绝对的“应该”或“不应该”。关键在于你是否准备好承担一段关系的责任，以及如何平衡恋爱与其他重要的人生目标。以下是一些思考角度，供你参考：\n",
      "\n",
      "---\n",
      "\n",
      "### **一、支持大学谈恋爱的理由**\n",
      "1. **成长机会**  \n",
      "   - 恋爱是探索自我、理解他人需求的过程，有助于提升沟通、共情和解决冲突的能力。\n",
      "   - 亲密关系中的磨合可能帮助你更清楚自己的价值观、边界感和未来期待。\n",
      "\n",
      "2. **情感支持**  \n",
      "   - 大学阶段面临学业压力、未来规划的迷茫，伴侣可能成为情感支柱，提供陪伴和鼓励。\n",
      "   - 共同分享经历（如备考、社团活动）可能加深联结，创造独特回忆。\n",
      "\n",
      "3. **自由的环境**  \n",
      "   - 相比工作后受现实条件（经济、地域、家庭）限制，大学阶段尝试恋爱的机会成本较低。\n",
      "   - 多元的社交环境（社团、课程、活动）更容易接触到志同道合的人。\n",
      "\n",
      "4. **试错的可能**  \n",
      "   - 年轻时经历恋爱中的“不完美”（如争吵、分手）是成长的一部分，为未来更成熟的亲密关系积累经验。\n",
      "\n",
      "---\n",
      "\n",
      "### **二、需要谨慎考虑的因素**\n",
      "1. **时间与精力分配**  \n",
      "   - 大学是积累专业知识、探索兴趣、建立社交网络的关键期。如果恋爱影响学业或个人发展，需权衡取舍。\n",
      "   - 长期关系需要投入时间维护，若双方目标不一致（如一方计划深造，另一方急于工作），可能产生矛盾。\n",
      "\n",
      "2. **情感成熟度**  \n",
      "   - 初恋或仓促开始的关系可能因缺乏经验而面临挑战，比如过度依赖、控制欲或沟通不畅。\n",
      "   - 需警惕“为了恋爱而恋爱”的心态，避免将亲密关系等同于自我价值的证明。\n",
      "\n",
      "3. **现实挑战**  \n",
      "   - 毕业后的异地、就业地域差异、家庭背景差异等问题，可能让校园恋情面临考验（但也是检验关系的机会）。\n",
      "   - 若一方以结婚为前提规划，另一方仅以体验为主，则需提前沟通目标是否一致。\n",
      "\n",
      "---\n",
      "\n",
      "### **三、如何判断自己是否适合开始一段关系？**\n",
      "1. **你为何想恋爱？**  \n",
      "   - 是出于孤独、从众心理，还是真正欣赏某人并愿意共同成长？内在动机比“是否应该”更重要。\n",
      "2. **能否平衡多方需求？**  \n",
      "   - 是否能在约会的同时保持学习、社交、独处的平衡？健康的关系应拓宽人生，而非缩小自由空间。\n",
      "3. **是否做好面对冲突的准备？**  \n",
      "   - 恋爱中的分歧（如消费观念、未来规划）需要理性沟通，而非仅靠感性处理。\n",
      "\n",
      "---\n",
      "\n",
      "### **四、无论是否恋爱，都可以关注的事**\n",
      "- **建立多元人际关系**：友谊、师生关系、社团联结同样能提供支持，不必将所有情感需求寄托于恋人。\n",
      "- **自我投资**：利用大学资源（讲座、实习、兴趣小组）提升能力，培养独立生活和思考的能力。\n",
      "- **开放心态**：爱情可能不期而至，也可能无疾而终，保持对生活的热情比执着于某种既定模式更重要。\n",
      "\n",
      "---\n",
      "\n",
      "### **总结**\n",
      "大学时期谈恋爱没有标准答案。**如果你遇到让你感到安心、互相尊重、共同进步的人，不妨尝试**；如果你更希望专注学业、探索自我，或暂时没遇到合适的人，也无需焦虑。人生每个阶段都有不同的课题，重要的是**忠于自己的节奏，不被外界的“应该”绑架**。  \n",
      "最终，无论是恋爱带来的甜蜜与成长，还是独处时的沉淀与积累，都是大学经历中宝贵的财富。\n",
      "\n",
      "【思考过程】\n",
      "嗯，用户问的是在大学里应该谈恋爱吗？这个问题看起来挺常见的，但其实背后可能有很多不同的因素需要考虑。首先，我得确定用户的具体情况，比如他是刚入学的新生，还是已经有了一些经历的学生？可能他正在犹豫要不要开始一段恋情，或者他的朋友都在谈恋爱，而他自己还没遇到合适的人，所以有点迷茫。\n",
      "\n",
      "接下来，我需要考虑不同的观点。支持谈恋爱的理由可能包括个人成长、社交技能提升、情感支持，以及大学环境相对自由，适合探索人际关系。而反对的观点可能涉及时间管理和学业压力，初恋可能带来的不成熟关系，以及毕业后面临的现实问题，比如异地恋或职业规划冲突。\n",
      "\n",
      "还要考虑到不同学生的个性差异。有些人天生外向，享受亲密关系带来的快乐，而内向的人可能更倾向于先专注于学业和个人发展。另外，文化背景也可能影响观点，比如在一些比较传统的家庭中，家长可能不支持大学期间谈恋爱，担心影响学业。\n",
      "\n",
      "用户可能没有直接说出的深层需求是什么呢？可能他真正想知道的是如何在恋爱和学业之间取得平衡，或者如何判断自己是否准备好进入一段关系。也有可能他担心被排除在外，或者害怕受伤，所以需要一些指导来评估自己的情况。\n",
      "\n",
      "需要避免给出绝对化的建议，比如“必须”或“不要”，而是提供一个权衡利弊的框架，让用户自己做出决定。同时，可以加入一些实际的建议，比如如何管理时间、沟通技巧，或者如何处理可能的冲突。\n",
      "\n",
      "还要注意语气，保持中立和支持，避免让用户觉得被评判。可能需要提醒他们，无论选择如何，都有不同的成长路径，没有对错之分。此外，提到如果决定尝试，如何建立健康的关系，比如互相支持、尊重对方的空间，以及沟通的重要性。\n",
      "\n",
      "最后，确保回答结构清晰，分点列出优缺点，然后给出一个总结性的建议，帮助用户根据自己的情况做出最适合自己的选择。\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 显示前5条结果（完整内容）\n",
    "if results:\n",
    "    print(f\"共有 {len(results)} 条推理结果\\n\")\n",
    "    print(\"前5条推理结果（完整内容）：\\n\")\n",
    "    \n",
    "    for i, result in enumerate(results[:5]):\n",
    "        print(\"=\"*80)\n",
    "        print(f\"第 {i+1} 条推理结果\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # result 是一个列表，包含对话消息\n",
    "        for msg in result:\n",
    "            if 'role' in msg:\n",
    "                if msg['role'] == 'user':\n",
    "                    print(f\"\\n【问题】\")\n",
    "                    print(msg['content'])\n",
    "                elif msg['role'] == 'assistant':\n",
    "                    print(f\"\\n【回答】\")\n",
    "                    print(msg['content'])\n",
    "            elif 'reasoning_content' in msg:\n",
    "                print(f\"\\n【思考过程】\")\n",
    "                print(msg['reasoning_content'])\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ce3b92c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "原始数据集中前2条数据的完整内容：\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "第 1 条原始数据\n",
      "================================================================================\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"content\": \"能给我讲一个寓意深刻的故事吗？\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"<think>\\n嗯，用户想要一个寓意深刻的故事。这个请求看似简单，但背后可能有几种需求：ta可能正在寻找人生启发，或是需要素材用于教学分享，也可能只是睡前想读点有深度的内容。从“寓意深刻”这个关键词看，ta应该厌倦了快餐式鸡汤，想要能引发真实思考的故事。\\n\\n让我想想该怎么设计……首先得避开陈词滥调。现在网上太多“小和尚问大师”式的伪寓言，用户肯定看腻了。最好用陌生化场景，比如把哲理藏在科幻设定里？但也不能太晦涩，毕竟用户没指定要硬核内容。\\n\\n啊，有了！可以借鉴《黑镜》那种科技寓言的调性，但去掉黑暗感。核心矛盾就设定在“记忆”上——人类总以为记住越多越好，但或许遗忘才是智慧？这个立意最近在心理学界也有讨论，2017年《自然》杂志就发过论文说遗忘是大脑主动清理的过程……\\n\\n主角名字很重要。不能太普通像“张三”，但也不能像科幻小说里乱起“赛博·星尘”这种。就叫林默吧，“林”有自然感，“默”暗喻沉默的智慧，用户应该能隐约感觉到名字的隐喻。\\n\\n故事结构得像剥洋葱：第一层是未来科技设定（记忆银行），第二层是主角困境（母亲失忆），第三层反转（主动删除记忆）。关键要让那个“删除童年记忆”的选择显得合理——不能写成自毁，得让读者最后觉得“啊，原来放下才是救赎”。\\n\\n特别要注意结尾。不能直接说教“所以我们要学会遗忘”，得用画面感呈现：当主角看着母亲空白的眼神，突然发现阳光里的尘埃比记忆更真实。对了，最后一句一定要落在“此刻”上，和开头“未来科技”形成轮回感……\\n\\n伏笔要埋在细节里。比如开头写记忆银行广告词“记住一切，拥有永恒”，后面主角删除记忆时，可以让他听见同样的广告词在街头循环播放，但这次他听出其中的荒谬。这种呼应用户应该会心一笑。\\n\\n等等，用户会不会觉得太沉重？得加点温暖元素。让母亲失忆后反而更爱笑，虽然认不出儿子但会摸陌生人的手说“你眼睛真亮”——这种无条件的善意，可能比带着记忆的亲情更接近爱的本质。\\n\\n反复推敲那个核心反转：为什么删除记忆是救赎？不能简单说“忘掉就快乐了”。要设计成主角发现母亲每次回忆都会重新经历丧夫之痛，而遗忘反而让她每天都是“新”的开始。就像……每天清晨的鸟鸣，从不需要昨天的曲谱。\\n\\n现在所有碎片齐了：科技外壳、情感内核、哲学反转。马上动笔，但得控制字数。用户没指定长度，800字左右应该刚好——太短讲不透哲理，太长可能失去锋利感。\\n</think>\\n\\n## 记忆银行\\n\\n未来都市里，记忆银行”如雨后春笋般林立，它们以“记住一切，拥有永恒”为广告词，吸引着无数人前来存入记忆。人们将珍贵的、痛苦的、甚至琐碎的片段，都交付给冰冷的机器，以为这样便能永远握住时光的尾巴。林默也成了其中一员，他小心翼翼地将母亲病重前所有清晰的记忆都存入银行，仿佛这样就能在母亲彻底迷失于阿尔茨海默症的迷雾之前，为她保留住最后一座灯塔。\\n\\n然而，母亲终究还是被疾病吞噬了。她坐在窗边，眼神空洞，如同被风沙磨平了刻痕的石碑，再也映照不出林默的面容。林默每日都带着记忆银行里提取出的“昨日”回家，将母亲年轻时的音容笑貌、他们共同经历的点点滴滴，通过设备投射在母亲面前。母亲起初会茫然地伸出手，指尖触碰到虚幻的光影，却只留下一片冰凉。后来，她连这微弱的反应也渐渐消失，只是呆坐，仿佛灵魂早已被抽离，只余下躯壳在时间的河流中漂浮。\\n\\n林默的心被日复一日的徒劳掏得空空如也。他坐在母亲床边，凝视着母亲那双曾经盛满慈爱、如今却如枯井般干涸的眼睛，忽然间，他忆起母亲在病痛初显时曾紧紧攥住他的手，声音颤抖：“小默，我……我好像又忘了你爸爸走那天的事了……可那痛，怎么还在？”——原来遗忘并未带走痛苦，它只是将痛苦从记忆的牢笼里释放出来，让它在每一次空白的间隙里重新扎根、疯长。母亲每一次被强行唤醒的“昨日”，都像一把钝刀，反复切割着早已溃烂的伤口。\\n\\n林默的心猛地一沉，仿佛坠入无底深渊。他长久以来以为的“守护”，竟成了母亲无法挣脱的刑具。他颤抖着，第一次没有提取任何记忆片段，只是轻轻握住母亲枯瘦的手。母亲缓缓转过头，目光迟滞地落在他脸上，竟慢慢漾开一丝极淡、极陌生的笑意，如同初春冰面下悄然涌动的暖流。她喃喃道：“你……眼睛真亮，像我儿子小时候……” 这陌生的温柔，竟比任何被机器复刻的“真实”更接近爱的温度。\\n\\n林默豁然醒悟：原来母亲并非需要背负所有记忆的沉重行囊才能活着；她真正需要的，是此刻阳光里飘浮的微尘，是掌心相触的微温，是眼前这张陌生面孔所传递的、无需记忆支撑的善意。他决然起身，走向记忆银行。在冰冷的终端前，他删除了所有为母亲存储的记忆——那些他以为珍贵的、实则不断制造新痛楚的“昨日”。当最后一段数据化为虚无的0与1，林默感到一种奇异的轻松，仿佛卸下了千斤重担。\\n\\n他回到母亲身边，窗外阳光正慷慨地洒落。母亲安静地坐在光里，脸上没有往日被记忆惊扰的惶惑，只有一种近乎孩童般的澄澈安宁。她偶尔会看看林默，眼神空茫，却不再有痛苦的涟漪。林默也安静地坐着，不再试图唤醒沉睡的幽灵。他第一次真正看清了母亲眼角新添的细纹，看清了阳光里飞舞的微尘，看清了此刻——这无需任何记忆凭证、却无比真实的“现在”。\\n\\n原来，真正的永恒并非锁在冰冷的数据库里，它只存在于放下重负后，彼此掌心相贴的微温之中；当人终于学会让记忆如秋叶般飘落，心才得以在当下这片无垠的晴空下，重新呼吸到自由的风。\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "数据类型: <class 'dict'>\n",
      "顶层键: ['messages']\n",
      "messages 列表长度: 2\n",
      "messages 中的角色: ['user', 'assistant']\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "第 2 条原始数据\n",
      "================================================================================\n",
      "{\n",
      "  \"messages\": [\n",
      "    {\n",
      "      \"content\": \"证明：如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。\",\n",
      "      \"role\": \"user\"\n",
      "    },\n",
      "    {\n",
      "      \"content\": \"<think>\\n嗯，我现在要证明的是：如果A和B是两个n阶方阵，并且满足AB=BA，那么tr(AB)=tr(BA)。不过，等一下，题目里已经说了AB=BA，那这样的话AB和BA本来就是同一个矩阵，所以它们的迹当然相等啊。这样的话，这个命题是不是有点奇怪？或者说题目是不是有问题？\\n\\n让我再仔细看一下题目。题目说：“如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。”可是，不管AB和BA是否相等，迹都是相等的，对吧？我记得迹有一个性质，就是tr(AB)=tr(BA)，无论A和B是否可交换。也就是说，即使AB≠BA，它们的迹也是相等的。所以题目中给出的条件AB=BA可能是个干扰项，或者题目可能有误？\\n\\n不过可能我哪里记错了？让我再确认一下这个迹的性质。\\n\\n首先，迹的定义是矩阵主对角线元素之和，即tr(A) = ΣA_ii。\\n\\n现在考虑tr(AB)，假设A和B都是n×n矩阵，那么AB的第i行第i列元素是Σ_{k=1}^n A_ik B_ki，所以tr(AB) = Σ_{i=1}^n Σ_{k=1}^n A_ik B_ki。\\n\\n同样地，tr(BA) = Σ_{i=1}^n Σ_{k=1}^n B_ik A_ki。\\n\\n不过这两个表达式看起来好像不一样，但其实可以交换求和的顺序。比如对于tr(AB)，我们可以把双重求和交换顺序：\\n\\ntr(AB) = Σ_{i=1}^n Σ_{k=1}^n A_ik B_ki = Σ_{k=1}^n Σ_{i=1}^n B_ki A_ik = Σ_{k=1}^n Σ_{i=1}^n (B A)_{kk} ?\\n\\n不对，让我再仔细算一遍。\\n\\n实际上，tr(AB) = Σ_{i=1}^n (AB)_{ii} = Σ_{i=1}^n Σ_{j=1}^n A_{ij} B_{ji}\\n\\n同样，tr(BA) = Σ_{i=1}^n (BA)_{ii} = Σ_{i=1}^n Σ_{j=1}^n B_{ij} A_{ji}\\n\\n现在比较这两个表达式，tr(AB)中的项是A_{ij}B_{ji}，而tr(BA)中的项是B_{ij}A_{ji}。如果我们让j和i在tr(AB)中互换，比如令i'=j，j'=i，那么tr(AB) = Σ_{i=1}^n Σ_{j=1}^n A_{ij} B_{ji} = Σ_{j=1}^n Σ_{i=1}^n A_{ji} B_{ij} = tr(BA)\\n\\n因为双重求和的顺序可以交换，所以这两个表达式其实是相等的。因此，不管A和B是否可交换，tr(AB)=tr(BA)总是成立的。\\n\\n所以题目中给出的条件AB=BA其实是多余的，或者说这个命题本身在没有这个条件的情况下也成立。因此，题目可能有问题，或者可能是想让我证明在AB=BA的情况下tr(AB)=tr(BA)，但实际上这个结论不需要AB=BA的条件。\\n\\n不过可能题目是想让我用AB=BA这个条件来证明tr(AB)=tr(BA)，但既然这个结论本来就是对的，不管AB是否等于BA，那这个条件可能只是用来迷惑人的？\\n\\n或者可能题目有其他的意图？比如可能原题是别的内容，比如证明tr(A^k B^m) = tr(B^m A^k)之类的，但被错误地写成了现在这样？\\n\\n不过根据用户给出的问题，确实是“如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)”。但根据上面的分析，这个结论在AB不一定等于BA的时候也成立，所以这里的条件AB=BA可能是多余的。\\n\\n不过可能用户想确认的是，在AB=BA的情况下，tr(AB)=tr(BA)，但其实这个等式总是成立的，所以可能题目有错别字，或者需要更深入的思考？\\n\\n或者可能用户想让我证明tr(AB)=tr(BA)，而题目中给出的AB=BA是误导？或者可能用户自己混淆了问题？\\n\\n让我再查一下资料，确认迹的这个性质是否正确。\\n\\n根据线性代数的知识，迹的一个重要性质是tr(AB)=tr(BA)，无论A和B是否可交换。例如，在维基百科上，迹的性质中确实提到：对于任意两个矩阵A和B，只要乘积AB和BA有定义（即A是m×n，B是n×m），则tr(AB)=tr(BA)。当A和B都是n×n的时候，当然成立。\\n\\n所以，题目中的条件AB=BA其实是不必要的，因为tr(AB)=tr(BA)恒成立。因此，这个题目可能是一个陷阱题，或者题目有误。\\n\\n不过，可能用户希望我按照题目的要求来证明，即使条件多余，也要用AB=BA来证明tr(AB)=tr(BA)。不过这样的话，证明就非常简单，因为如果AB=BA，那么AB和BA是同一个矩阵，所以它们的迹当然相等。\\n\\n不过这似乎太简单了，可能题目原本想问的是其他问题，比如证明tr(AB)=tr(A)tr(B)或者其他什么，但被错误地写成了现在这样？\\n\\n或者可能题目是想让我证明在AB=BA的情况下，tr(AB)=tr(BA)，但其实这个等式不需要这个条件？\\n\\n不过不管怎样，作为答题者，我需要按照题目给出的条件来回答。题目说“如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)。”\\n\\n既然AB=BA，那么AB和BA是同一个矩阵，所以tr(AB)=tr(BA)自然成立。不过可能题目希望用迹的某种性质来证明，而不是直接指出AB=BA意味着AB和BA相同？\\n\\n不过这样的话，可能题目的意图是让我用迹的循环性质来证明，即tr(AB)=tr(BA)，而不管AB是否等于BA，但题目中却给出了AB=BA的条件，这可能是一个误导。\\n\\n不过可能用户在这里犯了一个错误，把原本应该证明的tr(AB)=tr(BA)（无需交换条件）误写成需要AB=BA的条件？\\n\\n或者可能题目是另一个问题，比如证明tr(AB)=tr(A)tr(B)，但需要AB=BA的条件？\\n\\n不过根据现有的信息，我需要解决当前的问题。\\n\\n可能的思路：\\n\\n1. 直接指出因为AB=BA，所以AB和BA是同一个矩阵，因此它们的迹相等。\\n\\n2. 或者，即使不利用AB=BA，直接证明tr(AB)=tr(BA)，并指出条件AB=BA是多余的。\\n\\n不过题目明确要求“如果A和B是两个n阶方阵，且AB=BA，那么tr(AB)=tr(BA)”，所以可能需要按照题目的条件来写证明，即使条件多余。\\n\\n不过可能出题人认为学生可能会误以为只有当AB=BA时tr(AB)=tr(BA)，但实际上这个等式总是成立的，所以这个题目可能是为了纠正这个误解？\\n\\n不过无论如何，我需要给出一个证明过程。\\n\\n可能的证明步骤：\\n\\n由于AB=BA，所以AB和BA是同一个矩阵，因此它们的迹必然相等，即tr(AB)=tr(BA)。\\n\\n不过这个证明太简单了，可能不是出题人的本意，但根据题目的条件，这是正确的。\\n\\n不过可能题目有错别字，比如应该是证明tr(A+B)=tr(A)+tr(B)，或者其他什么，但目前的信息下只能按题目来。\\n\\n或者可能题目是想让我用AB=BA来证明tr(AB)=tr(BA)，但其实这个等式不需要这个条件，所以可能出题人自己混淆了。\\n\\n不过作为答题者，我需要给出一个符合题目要求的证明，即使条件多余。\\n\\n所以，可能的证明如下：\\n\\n证明：已知AB=BA，因此AB和BA是同一个矩阵。根据迹的定义，同一个矩阵的迹是唯一的，因此tr(AB)=tr(BA)。\\n\\n不过这可能太简单了，可能老师或出题人希望用迹的某种性质来证明，比如通过展开迹的表达式，但因为AB=BA，所以可以交换顺序？\\n\\n不过实际上，即使AB≠BA，tr(AB)=tr(BA)仍然成立，所以这里的条件AB=BA并没有被用到。\\n\\n或许题目原本是另一个问题，比如证明tr(AB)=tr(A)tr(B)，但需要AB=BA的条件？不过这个等式显然不成立，例如取A=B=I，那么左边是tr(I^2)=tr(I)=n，右边是tr(I)*tr(I)=n*n，显然不等，除非n=1。\\n\\n或者可能题目是证明tr(AB)=tr(BA)在AB=BA的情况下，但其实这个等式总是成立，所以可能题目有误。\\n\\n不过根据用户给出的问题，我需要完成这个证明，所以可能应该指出这个条件是多余的，但按照题目要求写出证明。\\n\\n或者可能用户自己犯了错误，把原本的题目写错了？\\n\\n比如，可能原题是证明如果AB=BA，那么tr(AB)=tr(A)tr(B)，或者其他什么，但被错误地写成了现在这样？\\n\\n不过在没有更多信息的情况下，我只能基于现有的题目来回答。\\n\\n综上所述，可能的结论是：\\n\\n题目中的条件AB=BA是多余的，因为对于任意两个方阵A和B，tr(AB)=tr(BA)恒成立。因此，当AB=BA时，自然有tr(AB)=tr(BA)。\\n\\n不过为了符合题目的要求，可能需要写出这个证明，即使条件多余。\\n\\n所以，正式的证明可以这样写：\\n\\n证明：对于任意两个n阶方阵A和B，迹的性质告诉我们tr(AB)=tr(BA)。因此，当AB=BA时，显然tr(AB)=tr(BA)成立。\\n\\n不过可能需要更详细的推导，比如展开迹的表达式。\\n\\n例如：\\n\\ntr(AB) = Σ_{i=1}^n (AB)_{ii} = Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki}\\n\\n同样，tr(BA) = Σ_{i=1}^n (BA)_{ii} = Σ_{i=1}^n Σ_{k=1}^n B_{ik}A_{ki}\\n\\n现在，如果我们交换求和的顺序和变量名，可以看到这两个表达式实际上是相等的：\\n\\ntr(AB) = Σ_{i=1}^n Σ_{k=1}^n A_{ik}B_{ki} = Σ_{k=1}^n Σ_{i=1}^n B_{ki}A_{ik} = tr(BA)\\n\\n这里，我们注意到双重求和的顺序可以交换，而且变量名i和k是哑变量，可以互换。因此，tr(AB)=tr(BA)对任意A和B成立，包括当AB=BA的情况。\\n\\n因此，题目中的条件AB=BA虽然成立，但并不是必要的，结论依然成立。\\n\\n不过题目特别指出在AB=BA的情况下，所以可能需要强调这一点，但其实没有必要。\\n\\n不过可能出题人希望学生意识到即使AB≠BA，这个等式也成立，所以这个题目可能是一个教学点，让学生不要误以为需要交换性条件。\\n\\n不过根据用户的问题，可能只需要按照题目要求写出证明，所以可能直接指出因为AB=BA，所以tr(AB)=tr(BA)。\\n\\n不过这样显得太简单，可能用户希望更严谨的证明，比如使用迹的性质。\\n\\n总结：\\n\\n正确的结论是tr(AB)=tr(BA)对所有方阵A,B成立，无需AB=BA的条件。因此，题目中的条件是多余的，但结论依然正确。\\n\\n不过为了完成题目，可以这样写：\\n\\n证明：由于AB=BA，所以AB和BA是同一个矩阵，因此它们的迹相等，即tr(AB)=tr(BA)。\\n\\n或者更详细地：\\n\\n因为AB=BA，所以矩阵AB等于矩阵BA。根据迹的定义，同一个矩阵的迹是唯一的，因此tr(AB)=tr(BA)。\\n\\n不过可能老师希望学生用迹的循环性质来证明，而不是直接利用AB=BA，但在这个情况下，因为AB=BA，所以直接得出结论即可。\\n\\n不过可能用户的问题存在错误，但作为答题者，只能根据题目给出的条件来回答。\\n\\n因此，最终的证明应该是这样的。\\n</think>\\n\\n**证明：**\\n\\n给定两个 $ n $ 阶方阵 $ A $ 和 $ B $，且满足 $ AB = BA $，需证明 $ \\\\text{tr}(AB) = \\\\text{tr}(BA) $。\\n\\n---\\n\\n### **关键观察**\\n尽管题目中给出了条件 $ AB = BA $，但事实上，**对于任意两个 $ n $ 阶方阵 $ A $ 和 $ B $**，迹的性质保证了：\\n$$\\n\\\\text{tr}(AB) = \\\\text{tr}(BA)\\n$$\\n这一结论**不依赖于 $ AB = BA $** 的条件。因此，题设中的条件 $ AB = BA $ 是**冗余的**，但结论依然成立。\\n\\n---\\n\\n### **直接证明（基于题设条件）**\\n\\n由于 $ AB = BA $，矩阵 $ AB $ 和 $ BA $ 是**完全相同的矩阵**。根据迹的定义，**同一矩阵的迹是唯一的**，因此：\\n$$\\n\\\\text{tr}(AB) = \\\\text{tr}(BA)\\n$$\\n\\n---\\n\\n### **更一般的证明（无需 $ AB = BA $）**\\n\\n若需进一步验证，可从迹的定义出发：\\n\\n- 设 $ AB $ 的第 $ i $ 行第 $ i $ 列元素为：\\n  $$\\n  (AB)_{ii} = \\\\sum_{k=1}^n A_{ik} B_{ki}\\n  $$\\n  因此：\\n  $$\\n  \\\\text{tr}(AB) = \\\\sum_{i=1}^n \\\\sum_{k=1}^n A_{ik} B_{ki}\\n  $$\\n\\n- 同理，$ BA $ 的迹为：\\n  $$\\n  \\\\text{tr}(BA) = \\\\sum_{i=1}^n \\\\sum_{k=1}^n B_{ik} A_{ki}\\n  $$\\n\\n通过交换求和顺序和变量名（令 $ i \\\\leftrightarrow k $），可得：\\n$$\\n\\\\text{tr}(AB) = \\\\sum_{k=1}^n \\\\sum_{i=1}^n B_{ki} A_{ik} = \\\\text{tr}(BA)\\n$$\\n\\n这表明，**无论 $ AB $ 是否等于 $ BA $**，迹的等式恒成立。\\n\\n---\\n\\n### **结论**\\n\\n题目中给出的条件 $ AB = BA $ 虽然成立，但并非必要。**对于任意 $ n $ 阶方阵 $ A $ 和 $ B $**，均有：\\n$$\\n\\\\text{tr}(AB) = \\\\text{tr}(BA)\\n$$\\n因此，在 $ AB = BA $ 的条件下，结论 $ \\\\text{tr}(AB) = \\\\text{tr}(BA) $ 自然成立。\\n\\n---\\n\\n**最终答案：**  \\n由于 $ AB = BA $，矩阵 $ AB $ 与 $ BA $ 完全相同，故其迹必然相等，即 $ \\\\text{tr}(AB) = \\\\text{tr}(BA) $。  \\n**证毕。**\",\n",
      "      \"role\": \"assistant\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "\n",
      "数据类型: <class 'dict'>\n",
      "顶层键: ['messages']\n",
      "messages 列表长度: 2\n",
      "messages 中的角色: ['user', 'assistant']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## 查看原始数据集中前2条数据的完整格式\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"原始数据集中前2条数据的完整内容：\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 重新读取前2条原始数据\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    for idx in range(2):\n",
    "        line = f.readline()\n",
    "        if line.strip():\n",
    "            data = json.loads(line.strip())\n",
    "            \n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"第 {idx+1} 条原始数据\")\n",
    "            print(f\"{'='*80}\")\n",
    "            \n",
    "            # 完整打印，使用缩进格式化\n",
    "            print(json.dumps(data, ensure_ascii=False, indent=2))\n",
    "            \n",
    "            print(f\"\\n数据类型: {type(data)}\")\n",
    "            print(f\"顶层键: {list(data.keys()) if isinstance(data, dict) else '不是字典类型'}\")\n",
    "            \n",
    "            if isinstance(data, dict) and 'messages' in data:\n",
    "                print(f\"messages 列表长度: {len(data['messages'])}\")\n",
    "                print(f\"messages 中的角色: {[msg.get('role') for msg in data['messages'] if isinstance(msg, dict)]}\")\n",
    "            \n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05488d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔄 数据格式转换\n",
      "============================================================\n",
      "\n",
      "配置信息:\n",
      "  输入文件: /data/private/outputs/qwen3_inference_1000_samples.jsonl\n",
      "  输出文件: /data/private/outputs/qwen3_sft_llama_factory.jsonl\n",
      "  输出格式: openai_with_thinking\n",
      "  思考处理: merge\n",
      "\n",
      "🔄 开始转换...\n",
      "\n",
      "✅ 转换完成！\n",
      "  成功: 1000 条\n",
      "  失败: 0 条\n",
      "  输出文件: /data/private/outputs/qwen3_sft_llama_factory.jsonl\n",
      "  文件大小: 1.31 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "## 数据格式转换：推理结果 → LLaMA-Factory SFT格式\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔄 数据格式转换\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== 配置 ====================\n",
    "\n",
    "# 输入文件（推理结果）\n",
    "INPUT_FILE = OUTPUT_PATH  # 之前保存的推理结果\n",
    "# INPUT_FILE = \"/data/private/outputs/qwen3_inference_1000_samples.jsonl\"\n",
    "\n",
    "# 输出文件（LLaMA-Factory格式）\n",
    "OUTPUT_FILE_LLAMA = f\"{OUTPUT_DIR}/qwen3_sft_llama_factory.jsonl\"\n",
    "\n",
    "# 格式选择\n",
    "FORMAT_TYPE = \"openai_with_thinking\"  # 可选: \"openai_with_thinking\", \"openai_no_thinking\", \"sharegpt\"\n",
    "\n",
    "# 思考内容处理方式\n",
    "THINKING_MODE = \"merge\"  # 可选: \"merge\"（合并到回答中）, \"separate\"（单独字段）, \"remove\"（移除）\n",
    "\n",
    "print(f\"\\n配置信息:\")\n",
    "print(f\"  输入文件: {INPUT_FILE}\")\n",
    "print(f\"  输出文件: {OUTPUT_FILE_LLAMA}\")\n",
    "print(f\"  输出格式: {FORMAT_TYPE}\")\n",
    "print(f\"  思考处理: {THINKING_MODE}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ==================== 转换函数 ====================\n",
    "\n",
    "def convert_to_openai_format(data, include_thinking=True, thinking_mode=\"merge\"):\n",
    "    \"\"\"\n",
    "    转换为OpenAI格式（LLaMA-Factory标准格式）\n",
    "    \n",
    "    参数:\n",
    "        data: 原始数据\n",
    "        include_thinking: 是否包含思考内容\n",
    "        thinking_mode: \"merge\"（合并）, \"separate\"（分离）, \"remove\"（移除）\n",
    "    \n",
    "    返回:\n",
    "        转换后的数据\n",
    "    \"\"\"\n",
    "    # 提取messages\n",
    "    if isinstance(data, dict) and 'messages' in data:\n",
    "        messages = data['messages']\n",
    "    elif isinstance(data, list):\n",
    "        messages = data\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # 构建新的messages列表\n",
    "    new_messages = []\n",
    "    reasoning_content = \"\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        if not isinstance(msg, dict):\n",
    "            continue\n",
    "        \n",
    "        # 处理user消息\n",
    "        if msg.get('role') == 'user':\n",
    "            new_messages.append({\n",
    "                \"role\": \"user\",\n",
    "                \"content\": msg.get('content', '')\n",
    "            })\n",
    "        \n",
    "        # 处理assistant消息\n",
    "        elif msg.get('role') == 'assistant':\n",
    "            content = msg.get('content', '')\n",
    "            \n",
    "            # 如果包含思考内容，根据模式处理\n",
    "            if include_thinking and reasoning_content:\n",
    "                if thinking_mode == \"merge\":\n",
    "                    # 合并：思考过程 + 回答\n",
    "                    content = f\"<think>\\n{reasoning_content}\\n</think>\\n\\n{content}\"\n",
    "                elif thinking_mode == \"separate\":\n",
    "                    # 分离：先添加思考消息\n",
    "                    new_messages.append({\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": f\"<think>\\n{reasoning_content}\\n</think>\"\n",
    "                    })\n",
    "            \n",
    "            new_messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": content\n",
    "            })\n",
    "        \n",
    "        # 提取reasoning_content\n",
    "        elif 'reasoning_content' in msg:\n",
    "            reasoning_content = msg.get('reasoning_content', '')\n",
    "    \n",
    "    # 返回OpenAI格式\n",
    "    return {\n",
    "        \"messages\": new_messages\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_to_sharegpt_format(data, include_thinking=True):\n",
    "    \"\"\"\n",
    "    转换为ShareGPT格式\n",
    "    \n",
    "    参数:\n",
    "        data: 原始数据\n",
    "        include_thinking: 是否包含思考内容\n",
    "    \n",
    "    返回:\n",
    "        转换后的数据\n",
    "    \"\"\"\n",
    "    # 提取messages\n",
    "    if isinstance(data, dict) and 'messages' in data:\n",
    "        messages = data['messages']\n",
    "    elif isinstance(data, list):\n",
    "        messages = data\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # 构建conversations列表\n",
    "    conversations = []\n",
    "    reasoning_content = \"\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        if not isinstance(msg, dict):\n",
    "            continue\n",
    "        \n",
    "        # 处理user消息\n",
    "        if msg.get('role') == 'user':\n",
    "            conversations.append({\n",
    "                \"from\": \"human\",\n",
    "                \"value\": msg.get('content', '')\n",
    "            })\n",
    "        \n",
    "        # 处理assistant消息\n",
    "        elif msg.get('role') == 'assistant':\n",
    "            content = msg.get('content', '')\n",
    "            \n",
    "            # 如果包含思考内容，合并\n",
    "            if include_thinking and reasoning_content:\n",
    "                content = f\"<think>\\n{reasoning_content}\\n</think>\\n\\n{content}\"\n",
    "            \n",
    "            conversations.append({\n",
    "                \"from\": \"gpt\",\n",
    "                \"value\": content\n",
    "            })\n",
    "        \n",
    "        # 提取reasoning_content\n",
    "        elif 'reasoning_content' in msg:\n",
    "            reasoning_content = msg.get('reasoning_content', '')\n",
    "    \n",
    "    # 返回ShareGPT格式\n",
    "    return {\n",
    "        \"conversations\": conversations\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_dataset(input_file, output_file, format_type=\"openai_with_thinking\", thinking_mode=\"merge\"):\n",
    "    \"\"\"\n",
    "    批量转换数据集\n",
    "    \n",
    "    参数:\n",
    "        input_file: 输入文件路径\n",
    "        output_file: 输出文件路径\n",
    "        format_type: 输出格式类型\n",
    "        thinking_mode: 思考内容处理方式\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"❌ 输入文件不存在: {input_file}\")\n",
    "        return\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 读取并转换\n",
    "    converted_data = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"🔄 开始转换...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # 根据格式类型转换\n",
    "                if format_type == \"openai_with_thinking\":\n",
    "                    converted = convert_to_openai_format(data, include_thinking=True, thinking_mode=thinking_mode)\n",
    "                elif format_type == \"openai_no_thinking\":\n",
    "                    converted = convert_to_openai_format(data, include_thinking=False, thinking_mode=thinking_mode)\n",
    "                elif format_type == \"sharegpt\":\n",
    "                    converted = convert_to_sharegpt_format(data, include_thinking=True)\n",
    "                else:\n",
    "                    print(f\"⚠️  未知格式类型: {format_type}\")\n",
    "                    converted = None\n",
    "                \n",
    "                if converted:\n",
    "                    converted_data.append(converted)\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️  第{line_num}行JSON解析失败: {e}\")\n",
    "                failed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  第{line_num}行转换失败: {e}\")\n",
    "                failed_count += 1\n",
    "    \n",
    "    # 保存转换后的数据\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for item in converted_data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # 统计信息\n",
    "    print(f\"\\n✅ 转换完成！\")\n",
    "    print(f\"  成功: {len(converted_data)} 条\")\n",
    "    print(f\"  失败: {failed_count} 条\")\n",
    "    print(f\"  输出文件: {output_file}\")\n",
    "    \n",
    "    # 文件大小\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"  文件大小: {file_size:.2f} MB\")\n",
    "    \n",
    "    return converted_data\n",
    "\n",
    "\n",
    "# ==================== 执行转换 ====================\n",
    "\n",
    "converted_data = convert_dataset(\n",
    "    INPUT_FILE, \n",
    "    OUTPUT_FILE_LLAMA,\n",
    "    format_type=FORMAT_TYPE,\n",
    "    thinking_mode=THINKING_MODE\n",
    ")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26383846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "🔄 转换为 Alpaca 格式\n",
      "============================================================\n",
      "\n",
      "配置:\n",
      "  输入: /data/private/outputs/qwen3_inference_1000_samples.jsonl\n",
      "  输出: /data/private/outputs/qwen3_sft_alpaca_format.json\n",
      "  保留思考: 是\n",
      "  思考格式: tags\n",
      "\n",
      "📖 读取数据...\n",
      "💾 保存数据...\n",
      "\n",
      "✅ 转换完成！\n",
      "  成功: 668 条\n",
      "  失败: 332 条\n",
      "  输出文件: /data/private/outputs/qwen3_sft_alpaca_format.json\n",
      "  文件大小: 2.85 MB\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "## 数据格式转换：推理结果 → Alpaca格式（JSON数组）\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"🔄 转换为 Alpaca 格式\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ==================== 配置 ====================\n",
    "\n",
    "# 输入文件（推理结果）\n",
    "INPUT_FILE = OUTPUT_PATH\n",
    "# INPUT_FILE = \"/data/private/outputs/qwen3_inference_1000_samples.jsonl\"\n",
    "\n",
    "# 输出文件（Alpaca格式 - JSON数组）\n",
    "OUTPUT_FILE_ALPACA = f\"{OUTPUT_DIR}/qwen3_sft_alpaca_format.json\"\n",
    "\n",
    "# 思考内容处理\n",
    "INCLUDE_THINKING = True  # True=保留思考内容，False=仅保留最终回答\n",
    "THINKING_FORMAT = \"tags\"  # \"tags\"=用<think>标签, \"prefix\"=前缀说明, \"remove\"=移除\n",
    "\n",
    "print(f\"\\n配置:\")\n",
    "print(f\"  输入: {INPUT_FILE}\")\n",
    "print(f\"  输出: {OUTPUT_FILE_ALPACA}\")\n",
    "print(f\"  保留思考: {'是' if INCLUDE_THINKING else '否'}\")\n",
    "print(f\"  思考格式: {THINKING_FORMAT}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# ==================== 转换函数 ====================\n",
    "\n",
    "def convert_to_alpaca_format(data, include_thinking=True, thinking_format=\"tags\"):\n",
    "    \"\"\"\n",
    "    转换为Alpaca格式\n",
    "    \n",
    "    Alpaca格式:\n",
    "    {\n",
    "      \"instruction\": \"用户问题\",\n",
    "      \"input\": \"\",\n",
    "      \"output\": \"模型回答\"\n",
    "    }\n",
    "    \n",
    "    参数:\n",
    "        data: 原始数据\n",
    "        include_thinking: 是否在output中包含思考过程\n",
    "        thinking_format: 思考内容的格式\n",
    "            - \"tags\": <think>思考</think>\\n\\n回答\n",
    "            - \"prefix\": 【思考过程】\\n思考\\n\\n【回答】\\n回答\n",
    "            - \"remove\": 仅保留最终回答\n",
    "    \n",
    "    返回:\n",
    "        Alpaca格式的数据，失败返回None\n",
    "    \"\"\"\n",
    "    # 提取messages\n",
    "    if isinstance(data, dict) and 'messages' in data:\n",
    "        messages = data['messages']\n",
    "    elif isinstance(data, list):\n",
    "        messages = data\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    # 提取instruction和output\n",
    "    instruction = \"\"\n",
    "    output = \"\"\n",
    "    reasoning_content = \"\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        if not isinstance(msg, dict):\n",
    "            continue\n",
    "        \n",
    "        # 提取用户问题\n",
    "        if msg.get('role') == 'user':\n",
    "            instruction = msg.get('content', '')\n",
    "        \n",
    "        # 提取模型回答\n",
    "        elif msg.get('role') == 'assistant':\n",
    "            output = msg.get('content', '')\n",
    "        \n",
    "        # 提取思考内容\n",
    "        elif 'reasoning_content' in msg:\n",
    "            reasoning_content = msg.get('reasoning_content', '')\n",
    "    \n",
    "    # 检查必填字段\n",
    "    if not instruction or not output:\n",
    "        return None\n",
    "    \n",
    "    # 处理思考内容\n",
    "    if include_thinking and reasoning_content:\n",
    "        if thinking_format == \"tags\":\n",
    "            # 格式：<think>思考</think>\\n\\n回答\n",
    "            output = f\"<think>\\n{reasoning_content}\\n</think>\\n\\n{output}\"\n",
    "        \n",
    "        elif thinking_format == \"prefix\":\n",
    "            # 格式：【思考过程】\\n思考\\n\\n【回答】\\n回答\n",
    "            output = f\"【思考过程】\\n{reasoning_content}\\n\\n【回答】\\n{output}\"\n",
    "        \n",
    "        elif thinking_format == \"remove\":\n",
    "            # 仅保留回答\n",
    "            pass\n",
    "    \n",
    "    # 返回Alpaca格式\n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": \"\",  # 我们的数据没有单独的input字段\n",
    "        \"output\": output\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_dataset_to_alpaca(input_file, output_file, include_thinking=True, thinking_format=\"tags\"):\n",
    "    \"\"\"\n",
    "    批量转换数据集为Alpaca格式（JSON数组）\n",
    "    \n",
    "    参数:\n",
    "        input_file: 输入文件路径（JSONL）\n",
    "        output_file: 输出文件路径（JSON数组）\n",
    "        include_thinking: 是否包含思考内容\n",
    "        thinking_format: 思考内容格式\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"❌ 输入文件不存在: {input_file}\")\n",
    "        return []\n",
    "    \n",
    "    # 确保输出目录存在\n",
    "    Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 读取并转换\n",
    "    alpaca_dataset = []\n",
    "    failed_count = 0\n",
    "    \n",
    "    print(f\"📖 读取数据...\")\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # 转换为Alpaca格式\n",
    "                alpaca_item = convert_to_alpaca_format(\n",
    "                    data, \n",
    "                    include_thinking=include_thinking,\n",
    "                    thinking_format=thinking_format\n",
    "                )\n",
    "                \n",
    "                if alpaca_item:\n",
    "                    alpaca_dataset.append(alpaca_item)\n",
    "                else:\n",
    "                    failed_count += 1\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️  第{line_num}行JSON解析失败: {e}\")\n",
    "                failed_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  第{line_num}行转换失败: {e}\")\n",
    "                failed_count += 1\n",
    "    \n",
    "    # 保存为JSON数组\n",
    "    print(f\"💾 保存数据...\")\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(alpaca_dataset, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # 统计信息\n",
    "    print(f\"\\n✅ 转换完成！\")\n",
    "    print(f\"  成功: {len(alpaca_dataset)} 条\")\n",
    "    print(f\"  失败: {failed_count} 条\")\n",
    "    print(f\"  输出文件: {output_file}\")\n",
    "    \n",
    "    # 文件大小\n",
    "    file_size = os.path.getsize(output_file) / (1024 * 1024)\n",
    "    print(f\"  文件大小: {file_size:.2f} MB\")\n",
    "    \n",
    "    return alpaca_dataset\n",
    "\n",
    "\n",
    "# ==================== 执行转换 ====================\n",
    "\n",
    "alpaca_dataset = convert_dataset_to_alpaca(\n",
    "    INPUT_FILE, \n",
    "    OUTPUT_FILE_ALPACA,\n",
    "    include_thinking=INCLUDE_THINKING,\n",
    "    thinking_format=THINKING_FORMAT\n",
    ")\n",
    "\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12ca7e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据集: /data/private/outputs/qwen3_sft_alpaca_format.json\n",
      "数据集总数: 668 条\n",
      "\n",
      "切分结果:\n",
      "训练集: 602 条 (90.1%)\n",
      "测试集: 66 条 (9.9%)\n",
      "\n",
      "训练集已保存到: /data/private/outputs/qwen3_sft_alpaca_format_train.json\n",
      "测试集已保存到: /data/private/outputs/qwen3_sft_alpaca_format_test.json\n",
      "\n",
      "训练集第一条数据示例:\n",
      "{\n",
      "  \"instruction\": \"写一篇小红书风格的帖子，标题是TF黑管口红合集来啦～\",\n",
      "  \"input\": \"\",\n",
      "  \"output\": \"<think>\\n好的，用户让我写一篇小红书风格的帖子，标题是“TF黑管口红合集来啦～”。首先，我需要了解小红书的风格是什么样的。小红书上的帖子通常比较活泼，用很多表情符号和分段标题，内容要亲切、实用，带有个人体验和推荐。所以，我得确保这篇帖子符合这些特点。\\n\\n接下来，用户的需求是展示TF黑管口红的合集，可能需要包括不同色号、质地、适合的场合和肤色等信息。用户可能希望吸引对TF口红感兴趣的粉丝，尤其是那些喜欢尝试不同妆容的人。他们可能想知道哪些色号最值得入手，或者如何根据自己的肤色选择合适的颜色。\\n\\n我需要考虑用户可能的深层需求。除了展示产品，他们可能还希望提供一些购买建议、使用技巧，或者与其他品牌口红的对比，这样能增加帖子的实用性和互动性。此外，加入一些个人体验会让内容更真实可信，比如提到自己用了多久，或者朋友的反馈。\\n\\n然后，我要确定结构。通常小红书帖子会有吸引眼球的标题，然后分点介绍，使用小标题和表情符号分隔。可能需...\n"
     ]
    }
   ],
   "source": [
    "## 9. 数据集切分（训练集/测试集）\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# 配置参数\n",
    "DATASET_PATH = \"/data/private/outputs/qwen3_sft_alpaca_format.json\"\n",
    "TRAIN_OUTPUT_PATH = \"/data/private/outputs/qwen3_sft_alpaca_format_train.json\"\n",
    "TEST_OUTPUT_PATH = \"/data/private/outputs/qwen3_sft_alpaca_format_test.json\"\n",
    "TEST_RATIO = 0.1  # 测试集比例，默认10%\n",
    "RANDOM_SEED = 42  # 随机种子，保证可复现\n",
    "\n",
    "# 读取数据集\n",
    "print(f\"正在读取数据集: {DATASET_PATH}\")\n",
    "with open(DATASET_PATH, 'r', encoding='utf-8') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "print(f\"数据集总数: {len(dataset)} 条\")\n",
    "\n",
    "# 设置随机种子\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# 随机打乱数据\n",
    "shuffled_data = dataset.copy()\n",
    "random.shuffle(shuffled_data)\n",
    "\n",
    "# 计算切分点\n",
    "test_size = int(len(shuffled_data) * TEST_RATIO)\n",
    "train_size = len(shuffled_data) - test_size\n",
    "\n",
    "# 切分数据\n",
    "train_data = shuffled_data[:train_size]\n",
    "test_data = shuffled_data[train_size:]\n",
    "\n",
    "print(f\"\\n切分结果:\")\n",
    "print(f\"训练集: {len(train_data)} 条 ({len(train_data)/len(dataset)*100:.1f}%)\")\n",
    "print(f\"测试集: {len(test_data)} 条 ({len(test_data)/len(dataset)*100:.1f}%)\")\n",
    "\n",
    "# 确保输出目录存在\n",
    "Path(TRAIN_OUTPUT_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 保存训练集\n",
    "with open(TRAIN_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 保存测试集\n",
    "with open(TEST_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\n训练集已保存到: {TRAIN_OUTPUT_PATH}\")\n",
    "print(f\"测试集已保存到: {TEST_OUTPUT_PATH}\")\n",
    "\n",
    "# 显示第一条训练数据示例\n",
    "if train_data:\n",
    "    print(\"\\n训练集第一条数据示例:\")\n",
    "    print(json.dumps(train_data[0], ensure_ascii=False, indent=2)[:500] + \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "716404bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据集: /data/private/outputs/qwen3_sft_alpaca_format.json\n",
      "原数据集总共有 668 条数据\n",
      "已抽取 10 条数据\n",
      "小批量数据集已保存到: /data/private/outputs/qwen3_sft_alpaca_format_sample10.json\n",
      "\n",
      "第一条数据示例:\n",
      "{\n",
      "  \"instruction\": \"请分析这段文字传达的中心思想。\\n输入：Jane Doe的故事展现了她作为一位杰出企业家的历程，向我们证明了拥有雄心壮志和坚定决心的人能够达成成功。\",\n",
      "  \"input\": \"\",\n",
      "  \"output\": \"<think>\\n嗯，用户让我分析这段文字的中心思想。输入是关于Jane Doe的故事，说她作为企业家展现了雄心壮志和坚定决心，最终成功。首先，我需要确认用户的需求是什么。他们可能是在做阅读理解练习，或者需要总结某个文章的核心观点，也可能是为了写报告或演讲稿需要提炼主题。\\n\\n接下来，我要仔细看看这段文字的关键点。里面提到了“杰出企业家”、“雄心壮志”、“坚定决心”和“成功”。这些都是关键词，可能需要在中心思想里强调这些要素。用户可能希望得到一个简洁明了的总结，同时包含这些元素。\\n\\n然后，我需要考虑用户可能的深层需求。也许他们想了解如何将类似的故事应用到其他领域，或者寻找激励他人的例子。也有可能用户是学生，老师布置了分析中心思想的作业，所以需要准确的分析步骤和结论。\\n\\n分析结构的话，原文的结构是先介绍Jane Doe的事例，然后点出主题。所以中心思想应该围绕她的品质和成功之间的关系。要注意避免加入个人见解，保持客观，但也要指出作者的意图是证明这些品质的重要性。\\n\\n还要确认是否有其他隐含的信息。比如，是否强调性别因素？原文提到Jane Doe，可能暗示女性在企业家中的成就，但用户没有特别指出这点，所以可能不需要深入，除非有明确提示。不过如果有上下文的话可能需要考虑，但根据现有输入，可能不需要。\\n\\n最后，确保回答清晰，结构合理，先点明中心思想，再分解要素，最后总结作者的意图。可能用户需要这样的结构来应用到其他分析中，所以展示分析过程也很重要。比如，先识别关键词，再确定主题，然后连接起来形成结论。\\n\\n</think>\\n\\n这段文字传达的中心思想是：通过Jane Doe作为企业家的奋斗历程，阐明了雄心壮志与坚定决心是实现成功的关键要素。\\n\\n分析过程：\\n1. 核心人物定位：以\\\"Jane Doe的故事\\\"为叙事载体，将抽象理念具象化\\n2. 价值导向提取：通过\\\"杰出企业家\\\"的定性判断，确立积极正向的价值坐标\\n3. 成功要素提炼：用\\\"雄心壮志\\\"（目标驱动）和\\\"坚定决心\\\"（意志品质）构建成功公式\\n4. 哲理升华：通过\\\"证明\\\"一词将个案上升为普遍规律，强调主观能动性对成就的决定作用\\n\\n深层意涵：作者通过人物叙事传递了一种成功学范式，暗示在逆境中突破需要兼具远大愿景和持续行动力，这种二元要素的结合具有超越个体案例的普适启示价值。\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# 原数据集路径\n",
    "input_file = \"/data/private/outputs/qwen3_sft_alpaca_format.json\"\n",
    "# 新数据集路径（小批量10条）\n",
    "output_file = \"/data/private/outputs/qwen3_sft_alpaca_format_sample10.json\"\n",
    "\n",
    "# 读取原数据集\n",
    "print(f\"正在读取数据集: {input_file}\")\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"原数据集总共有 {len(data)} 条数据\")\n",
    "\n",
    "# 抽取前10条数据\n",
    "sample_data = data[:10]\n",
    "print(f\"已抽取 {len(sample_data)} 条数据\")\n",
    "\n",
    "# 保存到新文件\n",
    "with open(output_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(sample_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"小批量数据集已保存到: {output_file}\")\n",
    "print(f\"\\n第一条数据示例:\")\n",
    "print(json.dumps(sample_data[0], ensure_ascii=False, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca22c892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d974b631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.20.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.1.3)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboard) (24.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.9)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tensorboard) (10.3.0)\n",
      "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (6.33.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.76.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard) (59.6.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.3.1)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.26.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
      "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.10/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorboard \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2ff5a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ERROR: Could not find `tensorboard`. Please ensure that your PATH\n",
       "contains an executable `tensorboard` program, or explicitly specify\n",
       "the path to a TensorBoard binary by setting the `TENSORBOARD_BINARY`\n",
       "environment variable."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir /data/private/outputs/qwen3_4b_lora_900samples1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb9a5b4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple/\n",
      "\u001b[33mDEPRECATION: The HTML index page being used (http://mirrors.aliyun.com/pypi/simple/peft/) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be well-formed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting peft\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/49/fe/a2da1627aa9cb6310b6034598363bd26ac301c4a99d21f415b1b2855891e/peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.9/504.9 KB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
      "\u001b[33mDEPRECATION: The HTML index page being used (http://mirrors.aliyun.com/pypi/simple/accelerate/) is not a proper HTML 5 document. This is in violation of PEP 503 which requires these pages to be well-formed HTML 5 documents. Please reach out to the owners of this index page, and ask them to update this index page to a valid HTML 5 document. pip 22.2 will enforce this behaviour change. Discussion can be found at https://github.com/pypa/pip/issues/10825\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting accelerate>=0.21.0\n",
      "  Downloading http://mirrors.aliyun.com/pypi/packages/77/85/85951bc0f9843e2c10baaa1b6657227056095de08f4d1eea7d8b423a6832/accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.8/375.8 KB\u001b[0m \u001b[31m169.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.35.3)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.57.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.9.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (1.1.10)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (4.15.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.25.0->peft) (2.31.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.9.90)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.93)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.8.90)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /usr/lib/python3/dist-packages (from triton==3.4.0->torch>=1.13.0->peft) (59.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2025.10.23)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.22.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.2.1)\n",
      "Installing collected packages: accelerate, peft\n",
      "Successfully installed accelerate-1.11.0 peft-0.17.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8657efda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ /data/private/merged_qwen3_4b_lora 不存在，将创建新目录\n",
      "正在加载基础模型...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded903a4ca5f4d95bbedc8fdf913802f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在加载 LoRA 适配器...\n",
      "正在合并模型...\n",
      "正在保存合并后的模型...\n",
      "正在保存 tokenizer...\n",
      "✅ 模型已成功保存至 /data/private/merged_qwen3_4b_lora\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# 检查保存目录是否已存在\n",
    "save_dir = \"/data/private/merged_qwen3_4b_lora\"\n",
    "if os.path.exists(save_dir):\n",
    "    print(f\"⚠️ 警告: {save_dir} 已存在，将会覆盖！\")\n",
    "else:\n",
    "    print(f\"✅ {save_dir} 不存在，将创建新目录\")\n",
    "\n",
    "# 加载基础模型（使用本地路径）\n",
    "print(\"正在加载基础模型...\")\n",
    "base = AutoModelForCausalLM.from_pretrained(\n",
    "    \"/data/public/models/base/Qwen/Qwen3-4B\",\n",
    "    torch_dtype=\"auto\"\n",
    ")\n",
    "\n",
    "# 加载 LoRA 适配器\n",
    "print(\"正在加载 LoRA 适配器...\")\n",
    "model = PeftModel.from_pretrained(\n",
    "    base, \n",
    "    \"/data/private/outputs/qwen3_4b_lora_900samples\"\n",
    ")\n",
    "\n",
    "# 合并并卸载\n",
    "print(\"正在合并模型...\")\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 保存合并后的模型\n",
    "print(\"正在保存合并后的模型...\")\n",
    "model.save_pretrained(save_dir)\n",
    "\n",
    "# 也保存 tokenizer（使用同样的本地路径）\n",
    "print(\"正在保存 tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"/data/public/models/base/Qwen/Qwen3-4B\"\n",
    ")\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"✅ 模型已成功保存至 {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94629947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "📖 从原始数据集提取第1001-2000条数据并转换为Alpaca格式\n",
      "============================================================\n",
      "源文件: /data/private/datasets/qwen3_235b_thinking_2507_distill_110k.jsonl\n",
      "输出文件: /data/private/outputs/qwen3_sft_alpaca_format1101-1200.json\n",
      "提取范围: 第1101条 - 第1200条\n",
      "============================================================\n",
      "\n",
      "📖 读取数据...\n",
      "\n",
      "💾 保存数据...\n",
      "\n",
      "============================================================\n",
      "✅ 转换完成！\n",
      "============================================================\n",
      "✅ 成功: 100 条\n",
      "❌ 失败: 0 条\n",
      "💾 输出文件: /data/private/outputs/qwen3_sft_alpaca_format1101-1200.json\n",
      "📊 文件大小: 0.74 MB\n",
      "\n",
      "============================================================\n",
      "📋 第一条数据示例（第1001条）\n",
      "============================================================\n",
      "Instruction: 樱桃开办了一家快递服务。她对于3-5公斤的货物收取2.50美元，对于6-8公斤的货物收取4美元。如果她每天送出四个5公斤的货物和两个8公斤的货物，一周她能赚多少钱？...\n",
      "Output: <think>\n",
      "我现在要解决的问题是樱桃开快递服务，计算她一周能赚多少钱。首先，我需要理清楚题目给出的信息。题目说，对于3-5公斤的货物收取2.50美元，6-8公斤的货物收取4美元。每天送出四个5公斤的货物和两个8公斤的货物，问一周赚多少钱。\n",
      "\n",
      "首先，我需要确定每个货物的收费属于哪个区间。5公斤的货物属于3-5公斤的区间，所以每个收费2.50美元。8公斤的货物属于6-8公斤的区间，所以每个收费4美...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 配置路径\n",
    "SOURCE_FILE = \"/data/private/datasets/qwen3_235b_thinking_2507_distill_110k.jsonl\"\n",
    "OUTPUT_FILE = \"/data/private/outputs/qwen3_sft_alpaca_format1101-1200.json\"\n",
    "\n",
    "# 提取范围：第1101-1200条（索引1100-1199）\n",
    "START_IDX = 1100\n",
    "END_IDX = 1200\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"📖 从原始数据集提取第1001-2000条数据并转换为Alpaca格式\")\n",
    "print(\"=\"*60)\n",
    "print(f\"源文件: {SOURCE_FILE}\")\n",
    "print(f\"输出文件: {OUTPUT_FILE}\")\n",
    "print(f\"提取范围: 第{START_IDX+1}条 - 第{END_IDX}条\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "def convert_to_alpaca_format(data):\n",
    "    \"\"\"\n",
    "    将原始数据格式转换为Alpaca格式\n",
    "    \n",
    "    原始格式:\n",
    "    {\n",
    "      \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"问题\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"<think>思考</think>\\n\\n回答\"}\n",
    "      ]\n",
    "    }\n",
    "    \n",
    "    Alpaca格式:\n",
    "    {\n",
    "      \"instruction\": \"问题\",\n",
    "      \"input\": \"\",\n",
    "      \"output\": \"<think>思考</think>\\n\\n回答\"\n",
    "    }\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict) and 'messages' in data:\n",
    "        messages = data['messages']\n",
    "    elif isinstance(data, list):\n",
    "        messages = data\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    instruction = \"\"\n",
    "    output = \"\"\n",
    "    \n",
    "    for msg in messages:\n",
    "        if not isinstance(msg, dict):\n",
    "            continue\n",
    "        \n",
    "        if msg.get('role') == 'user':\n",
    "            instruction = msg.get('content', '')\n",
    "        elif msg.get('role') == 'assistant':\n",
    "            output = msg.get('content', '')\n",
    "    \n",
    "    if not instruction or not output:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        \"instruction\": instruction,\n",
    "        \"input\": \"\",\n",
    "        \"output\": output\n",
    "    }\n",
    "\n",
    "# 读取并转换数据\n",
    "alpaca_data = []\n",
    "failed_count = 0\n",
    "\n",
    "print(\"📖 读取数据...\")\n",
    "with open(SOURCE_FILE, 'r', encoding='utf-8') as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        # 跳过前1000条\n",
    "        if idx < START_IDX:\n",
    "            continue\n",
    "        \n",
    "        # 读取到2000条后停止\n",
    "        if idx >= END_IDX:\n",
    "            break\n",
    "        \n",
    "        if not line.strip():\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            data = json.loads(line.strip())\n",
    "            alpaca_item = convert_to_alpaca_format(data)\n",
    "            \n",
    "            if alpaca_item:\n",
    "                alpaca_data.append(alpaca_item)\n",
    "            else:\n",
    "                failed_count += 1\n",
    "                print(f\"⚠️  第{idx+1}行转换失败（格式错误）\")\n",
    "        except json.JSONDecodeError as e:\n",
    "            failed_count += 1\n",
    "            print(f\"⚠️  第{idx+1}行JSON解析失败: {e}\")\n",
    "        except Exception as e:\n",
    "            failed_count += 1\n",
    "            print(f\"⚠️  第{idx+1}行转换失败: {e}\")\n",
    "\n",
    "# 保存为JSON数组\n",
    "print(f\"\\n💾 保存数据...\")\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
    "    json.dump(alpaca_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# 统计信息\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✅ 转换完成！\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"✅ 成功: {len(alpaca_data)} 条\")\n",
    "print(f\"❌ 失败: {failed_count} 条\")\n",
    "print(f\"💾 输出文件: {OUTPUT_FILE}\")\n",
    "\n",
    "import os\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / (1024 * 1024)\n",
    "    print(f\"📊 文件大小: {file_size:.2f} MB\")\n",
    "\n",
    "# 显示第一条示例\n",
    "if alpaca_data:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"📋 第一条数据示例（第1001条）\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Instruction: {alpaca_data[0]['instruction'][:100]}...\")\n",
    "    print(f\"Output: {alpaca_data[0]['output'][:200]}...\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8a9b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#评估脚本\n",
    "cd /data/private/LLaMA-Factory\n",
    "\n",
    "# 评估微调后的模型\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/merged_qwen3_4b_lora \\\n",
    "    --dataset qwen3_eval \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846bb6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 使用 vLLM 多卡推理评估\n",
    "\n",
    "```\n",
    "python scripts/vllm_infer.py --model_name_or_path meta-llama/Meta-Llama-3-8B-Instruct --template llama3 --dataset alpaca_en_demo\n",
    "python scripts/eval_bleu_rouge.py generated_predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf6c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /data/private/LLaMA-Factory\n",
    "\n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb512d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##效果对比\n",
    "#qwen3-4b原始\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-1.jsonl\n",
    "\n",
    "\n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/merged_qwen3_4b_lora \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-1.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-1.jsonl\n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a951c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##效果对比\n",
    "#qwen3-4b原始\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-2.jsonl \\\n",
    "    --template qwen3_nothink \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-2.jsonl\n",
    "\n",
    "\n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/merged_qwen3_4b_lora \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-4.jsonl \\\n",
    "    --template qwen3_nothink\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-4.jsonl\n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a3bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###nothinking\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-qwentemplate.jsonl \\\n",
    "    --template qwen \n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-nothinking.jsonl\n",
    "    \n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5629c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###nothinking\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-nothinkingnotemplate.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-nothinking.jsonl\n",
    "    \n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353c0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##效果对比\n",
    "#qwen3-4b原始\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-1.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-1.jsonl\n",
    "\n",
    "\n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/merged_qwen3_4b_lora \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-1.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-1.jsonl\n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl \n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-2.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c823f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###nothinking\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-qwentemplateF.jsonl \\\n",
    "    --template qwen \n",
    "    --enable_thinking false\n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-nothinking.jsonl\n",
    "    \n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl \\\n",
    "    --enable_thinking false\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-nothinking.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1629c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "###nothinking\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl \\\n",
    "    --template qwen3\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl\n",
    "    \n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-templateqwen3.jsonl \\\n",
    "    --template qwen3\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-templateqwen3.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e398a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "###nothinking\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl \\\n",
    "    --template qwen3\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl\n",
    "    \n",
    "    \n",
    "    \n",
    "#qwen3-4b-lora\n",
    "python scripts/vllm_infer.py \\\n",
    "    --model_name_or_path /data/private/models/qwen3_4b_merged1 \\\n",
    "    --dataset qwen3_eval \\\n",
    "    --save_name /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-newtemplateqwen3.jsonl \\\n",
    "    --template qwen3\n",
    "    \n",
    "    \n",
    "python scripts/eval_bleu_rouge.py \\\n",
    "    /data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-newtemplateqwen3.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d10fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307eac3be51846c3938e5635a0255306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "樱桃开办了一家快递服务。她对于3-5公斤的货物收取2.50美元，对于6-8公斤的货物收取4美元。如果她每天送出四个5公斤的货物和两个8公斤的货物，一周她能赚多少钱？（一周7天）\n",
      "\n",
      "樱桃每天送四个5公斤的货物和两个8公斤的货物。首先，我需要计算每天的收入，然后乘以7天。\n",
      "\n",
      "对于每个5公斤的货物，收费是2.50美元，所以四个的话就是4乘以2.50美元。计算一下：4 × 2.50 = 10美元。\n",
      "\n",
      "然后是两个8公斤的货物，每个收费4美元，所以是2乘以4美元。计算：2 × 4 = 8美元。\n",
      "\n",
      "每天的总收入就是10美元加上8美元，等于18美元。\n",
      "\n",
      "接下来，一周7天的总收入就是18美元乘以7天。计算：18 × 7 = 126美元。\n",
      "\n",
      "所以，樱桃一周能赚126美元。\n",
      "答案是126美元。樱桃一周能赚126美元。$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed{126}\n",
      "$$\n",
      "$$\n",
      "\\boxed\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 模型路径\n",
    "MODEL_PATH = \"/data/public/models/base/Qwen/Qwen3-4B\"\n",
    "\n",
    "# 问题\n",
    "prompt = (\n",
    "    \"樱桃开办了一家快递服务。她对于3-5公斤的货物收取2.50美元，\"\n",
    "    \"对于6-8公斤的货物收取4美元。如果她每天送出四个5公斤的货物和两个8公斤的货物，\"\n",
    "    \"一周她能赚多少钱？\"\n",
    ")\n",
    "\n",
    "# 加载模型与分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# 编码输入\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# 生成结果\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=1024, temperature=0.2)\n",
    "\n",
    "# 解码输出\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 打印结果\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a63ae65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-23 02:52:37--  https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/dataset_info.json\n",
      "Resolving aicloud-privoxy3.midea.com (aicloud-privoxy3.midea.com)... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "failed: Temporary failure in name resolution.\n",
      "wget: unable to resolve host address ‘aicloud-privoxy3.midea.com’\n"
     ]
    }
   ],
   "source": [
    "!cd /data/private/LLaMA-Factory\n",
    "!mkdir -p data\n",
    "!wget https://raw.githubusercontent.com/hiyouga/LLaMA-Factory/main/data/dataset_info.json -O data/dataset_info.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8605d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:2222] 2025-10-23 02:50:32,933 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1881] 2025-10-23 02:50:32,960 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,964 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,965 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,965 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,965 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,965 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,965 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:32,966 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-10-23 02:50:33,249 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:763] 2025-10-23 02:50:33,250 >> loading configuration file /data/public/models/base/Qwen/Qwen3-4B/config.json\n",
      "[INFO|configuration_utils.py:839] 2025-10-23 02:50:33,251 >> Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"dtype\": \"bfloat16\",\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9728,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,252 >> loading file vocab.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,252 >> loading file merges.txt\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,253 >> loading file tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,253 >> loading file added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,253 >> loading file special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,253 >> loading file tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2093] 2025-10-23 02:50:33,253 >> loading file chat_template.jinja\n",
      "[INFO|tokenization_utils_base.py:2364] 2025-10-23 02:50:33,555 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot open data/dataset_info.json due to [Errno 2] No such file or directory: 'data/dataset_info.json'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/data/private/LLaMA-Factory/src/llamafactory/data/parser.py:109\u001b[0m, in \u001b[0;36mget_dataset_list\u001b[0;34m(dataset_names, dataset_dir)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 109\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    110\u001b[0m         dataset_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/dataset_info.json'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m tok_mod \u001b[38;5;241m=\u001b[39m load_tokenizer(model_args)\n\u001b[1;32m     14\u001b[0m tpl \u001b[38;5;241m=\u001b[39m get_template_and_fix_tokenizer(tok_mod[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m], data_args)\n\u001b[0;32m---> 16\u001b[0m dataset_mod \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtpl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtok_mod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m train_data \u001b[38;5;241m=\u001b[39m dataset_mod[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_data[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m/data/private/LLaMA-Factory/src/llamafactory/data/loader.py:304\u001b[0m, in \u001b[0;36mget_dataset\u001b[0;34m(template, model_args, data_args, training_args, stage, tokenizer, processor)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;66;03m# Load and preprocess dataset\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mmain_process_first(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, local\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m data_args\u001b[38;5;241m.\u001b[39mdata_shared_file_system)):\n\u001b[0;32m--> 304\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43m_get_merged_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m     eval_dataset \u001b[38;5;241m=\u001b[39m _get_merged_dataset(\n\u001b[1;32m    306\u001b[0m         data_args\u001b[38;5;241m.\u001b[39meval_dataset,\n\u001b[1;32m    307\u001b[0m         model_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    311\u001b[0m         return_dict\u001b[38;5;241m=\u001b[39mdata_args\u001b[38;5;241m.\u001b[39meval_on_each_dataset,\n\u001b[1;32m    312\u001b[0m     )\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m training_args\u001b[38;5;241m.\u001b[39mmain_process_first(desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre-process dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, local\u001b[38;5;241m=\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m data_args\u001b[38;5;241m.\u001b[39mdata_shared_file_system)):\n",
      "File \u001b[0;32m/data/private/LLaMA-Factory/src/llamafactory/data/loader.py:178\u001b[0m, in \u001b[0;36m_get_merged_dataset\u001b[0;34m(dataset_names, model_args, data_args, training_args, stage, return_dict)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    177\u001b[0m datasets \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, dataset_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(dataset_names, \u001b[43mget_dataset_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_dir\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (stage \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dataset_attr\u001b[38;5;241m.\u001b[39mranking \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m (stage \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m dataset_attr\u001b[38;5;241m.\u001b[39mranking \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe dataset is not applicable in the current training stage.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/data/private/LLaMA-Factory/src/llamafactory/data/parser.py:113\u001b[0m, in \u001b[0;36mget_dataset_list\u001b[0;34m(dataset_names, dataset_dir)\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dataset_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 113\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot open \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m due to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(err)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    115\u001b[0m         dataset_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    117\u001b[0m dataset_list: \u001b[38;5;28mlist\u001b[39m[DatasetAttr] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot open data/dataset_info.json due to [Errno 2] No such file or directory: 'data/dataset_info.json'."
     ]
    }
   ],
   "source": [
    "from llamafactory.data import get_dataset, get_template_and_fix_tokenizer\n",
    "from transformers import Seq2SeqTrainingArguments\n",
    "from llamafactory.hparams import get_infer_args\n",
    "from llamafactory.model import load_tokenizer\n",
    "\n",
    "model_args, data_args, _, _ = get_infer_args({\n",
    "    \"model_name_or_path\": \"/data/public/models/base/Qwen/Qwen3-4B\",\n",
    "    \"template\": \"qwen3\",\n",
    "    \"dataset\": \"alpaca_en_demo\"\n",
    "})\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(output_dir=\"tmp\")\n",
    "tok_mod = load_tokenizer(model_args)\n",
    "tpl = get_template_and_fix_tokenizer(tok_mod[\"tokenizer\"], data_args)\n",
    "\n",
    "dataset_mod = get_dataset(tpl, model_args, data_args, training_args, \"ppo\", **tok_mod)\n",
    "train_data = dataset_mod[\"train_dataset\"]\n",
    "print(train_data[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e84e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##合并\n",
    "cd /data/private/LLaMA-Factory\n",
    "\n",
    "llamafactory-cli export \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --adapter_name_or_path /data/private/outputs/qwen3_4b_lora_900samples \\\n",
    "    --template qwen \\\n",
    "    --export_dir /data/private/models/qwen3_4b_merged1 \\\n",
    "    --export_size 2 \\\n",
    "    --export_device cpu \\\n",
    "    --export_legacy_format false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e66f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "export VLLM_USE_V1=1\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model /data/private/merged_qwen3_4b_lora \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 1024 \\\n",
    "    --gpu-memory-utilization 0.6 \\\n",
    "    --trust-remote-code \\\n",
    "    --max-num-seqs 64 \\\n",
    "    --enforce-eager\n",
    "    \n",
    "    \n",
    "export VLLM_USE_V1=1\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --host 0.0.0.0 \\\n",
    "    --port 8000 \\\n",
    "    --dtype bfloat16 \\\n",
    "    --max-model-len 1024 \\\n",
    "    --gpu-memory-utilization 0.6 \\\n",
    "    --trust-remote-code \\\n",
    "    --max-num-seqs 64 \\\n",
    "    --enforce-eager\n",
    "    \n",
    "    \n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"/data/public/models/base/Qwen/Qwen3-4B\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"樱桃开办了一家快递服务。她对于3-5公斤的货物收取2.50美元，对于6-8公斤的货物收取4美元。如果她每天送出四个5公斤的货物和两个8公斤的货物，一周她能赚多少钱？\"}\n",
    "        ],\n",
    "        \"max_tokens\": 900,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n",
    "    \n",
    "curl http://localhost:8000/v1/chat/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"/data/public/models/base/Qwen/Qwen3-4B\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"日本首都是哪？直接回答\"}\n",
    "        ],\n",
    "        \"max_tokens\": 900,\n",
    "        \"temperature\": 0.7\n",
    "    }'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60a1c40a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f136f1db03454a948268e6c9fbce1baf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当然，以下是一个关于大语言模型（Large Language Models，简称LLMs）的简介：\n",
      "\n",
      "---\n",
      "\n",
      "### 大语言模型简介\n",
      "\n",
      "大语言模型（Large Language Models，简称LLMs）是一种基于深度学习技术的先进自然语言处理（NLP）模型，能够理解和生成大量文本数据。它们通过在海量文本语料上进行训练，从而学习语言的复杂结构和语义，具备强大的语言理解和生成能力。\n",
      "\n",
      "#### 核心特点：\n",
      "\n",
      "1. **大规模参数量**  \n",
      "   大语言模型通常拥有数十亿甚至数千亿个参数，使其具备处理复杂任务的能力。\n",
      "\n",
      "2. **自监督预训练**  \n",
      "   这些模型通常在大量无标注文本数据上进行自监督学习，无需依赖人工标注数据，即可掌握广泛的语言知识。\n",
      "\n",
      "3. **上下文理解能力强**  \n",
      "   LLMs 能够处理长文本输入，理解上下文逻辑，生成连贯、自然的文本输出。\n",
      "\n",
      "4. **多任务能力**  \n",
      "   从文本生成、问答、翻译到代码生成、逻辑推理等，LLMs 能够在多种自然语言任务中表现出色。\n",
      "\n",
      "5. **微调与适配**  \n",
      "   通过微调（Fine-tuning）或提示工程（Prompt Engineering），LLMs 可以快速适应不同的应用场景和领域。\n",
      "\n",
      "#### 典型应用：\n",
      "\n",
      "- **聊天机器人与虚拟助手**\n",
      "- **自动摘要与内容生成**\n",
      "- **多语言翻译**\n",
      "- **代码生成与调试**\n",
      "- **学术写作与论文润色**\n",
      "- **个性化推荐系统**\n",
      "\n",
      "#### 发展现状：\n",
      "\n",
      "近年来，大语言模型取得了显著进展，如 GPT、BERT、T5、Llama、Qwen 等模型的出现，推动了自然语言处理领域的变革。这些模型不仅提升了文本生成质量，还促进了 AI 在实际业务中的应用。\n",
      "\n",
      "#### 挑战与展望：\n",
      "\n",
      "尽管大语言模型具有强大能力，但也面临数据隐私、算法偏见、安全性和伦理问题等挑战。未来的研究方向包括更高效的训练方法、更小规模但性能相近的模型、以及在特定领域更精准的应用。\n",
      "\n",
      "---\n",
      "\n",
      "如果你需要更详细的解释或某个具体模型的介绍，也可以告诉我！\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# 1️⃣ 指定本地模型路径\n",
    "model_name = \"/data/private/models/qwen3_4b_merged1\"\n",
    "\n",
    "# 2️⃣ 加载 tokenizer 和 model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,     # 或 \"auto\"\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# 3️⃣ 构造对话消息\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"请给我一个关于大语言模型的简介。\"}\n",
    "]\n",
    "\n",
    "# 4️⃣ 应用 chat 模板（注意：Qwen3 模板支持 enable_thinking）\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False# ⚠️ 这里改为 False 就是不输出 <think>\n",
    ")\n",
    "\n",
    "# 5️⃣ 生成输出\n",
    "inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=1024,\n",
    "    temperature=0.7,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "# 6️⃣ 解码并打印结果\n",
    "response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d006806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{%- if tools %}\n",
      "    {{- '<|im_start|>system\\n' }}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- messages[0].content + '\\n\\n' }}\n",
      "    {%- endif %}\n",
      "    {{- \"# Tools\\n\\nYou may call one or more functions to assist with the user query.\\n\\nYou are provided with function signatures within <tools></tools> XML tags:\\n<tools>\" }}\n",
      "    {%- for tool in tools %}\n",
      "        {{- \"\\n\" }}\n",
      "        {{- tool | tojson }}\n",
      "    {%- endfor %}\n",
      "    {{- \"\\n</tools>\\n\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\n<tool_call>\\n{\\\"name\\\": <function-name>, \\\"arguments\\\": <args-json-object>}\\n</tool_call><|im_end|>\\n\" }}\n",
      "{%- else %}\n",
      "    {%- if messages[0].role == 'system' %}\n",
      "        {{- '<|im_start|>system\\n' + messages[0].content + '<|im_end|>\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n",
      "{%- set ns = namespace(multi_step_tool=true, last_query_index=messages|length - 1) %}\n",
      "{%- for message in messages[::-1] %}\n",
      "    {%- set index = (messages|length - 1) - loop.index0 %}\n",
      "    {%- if ns.multi_step_tool and message.role == \"user\" and message.content is string and not(message.content.startswith('<tool_response>') and message.content.endswith('</tool_response>')) %}\n",
      "        {%- set ns.multi_step_tool = false %}\n",
      "        {%- set ns.last_query_index = index %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message.content is string %}\n",
      "        {%- set content = message.content %}\n",
      "    {%- else %}\n",
      "        {%- set content = '' %}\n",
      "    {%- endif %}\n",
      "    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) %}\n",
      "        {{- '<|im_start|>' + message.role + '\\n' + content + '<|im_end|>' + '\\n' }}\n",
      "    {%- elif message.role == \"assistant\" %}\n",
      "        {%- set reasoning_content = '' %}\n",
      "        {%- if message.reasoning_content is string %}\n",
      "            {%- set reasoning_content = message.reasoning_content %}\n",
      "        {%- else %}\n",
      "            {%- if '</think>' in content %}\n",
      "                {%- set reasoning_content = content.split('</think>')[0].rstrip('\\n').split('<think>')[-1].lstrip('\\n') %}\n",
      "                {%- set content = content.split('</think>')[-1].lstrip('\\n') %}\n",
      "            {%- endif %}\n",
      "        {%- endif %}\n",
      "        {%- if loop.index0 > ns.last_query_index %}\n",
      "            {%- if loop.last or (not loop.last and reasoning_content) %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n<think>\\n' + reasoning_content.strip('\\n') + '\\n</think>\\n\\n' + content.lstrip('\\n') }}\n",
      "            {%- else %}\n",
      "                {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "            {%- endif %}\n",
      "        {%- else %}\n",
      "            {{- '<|im_start|>' + message.role + '\\n' + content }}\n",
      "        {%- endif %}\n",
      "        {%- if message.tool_calls %}\n",
      "            {%- for tool_call in message.tool_calls %}\n",
      "                {%- if (loop.first and content) or (not loop.first) %}\n",
      "                    {{- '\\n' }}\n",
      "                {%- endif %}\n",
      "                {%- if tool_call.function %}\n",
      "                    {%- set tool_call = tool_call.function %}\n",
      "                {%- endif %}\n",
      "                {{- '<tool_call>\\n{\"name\": \"' }}\n",
      "                {{- tool_call.name }}\n",
      "                {{- '\", \"arguments\": ' }}\n",
      "                {%- if tool_call.arguments is string %}\n",
      "                    {{- tool_call.arguments }}\n",
      "                {%- else %}\n",
      "                    {{- tool_call.arguments | tojson }}\n",
      "                {%- endif %}\n",
      "                {{- '}\\n</tool_call>' }}\n",
      "            {%- endfor %}\n",
      "        {%- endif %}\n",
      "        {{- '<|im_end|>\\n' }}\n",
      "    {%- elif message.role == \"tool\" %}\n",
      "        {%- if loop.first or (messages[loop.index0 - 1].role != \"tool\") %}\n",
      "            {{- '<|im_start|>user' }}\n",
      "        {%- endif %}\n",
      "        {{- '\\n<tool_response>\\n' }}\n",
      "        {{- content }}\n",
      "        {{- '\\n</tool_response>' }}\n",
      "        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\n",
      "            {{- '<|im_end|>\\n' }}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|im_start|>assistant\\n' }}\n",
      "    {%- if enable_thinking is defined and enable_thinking is false %}\n",
      "        {{- '<think>\\n\\n</think>\\n\\n' }}\n",
      "    {%- endif %}\n",
      "{%- endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "182c1d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== enable_thinking=True ===\n",
      " <|im_start|>user\n",
      "给我一句大模型简介。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "=== enable_thinking=False ===\n",
      " <|im_start|>user\n",
      "给我一句大模型简介。<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"/data/public/models/base/Qwen/Qwen3-4B\", trust_remote_code=True)\n",
    "msgs = [{\"role\": \"user\", \"content\": \"给我一句大模型简介。\"}]\n",
    "\n",
    "prompt_T = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True, enable_thinking=True)\n",
    "prompt_F = tok.apply_chat_template(msgs, tokenize=False, add_generation_prompt=True, enable_thinking=False)\n",
    "\n",
    "print(\"=== enable_thinking=True ===\\n\", prompt_T[:400])\n",
    "print(\"=== enable_thinking=False ===\\n\", prompt_F[:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0c4bc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 测试Judge API连通性...\n",
      "API URL: https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\n",
      "============================================================\n",
      "📞 发送测试请求...\n",
      "❌ 连接失败: HTTPSConnectionPool(host='aimpapi.midea.com', port=443): Max retries exceeded with url: /t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7faff8fbf880>: Failed to resolve 'aimpapi.midea.com' ([Errno -3] Temporary failure in name resolution)\"))\n",
      "\n",
      "可能的原因:\n",
      "1. 不在美的内网或VPN未连接\n",
      "2. DNS无法解析域名 aimpapi.midea.com\n",
      "3. 需要配置代理\n",
      "\n",
      "建议:\n",
      "- 检查网络连接\n",
      "- 联系IT确认API访问权限\n",
      "- 暂时可以不使用Judge评分，只看基础统计\n",
      "\n",
      "============================================================\n",
      "测试完成\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "快速测试Judge API连通性\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# API配置\n",
    "JUDGE_API_URL = \"https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\"\n",
    "JUDGE_API_KEY = \"msk-c3e6e836acff79160513e563d6d288d3e24b9605349a03bb9564e9a7b3bafefe\"\n",
    "\n",
    "print(\"🔍 测试Judge API连通性...\")\n",
    "print(f\"API URL: {JUDGE_API_URL}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 测试请求\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {JUDGE_API_KEY}\"\n",
    "}\n",
    "\n",
    "payload = {\n",
    "    \"model\": \"/model/qwen3-235b-a22b\",\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"请给数字5打个分,只输出0-10之间的一个数字即可。\"\n",
    "        }\n",
    "    ],\n",
    "    \"stream\": True,\n",
    "    \"chat_template_kwargs\": {\"enable_thinking\": False}\n",
    "}\n",
    "\n",
    "try:\n",
    "    print(\"📞 发送测试请求...\")\n",
    "    response = requests.post(\n",
    "        JUDGE_API_URL,\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        timeout=30\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ HTTP状态码: {response.status_code}\")\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ 连接成功！\")\n",
    "        print(\"\\n📝 解析流式响应...\")\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for line in response.iter_lines():\n",
    "            if line:\n",
    "                line_str = line.decode('utf-8')\n",
    "                if line_str.startswith('data: '):\n",
    "                    data_str = line_str[6:]\n",
    "                    if data_str.strip() == '[DONE]':\n",
    "                        break\n",
    "                    try:\n",
    "                        data = json.loads(data_str)\n",
    "                        if 'choices' in data and len(data['choices']) > 0:\n",
    "                            delta = data['choices'][0].get('delta', {})\n",
    "                            content = delta.get('content', '')\n",
    "                            full_response += content\n",
    "                            print(content, end='', flush=True)\n",
    "                    except json.JSONDecodeError:\n",
    "                        continue\n",
    "        \n",
    "        print(f\"\\n\\n✅ 完整响应: {full_response}\")\n",
    "        print(\"\\n🎉 Judge API 测试成功！可以正常使用评分功能。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"❌ 请求失败: {response.status_code}\")\n",
    "        print(f\"响应内容: {response.text}\")\n",
    "        \n",
    "except requests.exceptions.ConnectionError as e:\n",
    "    print(f\"❌ 连接失败: {e}\")\n",
    "    print(\"\\n可能的原因:\")\n",
    "    print(\"1. 不在美的内网或VPN未连接\")\n",
    "    print(\"2. DNS无法解析域名 aimpapi.midea.com\")\n",
    "    print(\"3. 需要配置代理\")\n",
    "    print(\"\\n建议:\")\n",
    "    print(\"- 检查网络连接\")\n",
    "    print(\"- 联系IT确认API访问权限\")\n",
    "    print(\"- 暂时可以不使用Judge评分，只看基础统计\")\n",
    "    \n",
    "except requests.exceptions.Timeout:\n",
    "    print(\"❌ 请求超时（30秒）\")\n",
    "    print(\"API响应太慢，可能需要调整超时时间\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 未知错误: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"测试完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c56ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "python evaluate_cot_model.py \\\n",
    "    --base_model /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --tuned_model /data/private/models/qwen3_4b_merged1 \\\n",
    "    --test_data /data/private/outputs/qwen3_sft_alpaca_format1101-1200.json \\\n",
    "    --num_samples 10 \\\n",
    "    --max_model_len 2048 \\\n",
    "    --output_dir /data/private/outputs/eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90c0cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "测试tokenizer是否支持enable_thinking参数\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_path = \"/data/public/models/base/Qwen/Qwen3-4B\"\n",
    "\n",
    "print(\"🔍 测试Qwen3-4B tokenizer的enable_thinking支持...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": \"1+1等于几？\"}]\n",
    "    \n",
    "    # 测试1: enable_thinking=False\n",
    "    print(\"\\n📋 测试1: enable_thinking=False\")\n",
    "    print(\"-\"*60)\n",
    "    try:\n",
    "        prompt_false = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            chat_template_kwargs={\"enable_thinking\": False}\n",
    "        )\n",
    "        print(\"✅ 成功生成prompt (False)\")\n",
    "        print(f\"Prompt前200字符:\\n{prompt_false[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 失败: {e}\")\n",
    "    \n",
    "    # 测试2: enable_thinking=True  \n",
    "    print(\"\\n📋 测试2: enable_thinking=True\")\n",
    "    print(\"-\"*60)\n",
    "    try:\n",
    "        prompt_true = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            chat_template_kwargs={\"enable_thinking\": True}\n",
    "        )\n",
    "        print(\"✅ 成功生成prompt (True)\")\n",
    "        print(f\"Prompt前200字符:\\n{prompt_true[:200]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 失败: {e}\")\n",
    "    \n",
    "    # 对比\n",
    "    if 'prompt_false' in locals() and 'prompt_true' in locals():\n",
    "        print(\"\\n🔍 对比两个prompts:\")\n",
    "        print(\"-\"*60)\n",
    "        if prompt_false == prompt_true:\n",
    "            print(\"❌ 两个prompts完全一样！enable_thinking参数没有起作用！\")\n",
    "        else:\n",
    "            print(\"✅ 两个prompts不同，参数有效果\")\n",
    "            print(f\"\\nFalse长度: {len(prompt_false)}\")\n",
    "            print(f\"True长度: {len(prompt_true)}\")\n",
    "    \n",
    "    # 测试3: 不传参数（默认行为）\n",
    "    print(\"\\n📋 测试3: 不传chat_template_kwargs\")\n",
    "    print(\"-\"*60)\n",
    "    prompt_default = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    print(f\"默认prompt前200字符:\\n{prompt_default[:200]}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ 加载tokenizer失败: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"测试完成\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb25f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /data/private/LLaMA-Factory \n",
    "llamafactory-cli train /data/private/qwen3_4b_lora_train_config_copy.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e523e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /data/private/LLaMA-Factory\n",
    "\n",
    "llamafactory-cli export \\\n",
    "    --model_name_or_path /data/public/models/base/Qwen/Qwen3-4B \\\n",
    "    --adapter_name_or_path /data/private/outputs/qwen3_4b_lora_900samples1 \\\n",
    "    --template qwen3 \\\n",
    "    --export_dir /data/private/models/qwen3_4b_merged1 \\\n",
    "    --export_size 2 \\\n",
    "    --export_device cpu \\\n",
    "    --export_legacy_format false "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f25c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📖 加载数据...\n",
      "✅ Base模型: 100 条\n",
      "✅ Tuned模型: 100 条\n",
      "\n",
      "🔍 分析生成结果...\n",
      "\n",
      "📊 基础统计:\n",
      "  Base模型 <think>生成率: 58.0%\n",
      "  Tuned模型 <think>生成率: 62.0%\n",
      "  Base平均thinking长度: 534 字符\n",
      "  Tuned平均thinking长度: 599 字符\n",
      "\n",
      "🎯 开始Judge评分 (并发数=10)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1477db58a22a454cab3517a4cf0cbe20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "评分进度:   0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "API错误: 429\n",
      "\n",
      "======================================================================\n",
      "📊 评测结果汇总\n",
      "======================================================================\n",
      "{\n",
      "  \"样本数量\": 100,\n",
      "  \"原始模型 (Qwen3-4B)\": {\n",
      "    \"Thinking平均分\": \"5.53/10\",\n",
      "    \"Answer平均分\": \"6.31/10\",\n",
      "    \"综合评分\": \"5.92/10\"\n",
      "  },\n",
      "  \"微调模型 (Qwen3-4B-LoRA)\": {\n",
      "    \"Thinking平均分\": \"5.57/10\",\n",
      "    \"Answer平均分\": \"6.38/10\",\n",
      "    \"综合评分\": \"5.97/10\"\n",
      "  },\n",
      "  \"提升幅度\": {\n",
      "    \"Thinking提升\": \"+0.04\",\n",
      "    \"Answer提升\": \"+0.08\",\n",
      "    \"综合提升\": \"+0.05\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 36136 (\\N{CJK UNIFIED IDEOGRAPH-8D28}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 32508 (\\N{CJK UNIFIED IDEOGRAPH-7EFC}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 21512 (\\N{CJK UNIFIED IDEOGRAPH-5408}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 35780 (\\N{CJK UNIFIED IDEOGRAPH-8BC4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 22343 (\\N{CJK UNIFIED IDEOGRAPH-5747}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 23545 (\\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 24494 (\\N{CJK UNIFIED IDEOGRAPH-5FAE}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 35843 (\\N{CJK UNIFIED IDEOGRAPH-8C03}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_204/1733477998.py:270: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35780 (\\N{CJK UNIFIED IDEOGRAPH-8BC4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 20998 (\\N{CJK UNIFIED IDEOGRAPH-5206}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24179 (\\N{CJK UNIFIED IDEOGRAPH-5E73}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22343 (\\N{CJK UNIFIED IDEOGRAPH-5747}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 23545 (\\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27604 (\\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 36136 (\\N{CJK UNIFIED IDEOGRAPH-8D28}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 37327 (\\N{CJK UNIFIED IDEOGRAPH-91CF}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 32508 (\\N{CJK UNIFIED IDEOGRAPH-7EFC}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21512 (\\N{CJK UNIFIED IDEOGRAPH-5408}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 21407 (\\N{CJK UNIFIED IDEOGRAPH-539F}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22987 (\\N{CJK UNIFIED IDEOGRAPH-59CB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 27169 (\\N{CJK UNIFIED IDEOGRAPH-6A21}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 22411 (\\N{CJK UNIFIED IDEOGRAPH-578B}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24494 (\\N{CJK UNIFIED IDEOGRAPH-5FAE}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 35843 (\\N{CJK UNIFIED IDEOGRAPH-8C03}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.10/dist-packages/IPython/core/pylabtools.py:170: UserWarning: Glyph 24067 (\\N{CJK UNIFIED IDEOGRAPH-5E03}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ 详细结果已保存: /data/private/evaluation_results.csv\n",
      "\n",
      "🎉 评测完成！\n"
     ]
    }
   ],
   "source": [
    "##llm as judge\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "使用Qwen3-235B作为Judge评测微调效果\n",
    "在Jupyter Notebook中运行\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==================== 配置 ====================\n",
    "API_URL = \"https://aimpapi.midea.com/t-aigc/aimp-qwen3-235b-a22b/v1/chat/completions\"\n",
    "API_KEY = \"msk-c3e6e836acff79160513e563d6d288d3e24b9605349a03bb9564e9a7b3bafefe\"\n",
    "\n",
    "BASE_MODEL_FILE = \"/data/private/LLaMA-Factory/generated_predictions-qwen3-4b-templateqwen3.jsonl\"\n",
    "TUNED_MODEL_FILE = \"/data/private/LLaMA-Factory/generated_predictions-qwen3-4b-lora-newtemplateqwen3.jsonl\"\n",
    "\n",
    "MAX_WORKERS = 10  # 并发数\n",
    "NUM_SAMPLES = None  # None=全部数据，或设置数字如10\n",
    "\n",
    "# ==================== 辅助函数 ====================\n",
    "\n",
    "def extract_think_and_answer(text):\n",
    "    \"\"\"提取thinking和answer部分\"\"\"\n",
    "    think_pattern = r'<think>(.*?)</think>'\n",
    "    match = re.search(think_pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        thinking = match.group(1).strip()\n",
    "        answer = text[match.end():].strip()\n",
    "        return thinking, answer\n",
    "    else:\n",
    "        return \"\", text.strip()\n",
    "\n",
    "def call_judge_api(prompt, thinking, answer, ground_truth, eval_type):\n",
    "    \"\"\"调用Judge API评分\"\"\"\n",
    "    if eval_type == \"thinking\":\n",
    "        user_content = f\"\"\"你是一个专业的思维过程评估专家。请评估以下思维过程(thinking)的质量。\n",
    "\n",
    "评分标准(0-10分):\n",
    "- 逻辑连贯性(3分): 思维过程是否有清晰的推理链条\n",
    "- 完整性(3分): 是否涵盖了解决问题的关键步骤\n",
    "- 深度(2分): 思考是否深入,有无考虑多个角度\n",
    "- 结构性(2分): 思维组织是否清晰,易于理解\n",
    "\n",
    "问题: {prompt}\n",
    "\n",
    "思维过程:\n",
    "{thinking if thinking else \"[无思维过程]\"}\n",
    "\n",
    "标准答案供参考:\n",
    "{ground_truth[:500]}...\n",
    "\n",
    "请只输出一个0-10之间的数字分数,不要有其他内容。\"\"\"\n",
    "    else:  # answer\n",
    "        user_content = f\"\"\"你是一个专业的答案质量评估专家。请评估以下答案的质量。\n",
    "\n",
    "评分标准(0-10分):\n",
    "- 准确性(4分): 答案是否正确回答了问题\n",
    "- 完整性(3分): 答案是否全面,有无遗漏关键信息\n",
    "- 清晰度(2分): 表达是否清晰易懂\n",
    "- 专业性(1分): 是否使用恰当的专业术语和表达方式\n",
    "\n",
    "问题: {prompt}\n",
    "\n",
    "生成的答案:\n",
    "{answer if answer else \"[无答案]\"}\n",
    "\n",
    "标准答案供参考:\n",
    "{ground_truth[:500]}...\n",
    "\n",
    "请只输出一个0-10之间的数字分数,不要有其他内容。\"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": \"/model/qwen3-235b-a22b\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": user_content}],\n",
    "        \"stream\": False,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(API_URL, headers=headers, json=payload, timeout=60)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            score_text = result['choices'][0]['message']['content'].strip()\n",
    "            # 提取数字\n",
    "            score = float(re.search(r'\\d+\\.?\\d*', score_text).group())\n",
    "            return min(max(score, 0), 10)\n",
    "        else:\n",
    "            print(f\"API错误: {response.status_code}\")\n",
    "            return 5.0\n",
    "    except Exception as e:\n",
    "        print(f\"评分失败: {e}\")\n",
    "        return 5.0\n",
    "\n",
    "# ==================== 加载数据 ====================\n",
    "\n",
    "print(\"📖 加载数据...\")\n",
    "with open(BASE_MODEL_FILE, 'r', encoding='utf-8') as f:\n",
    "    base_data = [json.loads(line) for line in f]\n",
    "\n",
    "with open(TUNED_MODEL_FILE, 'r', encoding='utf-8') as f:\n",
    "    tuned_data = [json.loads(line) for line in f]\n",
    "\n",
    "if NUM_SAMPLES:\n",
    "    base_data = base_data[:NUM_SAMPLES]\n",
    "    tuned_data = tuned_data[:NUM_SAMPLES]\n",
    "\n",
    "print(f\"✅ Base模型: {len(base_data)} 条\")\n",
    "print(f\"✅ Tuned模型: {len(tuned_data)} 条\")\n",
    "\n",
    "# 确保数据对应\n",
    "assert len(base_data) == len(tuned_data), \"两个数据集长度不一致！\"\n",
    "\n",
    "# ==================== 分析数据 ====================\n",
    "\n",
    "print(\"\\n🔍 分析生成结果...\")\n",
    "results = []\n",
    "\n",
    "for i, (base_item, tuned_item) in enumerate(zip(base_data, tuned_data)):\n",
    "    # 提取thinking和answer\n",
    "    base_think, base_ans = extract_think_and_answer(base_item['predict'])\n",
    "    tuned_think, tuned_ans = extract_think_and_answer(tuned_item['predict'])\n",
    "    \n",
    "    results.append({\n",
    "        'index': i,\n",
    "        'prompt': base_item['prompt'],\n",
    "        'label': base_item['label'],\n",
    "        'base_predict': base_item['predict'],\n",
    "        'tuned_predict': tuned_item['predict'],\n",
    "        'base_has_think': bool(base_think),\n",
    "        'base_think_length': len(base_think),\n",
    "        'tuned_has_think': bool(tuned_think),\n",
    "        'tuned_think_length': len(tuned_think),\n",
    "    })\n",
    "\n",
    "# 统计基础信息\n",
    "base_think_rate = sum(r['base_has_think'] for r in results) / len(results) * 100\n",
    "tuned_think_rate = sum(r['tuned_has_think'] for r in results) / len(results) * 100\n",
    "\n",
    "print(f\"\\n📊 基础统计:\")\n",
    "print(f\"  Base模型 <think>生成率: {base_think_rate:.1f}%\")\n",
    "print(f\"  Tuned模型 <think>生成率: {tuned_think_rate:.1f}%\")\n",
    "print(f\"  Base平均thinking长度: {sum(r['base_think_length'] for r in results)/len(results):.0f} 字符\")\n",
    "print(f\"  Tuned平均thinking长度: {sum(r['tuned_think_length'] for r in results)/len(results):.0f} 字符\")\n",
    "\n",
    "# ==================== Judge评分 ====================\n",
    "\n",
    "print(f\"\\n🎯 开始Judge评分 (并发数={MAX_WORKERS})...\")\n",
    "\n",
    "tasks = []\n",
    "for i, result in enumerate(results):\n",
    "    base_think, base_ans = extract_think_and_answer(result['base_predict'])\n",
    "    tuned_think, tuned_ans = extract_think_and_answer(result['tuned_predict'])\n",
    "    \n",
    "    # 4个评分任务\n",
    "    tasks.extend([\n",
    "        (i, 'base_think', result['prompt'], base_think, base_ans, result['label'], 'thinking'),\n",
    "        (i, 'base_answer', result['prompt'], base_think, base_ans, result['label'], 'answer'),\n",
    "        (i, 'tuned_think', result['prompt'], tuned_think, tuned_ans, result['label'], 'thinking'),\n",
    "        (i, 'tuned_answer', result['prompt'], tuned_think, tuned_ans, result['label'], 'answer'),\n",
    "    ])\n",
    "\n",
    "# 并发评分\n",
    "scores = {}\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {\n",
    "        executor.submit(call_judge_api, task[2], task[3], task[4], task[5], task[6]): task\n",
    "        for task in tasks\n",
    "    }\n",
    "    \n",
    "    for future in tqdm(as_completed(futures), total=len(tasks), desc=\"评分进度\"):\n",
    "        task = futures[future]\n",
    "        idx, score_type = task[0], task[1]\n",
    "        try:\n",
    "            score = future.result()\n",
    "            scores[(idx, score_type)] = score\n",
    "        except Exception as e:\n",
    "            print(f\"任务失败: {e}\")\n",
    "            scores[(idx, score_type)] = 5.0\n",
    "\n",
    "# 填充评分\n",
    "for i, result in enumerate(results):\n",
    "    result['base_think_score'] = scores.get((i, 'base_think'), 5.0)\n",
    "    result['base_answer_score'] = scores.get((i, 'base_answer'), 5.0)\n",
    "    result['tuned_think_score'] = scores.get((i, 'tuned_think'), 5.0)\n",
    "    result['tuned_answer_score'] = scores.get((i, 'tuned_answer'), 5.0)\n",
    "\n",
    "# ==================== 生成报告 ====================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 评测结果汇总\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 计算平均分\n",
    "base_think_avg = sum(r['base_think_score'] for r in results) / len(results)\n",
    "base_answer_avg = sum(r['base_answer_score'] for r in results) / len(results)\n",
    "tuned_think_avg = sum(r['tuned_think_score'] for r in results) / len(results)\n",
    "tuned_answer_avg = sum(r['tuned_answer_score'] for r in results) / len(results)\n",
    "\n",
    "report = {\n",
    "    \"样本数量\": len(results),\n",
    "    \"原始模型 (Qwen3-4B)\": {\n",
    "        \"Thinking平均分\": f\"{base_think_avg:.2f}/10\",\n",
    "        \"Answer平均分\": f\"{base_answer_avg:.2f}/10\",\n",
    "        \"综合评分\": f\"{(base_think_avg + base_answer_avg)/2:.2f}/10\"\n",
    "    },\n",
    "    \"微调模型 (Qwen3-4B-LoRA)\": {\n",
    "        \"Thinking平均分\": f\"{tuned_think_avg:.2f}/10\",\n",
    "        \"Answer平均分\": f\"{tuned_answer_avg:.2f}/10\",\n",
    "        \"综合评分\": f\"{(tuned_think_avg + tuned_answer_avg)/2:.2f}/10\"\n",
    "    },\n",
    "    \"提升幅度\": {\n",
    "        \"Thinking提升\": f\"{tuned_think_avg - base_think_avg:+.2f}\",\n",
    "        \"Answer提升\": f\"{tuned_answer_avg - base_answer_avg:+.2f}\",\n",
    "        \"综合提升\": f\"{((tuned_think_avg + tuned_answer_avg) - (base_think_avg + base_answer_avg))/2:+.2f}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(report, ensure_ascii=False, indent=2))\n",
    "\n",
    "# ==================== 可视化 ====================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 图1: 平均分对比\n",
    "categories = ['Thinking质量', 'Answer质量', '综合评分']\n",
    "base_scores = [base_think_avg, base_answer_avg, (base_think_avg + base_answer_avg)/2]\n",
    "tuned_scores = [tuned_think_avg, tuned_answer_avg, (tuned_think_avg + tuned_answer_avg)/2]\n",
    "\n",
    "x = range(len(categories))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar([i - width/2 for i in x], base_scores, width, label='原始模型', alpha=0.8)\n",
    "axes[0].bar([i + width/2 for i in x], tuned_scores, width, label='微调模型', alpha=0.8)\n",
    "axes[0].set_ylabel('评分 (0-10)')\n",
    "axes[0].set_title('平均分对比')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(categories)\n",
    "axes[0].legend()\n",
    "axes[0].set_ylim(0, 10)\n",
    "\n",
    "# 图2: 分数分布\n",
    "axes[1].scatter(\n",
    "    [r['base_think_score'] for r in results],\n",
    "    [r['base_answer_score'] for r in results],\n",
    "    alpha=0.5, label='原始模型', s=50\n",
    ")\n",
    "axes[1].scatter(\n",
    "    [r['tuned_think_score'] for r in results],\n",
    "    [r['tuned_answer_score'] for r in results],\n",
    "    alpha=0.5, label='微调模型', s=50\n",
    ")\n",
    "axes[1].set_xlabel('Thinking评分')\n",
    "axes[1].set_ylabel('Answer评分')\n",
    "axes[1].set_title('评分分布')\n",
    "axes[1].legend()\n",
    "axes[1].plot([0, 10], [0, 10], 'k--', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ==================== 保存结果 ====================\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "output_file = \"/data/private/evaluation_results.csv\"\n",
    "df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\n✅ 详细结果已保存: {output_file}\")\n",
    "\n",
    "print(\"\\n🎉 评测完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee51691",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "======================================================================\n",
    "📈 评测结果汇总\n",
    "======================================================================\n",
    "\n",
    "📊 总评测样本数: 100\n",
    "\n",
    "【胜率统计】\n",
    "  模型A（原始）胜: 21/100 (21.0%)\n",
    "  模型B（微调）胜: 62/100 (62.0%)\n",
    "  平局:           17/100 (17.0%)\n",
    "\n",
    "【各维度平均分对比】(1-5分制)\n",
    "维度              模型A        模型B        差值        \n",
    "--------------------------------------------------\n",
    "正确性             3.60       3.93       +0.33\n",
    "相关性             3.55       4.03       +0.48\n",
    "完整性             3.56       3.94       +0.38\n",
    "推理能力            3.50       3.93       +0.43\n",
    "表达清晰度           3.31       4.12       +0.81\n",
    "有用性             3.28       4.12       +0.84\n",
    "\n",
    "【整体平均分】\n",
    "  模型A: 3.48\n",
    "  模型B: 4.28\n",
    "\n",
    "【结论】: ✅ 微调模型(B)表现明显优于原始模型(A)\n",
    "\n",
    "======================================================================\n",
    "🎉 评测完成！\n",
    "======================================================================\n",
    "\n",
    "📁 输出文件:\n",
    "  1. Pairwise数据:    /data/private/outputs/pairwise_comparison_data.jsonl\n",
    "  2. 详细评测结果:    /data/private/outputs/evaluation_results.jsonl\n",
    "  3. 汇总报告:        /data/private/outputs/evaluation_summary.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8fd23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "python3 tokens.py \\\n",
    "    --input \"/data/private/outputs/qwen3_sft_alpaca_format.json\" \\\n",
    "    --tokenizer Qwen/Qwen2.5-4B \\\n",
    "    --output token_report_qwen3.json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
